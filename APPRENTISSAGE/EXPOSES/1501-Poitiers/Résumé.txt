45' - 50'

* introduction Ã  l'inférence bayésienne et l'enjeu du calcul de lois conditionnelles avec les 3 approches traditionnelles : calcul exact (quand on peut), échantillonnage (MCMC) ou approximation (variationnelle) ;
* un exemple de calcul exact (cf segmentation, Cambridge) ;
* un exemple de calcul approché (cf inférence de modèle de graphe aléatoire: http://arxiv.org/abs/1310.6150).

Two examples of Bayesian inference : exact and approximate 

Statistical models are getting more and complex and involve ever more intricate dependency structures. As a consequence, the derivation of the statistical properties of the estimates is getting more and more difficult. Bayesian inference can be a way to circumvent this difficulty as it aims at providing the posterior distribution of the parameters, that is their conditional distribution given the data. In some cases, this distribution can be computed in an exact manner but, in more complex cases, either sampling (Monte-Carlo) techniques or approximations must be considered.

We will first present a segmentation problem. In this problem, the Bayesian inference requires to sum up over the set of all possible segmentations, which grows exponentially with the size of the data. We will prove that some nice algebraic properties allow to determine the posterior distribution of the change points in an exact manner.

We will then introduce a popular model for social network named the stochastic block model (SBM), which consists in a mixture model for random graph. We will show that the conditional distribution of the hidden variables cannot be determined in an exact manner and describe the variational Bayes approach that is often used to perform approximate Bayesian inference.