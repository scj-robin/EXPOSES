\documentclass[10pt]{beamer}

% Beamer style
%\usetheme[secheader]{Madrid}
% \usetheme{CambridgeUS}
\useoutertheme{infolines}
\usecolortheme[rgb={0.65,0.15,0.25}]{structure}
% \usefonttheme[onlymath]{serif}
\beamertemplatenavigationsymbolsempty
%\AtBeginSubsection

% Packagesg
%\usepackage[french]{babel}
\usepackage[latin1]{inputenc}
\usepackage{color}
\usepackage{xspace}
\usepackage{enumerate}
\usepackage{dsfont, stmaryrd}
% \usepackage{amsmath, amsfonts, amssymb}
\usepackage{amsmath, amsfonts, amssymb, MnSymbol}
\usepackage{epsfig}
\usepackage{tikz}
\usepackage{url}
\usepackage{/home/robin/LATEX/Biblio/astats}
%\usepackage[all]{xy}
\usepackage{graphicx}

% Commands
\definecolor{darkred}{rgb}{0.65,0.15,0.25}
\newcommand{\emphase}[1]{\textcolor{darkred}{#1}}
% \newcommand{\emphase}[1]{{#1}}
\newcommand{\paragraph}[1]{\textcolor{darkred}{#1}}
\newcommand{\refer}[1]{{\small{\textcolor{gray}{{[\cite{#1}]}}}}}
% \newcommand{\Refer}[1]{{\small{\textcolor{gray}{{[#1]}}}}}
\renewcommand{\newblock}{}

% Symbols
\newcommand{\Abf}{{\bf A}}
\newcommand{\Beta}{\text{B}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\BIC}{\text{BIC}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\dd}{\text{~d}}
\newcommand{\dbf}{{\bf d}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Esp}{\mathbb{E}}
\newcommand{\Espt}{\widetilde{\Esp}}
\newcommand{\Ebf}{{\bf E}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Gam}{\mathcal{G}\text{am}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ibb}{\mathbb{I}}
\newcommand{\Ibf}{{\bf I}}
\newcommand{\ICL}{\text{ICL}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\Corr}{\mathbb{C}\text{orr}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Vart}{\widetilde{\Var}}
\newcommand{\Vsf}{\mathsf{V}}
\newcommand{\pen}{\text{pen}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Hbf}{{\bf H}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kbf}{{\bf K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\mbf}{{\bf m}}
\newcommand{\mum}{\mu(\mbf)}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Nbf}{{\bf N}}
\newcommand{\Nm}{N(\mbf)}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Obf}{{\bf 0}}
\newcommand{\Omegas}{\underset{s}{\Omega}}
\newcommand{\Pbf}{{\bf P}}
\newcommand{\pt}{\widetilde{p}}
\newcommand{\Pt}{\widetilde{P}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\BP}{\text{BP}}
\newcommand{\EM}{\text{EM}}
\newcommand{\SBMreg}{{\text{SBM-reg}\xspace}}
\newcommand{\VEM}{\text{VEM}}
\newcommand{\VBEM}{\text{VBEM}}
\newcommand{\cst}{\text{cst}}
\newcommand{\obs}{\text{obs}}
\newcommand{\ra}{\emphase{\mathversion{bold}{$\rightarrow$}~}}
%\newcommand{\transp}{\text{{\tiny $\top$}}}
\newcommand{\transp}{\text{{\tiny \mathversion{bold}{$\top$}}}}
\newcommand{\logit}{\text{logit}\xspace}

% Directory
% \newcommand{\figpaper}{/home/robin/Bureau/Dropbox/VBEM-IS/Article_New/plots}
\newcommand{\figpaper}{/home/robin/Bureau/VBEM-IS/DropBox-04-01-18/Article_notes/archives_article/Article_New/plots}

%====================================================================
%====================================================================

%====================================================================
%====================================================================
\begin{document}
%====================================================================
%====================================================================

%====================================================================
\title[From variational Bayes to exact posterior]{From variational Bayes to exact posterior: \\
  a bridge sampling scheme}

\author[S. Robin]{S. Robin \\ ~\\
    Joint work with \underline{S. Donnet}
  }

\institute[INRA / AgroParisTech]{~ \\%INRA / AgroParisTech \\
  \vspace{-.1\textwidth}
  \begin{tabular}{ccc}
    \includegraphics[height=.25\textheight]{../FIGURES/LogoINRA-Couleur} & 
    \hspace{.02\textheight} &
    \includegraphics[height=.06\textheight]{../FIGURES/logagroptechsolo} % & 
%     \hspace{.02\textheight} &
%     \includegraphics[height=.09\textheight]{\fignet/logo-ssb}
    \\ 
  \end{tabular} \\
  \bigskip
  }

\date[June 2017, Paris]{AppliBugs, Jun. 2017, Paris}

%====================================================================
%====================================================================
\maketitle
%====================================================================

%====================================================================
%====================================================================
\section{A network model}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}
%====================================================================
\frame{\frametitle{Ecological network}

  \begin{tabular}{ccc}
  Network & Taxonomic dist. & Geographic dist. \\
  \includegraphics[width=.3\textwidth]{../FIGURES/FigVBEM-IS-Tree-Network.pdf}
  &
  \includegraphics[width=.3\textwidth]{../FIGURES/FigVBEM-IS-Tree-TaxonomicDistance.pdf}
  &
  \includegraphics[width=.3\textwidth]{../FIGURES/FigVBEM-IS-Tree-GeographicDistance.pdf}
  \end{tabular}

  \bigskip
  \paragraph{Data.}
  \begin{itemize}
   \item $n = 51$ tree species,
   \item $Y_{ij} = 1$ if species $i$ and $j$ share some (fungal) parasite, 0 otherwise
   \item $Y = (Y_{ij}) = $ adjacency matrix of the network
   \item $x_{ij} = (x_{ij}^1, \dots , x_{ij}^d) = $ vector of covariates for species pair $(i, j)$.
  \end{itemize}
}
  
%====================================================================
\frame{\frametitle{Problem}

  \paragraph{Questions.}
  \begin{enumerate}
   \item Do covariates contribute to explain the links between the species?
   \item Is there a remaining structure in the network?
   \item If so, how is it organized?
  \end{enumerate}
  
  \pause \bigskip 
  \paragraph{'\SBMreg' model.} \refer{LaR15,LRO15}
  \begin{itemize}
   \item Each species $i$ belongs to a (hidden) class $Z_i \in \{1, \dots K\}$
   \item $\pi = (\pi_1, \dots, \pi_K) = $ class proportions
   \item $\alpha = (\alpha_{k\ell}) =$ between-classes interaction coefficients 
   \item $\beta =$ regression coefficients
  \end{itemize}
  Edges $(Y_{ij})$ are independent conditional on the classes $(Z_i)$: 
  $$
  \logit \; P(Y_{ij} = 1 | Z_i, Z_j) = \emphase{x_{ij}^\intercal \beta + \alpha_{Z_i, Z_j}}.
  $$
}
  
%====================================================================
\frame{\frametitle{Bayesian inference}

  \paragraph{Priors.} Parameter $\theta = (\pi, \alpha, \beta)$
  $$
  \pi \sim \Dcal(\cdot), \qquad \alpha_{kl} \sim \Ncal(\cdot, \cdot), \qquad \beta \sim \Ncal(\cdot, \cdot), \qquad K \sim \Ucal[\dots]
  $$
  
  \bigskip \pause
  \paragraph{Questions.}
  \begin{enumerate}
   \item Effect of the covariates: $p(\beta | Y)$
   \item Remaining structure: $P(K = 1 | Y)$
   \item Classification $p(Z_i = k | Y)$
  \end{enumerate}

  \bigskip \bigskip \pause
  \paragraph{Problem.} No conjugacy holds \ra intractable posterior and conditional
  $$
  p(\theta | Y), \qquad p(Z | Y), \qquad p(\theta, Z | Y)
  $$
}
  
%====================================================================
\frame{\frametitle{Variational Bayes inference}

  \paragraph{General principle.} Approximate $p(\theta, Z | Y)$ with
  $$
  \pt_Y(\theta, Z) = \arg\min_{q \in \Qcal} \; KL\left[q(\cdot, \cdot) || p(\cdot, \cdot | Y)\right]
  $$
  $\pt_Y$ can be retrieved using a VB-EM algorithm \refer{BeG03}

  \bigskip \bigskip \pause
  \paragraph{Here} we take
  $$
  q(\theta, Z) = \Dcal(\pi; \cdot) 
  \times \prod_{k \leq \ell} \Ncal(\alpha_{kl}; \cdot, \cdot)
  \times \Ncal(\beta; \cdot, \cdot) 
  \times \prod_i \Mcal(Z_i; \cdot)
  $$
  \begin{itemize}
   \item \refer{LaR15} + R package Mixer % \\
%   \ra \refer{VMR12} to get $\pt_Y(K)$ 
  \end{itemize}

}

%====================================================================
\frame{\frametitle{So far so good, but}

  \paragraph{Properties of VB inference.}
  \begin{itemize}
   \item No general theoretical guaranties
   \item Some specific favorable cases (SBM)
   \item In most cases
   $$
   \Espt(\theta) \simeq \Esp(\theta | Y), 
   \qquad
   \Vart(\theta) \ll \Var(\theta | Y), 
   $$
   KL-minimization captures the mode but underestimates the variability.
  \end{itemize}
  
  \bigskip
  $$
  \text{\ra Credibility intervals and model selection may not be valid}
  $$
}

%====================================================================
%====================================================================
\section{General problem}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}
%====================================================================
\frame{\frametitle{General problem}

  \paragraph{Bayesian inference:} $\theta =$ parameter, $Y =$ observed data
  \begin{eqnarray*}
   \text{prior distribution:} & \theta & \sim \pi(\cdot) \\
   \text{likelihood:} & Y | \theta & \sim \ell(\cdot | \theta) \\
   \text{posterior distribution:} & \theta | Y & \sim p(\cdot | Y) 
  \end{eqnarray*}
  \ra Goal: evaluate $p(\theta | Y)$
  
  \pause \bigskip \bigskip 
  \paragraph{With latent variables:} same goal for
  $$
  p(\theta, Z | Y) 
  $$
  
  \bigskip 
  Now focusing on $p(\theta | Y)$
}
  
%====================================================================
\frame{\frametitle{Our goal}
  
  \paragraph{Three main approaches} to get $p(\theta | Y)$
  $$
  \begin{tabular}{lccc}
   & Method & Pros.  & Cons. \\ \hline 
   Exact & conjugacy, & exact & often intractable \\ 
   & algebra & & \\ \hline 
   Approximate & VB & \emphase{fast} & approximate \\ \hline 
   Sampling & MCMC, Gibbs, & \emphase{exact} & \emphase{computational} \\
   & SMC & \emphase{(if convergence)} & \emphase{time} 
  \end{tabular}
  $$
  
  \bigskip \pause
  \paragraph{Our goal}
  \begin{itemize}
   \item Get an exact sample from $p(\theta | Y)$
   \item Avoiding MCMC's convergence issues
   \item Taking advantage of a quickly available approximation $\pt_Y(\theta)$
  \end{itemize}

}

%====================================================================
\frame{\frametitle{First idea: Importance sampling}

  \paragraph{Goal.} Estimate
  $$
  \Esp[f(\theta) | Y] = \int f(\theta) p(\theta | Y) \dd \theta
  $$
  
  \pause \bigskip \bigskip
  \paragraph{Monte-Carlo: } $(\theta^m) =$ iid sample from $p(\cdot | Y)$
  $$
  \widehat{\Esp}[f(\theta) | Y] = \frac1M \sum_m f(\theta^m)
  $$

  \pause \bigskip \bigskip
  \paragraph{Importance sampling:} $(\theta^m) =$ iid sample from a \emphase{proposal $q$}  
  $$
  \widehat{\Esp}[f(\theta) | Y] = \sum_m  W^m f(\theta^m), 
  \qquad w^m = \frac{p(\theta^m | Y)}{q(\theta^m)}, \qquad W^m = \frac{w^m}{\sum_m w^m}
  $$
  $\Ecal = \{(\theta^m, 1)\} = q$-sample \; \ra \; $\Ecal' = \{(\theta^m, w^m)\} = p^*$-sample, 
}
  
%====================================================================
\frame{\frametitle{Importance of the proposal}

  Effective sample size $= ESS := \overline{w}^2 / \overline{w^2}$.

  \bigskip
  \begin{tabular}{ccc}
   \begin{tabular}{c}
    \includegraphics[width=.275\textwidth]{../FIGURES/FigVBEM-IS-ISpost.pdf}
   \end{tabular}
   &
   \begin{tabular}{c}
    \includegraphics[width=.275\textwidth]{../FIGURES/FigVBEM-IS-ISprior.pdf}
   \end{tabular}
   &
   \begin{tabular}{c}
    \includegraphics[width=.275\textwidth]{../FIGURES/FigVBEM-IS-ISgood.pdf}   
   \end{tabular}
   \\
   \begin{tabular}{c}
    \includegraphics[width=.275\textwidth]{../FIGURES/FigVBEM-IS-ISwrong.pdf}   
   \end{tabular}
   &
   \begin{tabular}{c}
    \includegraphics[width=.275\textwidth]{../FIGURES/FigVBEM-IS-ISvb.pdf}
   \end{tabular}
   &
   \begin{tabular}{c}
    \textcolor{red}{Proposal} \\ \textcolor{blue}{Target} \\ Sample
   \end{tabular}
  \end{tabular}

}
  
%====================================================================
%====================================================================
\section{Bridge sampling}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}
%====================================================================
\frame{\frametitle{Bridge sampling\footnote{'Bridge sampling' = 'Sequential importance sampling' (= 'SMC' ?)} principle}

  

  \begin{tabular}{cc}
    \begin{tabular}{p{.5\textwidth}}
	 \begin{itemize}
	   \onslide+<1->{\item $\textcolor{red}{q} = $ proposal, $\textcolor{blue}{p^*} = $ target \\}	 
	   \onslide+<2->{\item Define intermediate distributions
		$$p_0, p_1, ..., p_H$$
		with $p_0 = q$, $p_H = p^*$ \\}	 
	   \onslide+<3->{\item Iteratively: \\
	   use $p_h$ to get a sample from $p_{h+1}$}
	 \end{itemize}
    \end{tabular}
    & 
    \hspace{-.02\textwidth}
    \begin{tabular}{p{.5\textwidth}}
 	 \begin{overprint}
 	   \onslide<1> 
 	   \includegraphics[width=.4\textwidth]{../FIGURES/FigVBEM-IS-PropTarget.pdf}
 	   \onslide<2> 
 	   \includegraphics[width=.4\textwidth]{../FIGURES/FigVBEM-IS-Tempering.pdf}
 	   \onslide<3> 
 	   \includegraphics[width=.4\textwidth]{../FIGURES/FigVBEM-IS-Tempering-step1.pdf}
 	   \onslide<4> 
 	   \includegraphics[width=.4\textwidth]{../FIGURES/FigVBEM-IS-Tempering-step2.pdf}
 	   \onslide<5> 
 	   \includegraphics[width=.4\textwidth]{../FIGURES/FigVBEM-IS-Tempering-step3.pdf}
 	   \onslide<6-> 
 	   \includegraphics[width=.4\textwidth]{../FIGURES/FigVBEM-IS-Tempering-step4.pdf}
 	 \end{overprint}
    \end{tabular}
  \end{tabular}

  \onslide+<7->{\bigskip \pause
  \paragraph{Application:}
  $$
  q = \pt_Y, \qquad p^* = p(\cdot | Y)
  $$}
}
  
%====================================================================
\frame{\frametitle{Path sampling}

  \paragraph{Distribution path\footnote{\refer{Nea01}: $p_h(\theta) \propto \pi(\theta) \ell(Y | \theta)^{\rho_h}$, i.e. $\pt_Y = \pi$}:} 
    set $0 = \rho_0 < \rho_1 < \dots < \rho_{H-1} < \rho_H = 1$,
  \begin{eqnarray*}
     p_h(\theta) & \propto & \pt_Y(\theta)^{\emphase{{1-\rho_h}}} \; \times \; p(\theta | Y)^{\emphase{{\rho_h}}} \\
%      \\
     & \propto & \pt_Y(\theta) \; \times \; \alpha(\theta)^{\emphase{{\rho_h}}}, \qquad  \alpha(\theta) = \frac{\pi(\theta) \ell(Y | \theta)}{\pt_Y(\theta)}
  \end{eqnarray*}
  
  \bigskip \pause
  \paragraph{Aim of bridge sampling:} at each step $h$, provide
  $$
  \Ecal_h = \{(\theta_h^m, w_h^m)\}_m = \text{ weighted sample of } p_h
  $$
  
  
  \pause \bigskip 
  \paragraph{Questions}
  \begin{itemize}
   \item Step number $H$ ?
   \item Step size $\rho_h - \rho_{h-1}$?
   \item How to actually sample $p_h$ from the \emphase{sample} $\Ecal_{h-1}$?
  \end{itemize}
}
  
%====================================================================
\frame{\frametitle{Proposed 'SMC' algorithm}

  \begin{description}
   \item[Init.:] Sample $(\theta_0^m)_m$ iid $\sim \pt_Y$, $w_0^m = 1$ \\ ~
   \pause
   \item[Step $h$:] Using the previous sample $\Ecal_{h-1} = \{(\theta_{h-1}^m, w_{h-1}^m)\}$ \\ ~
   \pause
   \begin{enumerate}
    \item set $\rho_h$ such that $\emphase{cESS}(\Ecal_{h-1}; p_{h-1}, p_h) = \emphase{\tau_1}$ \\ ~
    \pause
    \item compute $w_h^m = w_{h-1}^m \times (\alpha_h^m)^{\rho_h - \rho_{h-1}}$ \\ ~
    \pause
    \item if $ESS_h = \overline{w}_h^2 / \overline{w_h^2} < \emphase{\tau_2}$, resample the particles \\ ~
    \pause
    \item propagate the particles $\theta_h^m \sim \emphase{K_h}(\theta_h^m | \theta_{h-1}^m)$
   \end{enumerate} ~ 
   \pause
   \item[Stop:] When $\rho_h$ reaches 1.
  \end{description}
}
  
%====================================================================
\frame{\frametitle{Some comments}

  \paragraph{Resampling (optional step 3).}
  \begin{itemize}
   \item avoids degeneracy
   \item set weights $w_h^m = 1$ after resampling 
  \end{itemize}
  
  \bigskip \bigskip 
  \paragraph{Propagation kernel $K_h$ (step 4).}
  \begin{itemize}
   \item with stationary distribution \emphase{$p_h$} (e.g. Gibbs sampler)
   \item just propagation: does not change the distribution \ra no convergence needed 
  \end{itemize}
  
  \bigskip \bigskip \pause
  \paragraph{Main property.} The last sample $\Ecal_H = \{(\theta^m_H, w^m_H)\}$ is a weighted sample of the target distribution  $p^*(\theta) = p(\theta | Y)$.

}
  
%====================================================================
\frame{\frametitle{Theoretical justification}

  At each step $h$, \refer{DDJ06} construct a distribution for the whole particle path with marginal $p_h$. \\ ~
  
  \begin{itemize}
   \item $\overline{p}_h(\theta_{0:h})$ distribution of the particle path
   $$
   \overline{p}_h(\theta_{0:h}) \propto p_h(\theta_h) \prod_{k=1}^h L_k(\theta_{k-1} |
   \theta_k)
   $$
   \item $L_h = $ backward kernel
   $$
   L_h(\theta_{h-1} | \theta_h) = K_h(\theta_h | \theta_{h-1}) p_h(\theta_{h-1}) /
   p_h(\theta_h)
   $$
   \item Update for the weights
   $$
   w_h(\theta_{0:h}) = w_{h-1}(\theta_{0:h-1}) \alpha(\theta_h)^{\rho_h - \rho_{h-1}}
   $$
  \end{itemize}
}
  
%====================================================================
\frame{\frametitle{Adaptive step size}

  \paragraph{Conditional ESS:} efficiency of sample $\Ecal$ from $q$ for distribution $p$
  $$
  cESS(\Ecal; q, p) = \frac{M \left(\sum_m W^m a^m\right)^2}{\sum_m W^m (a^m)^2}, 
  \qquad a^m = \frac{p(\theta^m)}{q(\theta^m)}
  $$
  \ra Step 1: find next $p_h$ s.t. sample $\Ecal_{h-1}$ is reasonably efficient.
  
%   \bigskip \bigskip \bigskip \pause
%   \paragraph{ESS:} intrinsic efficiency of a sample 
%   $$
%   ESS(\{(\theta^m, w^m)\}) = \frac{M \left(\sum_m w^m\right)^2}{\sum_m (w^m)^2}
%   $$
%   \ra Step 1c: resample if to few particles actually contribute
  
  \bigskip \bigskip \pause
  Thanks to the update formula of the weights
  $$
  cESS(\Ecal_{h-1}; p_{h-1}, p_h) 
  = 
  \frac
  {M \left[\sum_m W_{h-1}^m \; (\alpha^m_{h-1})^{\rho_h -\rho_{h-1}}\right]^2}
  {\sum_m W_{h-1}^m \; (\alpha^m_{h-1})^{2\rho_h - 2\rho_{h-1}}}
  $$
  can be computed for any $\rho_h$ \emphase{before sampling}.
  
  \bigskip
  \ra $\rho_h$ tuned to meet $\tau_1$, which controls the step size $\rho_h - \rho_{h-1}$ (and $H$)
}

%====================================================================
\frame{\frametitle{Marginal likelihood}

  Denote
  $$
  \gamma_h(\theta) = \pt_Y(\theta) \alpha(\theta)^{\rho_h}, 
  \qquad Z_h = \int \gamma_h(\theta) \dd \theta,
  \qquad p_h = \gamma_h(\theta) / Z_h
  $$
  
  \bigskip
  The marginal likelihood is given by
  $$
  p(Y) = \int \pi(\theta) \ell(Y|\theta) \dd \theta = \int \gamma_H(\theta) \dd \theta = Z_H
  $$

  \bigskip
  which can be estimated without bias with
  $$
  \widehat{\left(\frac{Z_H}{Z_0}\right)} = \prod_{h=1}^H \widehat{\left(\frac{Z_h}{Z_{h-1}}\right)} 
  \qquad \text{where} \quad
  \widehat{\left(\frac{Z_h}{Z_{h-1}}\right)} = \sum_m W_h^m (\alpha_h^m)^{\rho_h - \rho_{h-1}}
  $$
}
  
%====================================================================
%====================================================================
\section{Some simulations}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}
%====================================================================
\frame{\frametitle{Logistic regression}
  
  \paragraph{Model.} $x_i=$ covariates, $\beta =$ regression coefficients, $Y_i=$ binary outcome
  $$
  \beta \sim \Ncal, \qquad (Y_i) \text{ indep.} \;|\; \beta \sim \Bcal(p_i), \qquad \logit(p_i) = x_i^\intercal \beta
  $$
  \refer{JaJ00} VBEM with approximate Gaussian posterior for $\beta$.
  
  \bigskip \bigskip \pause
  \paragraph{Simulation study.}
  \begin{itemize}
   \item $n = 200$, $d= 4$ covariates
   \item Aim: compare initial proposals 
%    $\pt_Y$: $\Delta_{VB} = \text{diag}(\Sigma_{VB})$
%    \begin{eqnarray*}
%     \pt^1_Y(\beta) & = & \Ncal(\mu_{VB}, \Sigma_{VB}), \qquad
%     \pt^2_Y(\beta) \; = \; \Ncal(\mu_{ML}, \Sigma_{ML}), \\
%     \pt^3_Y(\beta) & = & \Ncal(\mu_{VB}, \Delta_{VB}/5), \qquad
%     \pt^4_Y(\beta) \; = \; \Ncal(\mu_{VB}, 10 \Delta_{VB}), \\
%     \pt^5_Y(\beta) & = & \Ncal(\mu_{VB}+.5, \Delta_{VB}/5)
%    \end{eqnarray*}
  \end{itemize}
}

%====================================================================
\frame{\frametitle{Logistic regression: Sampling path}

  \begin{tabular}{cc}
    \begin{tabular}{p{.5\textwidth}}
	 SMC: ($\Delta_{VB}  = \text{diag}(\Sigma_{VB})$) \\
	 $\textcolor{green}{\filleddiamond}: \pt_Y = \pt_{VB}$ \\
	 $\textcolor{purple}{\filleddiamond}: \pt_Y = \pt_{ML}$ \\
	 $\textcolor{red}{\filleddiamond}:$ variance $\pt_Y = \Delta_{VB}/5$ \\
	 $\textcolor{blue}{\filleddiamond}:$ variance $\pt_Y = 10\Delta_{VB}$ \\
	 $\textcolor{orange}{\filleddiamond}:$ $\pt_Y = \Ncal(\mu_{VB}+.5, \Delta_{VB}/5)$ \\
	 ~ \\
	 \refer{Nea01}: \\
	 $\bullet: $ $\pt_Y = \pi$ \\
	 \\
	 $\textcolor{blue}{\filledtriangleup} = $ hybrid
    \end{tabular}
    & 
    \hspace{-.1\textwidth}
    \begin{tabular}{c}
	 $\rho_h$ \\
	 \includegraphics[width=.45\textwidth]{\figpaper/LogReg_rho.pdf} 
    \end{tabular}
  \end{tabular}
}
  
%====================================================================
\frame{\frametitle{\SBMreg model}

  \paragraph{Model.} $Z_i$ node class, $Y_{ij}$ links, $x_{ij}$ edge covariates.
  $$
  (Z_i) \text{ iid } \Mcal(1; \pi), 
  \quad Y_{ij} | Z_i, Z_j \sim \Bcal(p_{ij}), 
  \quad \logit(p_{ij}) = x_{ij}^\intercal \beta + \alpha_{Z_i, Z_j}
  $$
  Parameter $\theta = (\pi, \alpha, \beta)$.
  
  \bigskip \bigskip \pause
  \paragraph{Simulation design.} 
  \begin{itemize}
   \item $n = 20, 50$ nodes, $K^* = 1, 2$ classes, $d = 3$ covariates, 
   \item $M = 1000$ particles, $B = 100$ samples.
   \item Parameters \emphase{sampled from the prior}.
  \end{itemize}
  
  \bigskip \bigskip \pause
  \paragraph{Property check.} $\theta^* \sim \pi$, $Y \sim \ell(Y | \theta^*)$ and $\{(\theta^m, w^m)\}$ a sample from $q(\theta)$:
  $$
  q(\theta) = p(\theta | Y) 
  \qquad \Rightarrow \qquad
  \sum_m W_m \Ibb\{\theta^m \leq \theta^*\} \sim \Ucal[0, 1]
  $$
}
  
%====================================================================
\frame{\frametitle{\SBMreg: $K^*$ known}

  \paragraph{Posterior distribution} of the regression coefficients $\beta_\ell$ \\ ~
  
  \begin{tabular}{ccc}
    posterior mean & posterior sd & dist. check \\
    \includegraphics[width=.3\textwidth]{\figpaper/SBMReg_results_global_VarParm1_M1000_prior_betaMean.pdf} 
    & 
    \includegraphics[width=.3\textwidth]{\figpaper/SBMReg_results_global_VarParm1_M1000_prior_betaSd.pdf} 
    & 
    \includegraphics[width=.3\textwidth]{\figpaper/SBMReg_results_global_VarParm1_M1000_prior_posteriorP.pdf} 
  \end{tabular}
  
  \bigskip \bigskip \pause
  Empirical level of 95\%-credibility intervals (CI):
  $$
  \text{VB: } 84.75\%, \qquad \text{SMC: } 93.75\%
  $$
}
  
%====================================================================
\frame{\frametitle{\SBMreg: Model selection}

  For each sample, compute 
  $$
  p_{SMC}(K|Y) = \widehat{Z}_H, \qquad p_{VB}(K|Y) = \pt_Y(K)
  $$
  $\widehat{K}_{SMC} = \arg\max_K p_{SMC}(K|Y)$, idem $\widehat{K}_{VB}$
  
  \bigskip \bigskip \pause
  \paragraph{Results.}
  $$
  \footnotesize{
  \begin{tabular}{ll|cc|cc}
  & & 
  \multicolumn{2}{c|}{$\widehat{K} = K^*$} & 
  \multicolumn{2}{c}{mean $p(K^* | Y)$} \\
  $n$ & $g^*$ & VB & SMC & VB & SMC \\
  \hline
  20 & 1 & 1.00 & 0.46 & 0.947 & 0.435 \\ 
  20 & 2 & 0.10 & 0.23 & 0.138 & 0.257 \\ 
  50 & 1 & 1.00 & 0.60 & 0.982 & 0.562 \\ 
  50 & 2 & 0.42 & 0.36 & 0.410 & 0.387 
  \end{tabular}
  }
  $$ 
  
  \bigskip
  \ra Better performances for VB...
}

%====================================================================
\frame{\frametitle{\SBMreg: Model averaging}

  \paragraph{Account for model uncertainty \refer{HMR99}:} Rather than choosing $\widehat{K}$, consider
  \begin{eqnarray*}
  p(\theta | Y) & = & \sum_K p(K|Y) p(\theta | Y, K) \\
  \Rightarrow \qquad \Var(\theta | Y) & = & \underset{\text{within models}}{\underbrace{\Esp_{K|Y} \left[\Var(\theta | Y, K)\right]}} + \underset{\text{between models}}{\underbrace{\Var_{K|Y} \left[\Esp(\theta|Y, K)\right]}}
  \end{eqnarray*}
  
  \bigskip \pause
  \paragraph{Results.}
  \begin{tabular}{cc}
   \begin{tabular}{p{.5\textwidth}}
    Empirical level of 95\%-CI:
    $$
    \text{VB: } 85.8\%
    $$
    $$
    \text{SMC: } 93.25\%
    $$
   \end{tabular}
   & 
   \hspace{-.02\textwidth}
   \begin{tabular}{p{.5\textwidth}}
   \includegraphics[width=.35\textwidth]{\figpaper/SBMReg_results_global_VarParm1_M1000_prior_posteriorP_BMA.pdf} 
   \end{tabular}
  \end{tabular}
}
  
%====================================================================
%====================================================================
\section{Illustrations}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}
%====================================================================
\frame{\frametitle{Tree network}

  \paragraph{Covariates:} $x_{ij} =$ genetic, geographic and taxonomic distances
  
  \bigskip \bigskip 
  \paragraph{Posterior distribution} of the regression coefficients
  $$
  \footnotesize{
    \begin{tabular}{l|ccc|ccc}
    & \multicolumn{3}{c|}{VB} & \multicolumn{3}{c}{SMC} \\
    & genet. & geo. & taxo. & genet. & geo. & taxo. \\ \hline
    mean & $4.6 \; 10^{-5}$ & $2.3 \; 10^{-1}$ & $-9.0 \; 10^{-1}$ & $4.1 \; 10^{-5}$ & $3.6 \; 10^{-1}$ & $-9.1 \; 10^{-1}$ \\ 
    within var. & $2.2 \; 10^{-10}$ & $4.3 \; 10^{-2}$ & \emphase{$1.7 \; 10^{-3}$} & $1.1 \; 10^{-9}$ & $2.2 \; 10^{-1}$ & \emphase{$8.9 \; 10^{-3}$} \\ 
    between var. & $5.6 \; 10^{-17}$ & $1.2 \; 10^{-6}$ & \emphase{$2.4 \; 10^{-7}$} & $4.0 \; 10^{-12}$ & $1.9 \; 10^{-3}$ & \emphase{$2.8 \; 10^{-3}$} \\ 
    st. dev. & \emphase{$1.5 \; 10^{-5}$} & \emphase{$2.1 \; 10^{-1}$} & \emphase{$4.2 \; 10^{-2}$} & \emphase{$3.3 \; 10^{-5}$} & \emphase{$4.7 \; 10^{-1}$} & \emphase{$1.1 \; 10^{-1}$} \\ 
    ratio &  \emphase{$3.1$} & $1.1$ & $-21$ & \emphase{$1.2$} & $7.6 \; 10^{-1}$ & $-8.4$
    \end{tabular}
    }
  $$  
  \begin{itemize}
   \item Smaller posterior between-model variance with VB 
   \item Smaller posterior variance with VB 
   \item Can affect the conclusions in terms of significance
  \end{itemize}
}
  
%====================================================================
\frame{\frametitle{Residual structure}

  Following \refer{LRO15}, 
  $$
  P(K=1 |Y) = P(\text{no residual structure} | Y)
  $$ 
  measures the goodness-of-fit of the regression model

  \bigskip \bigskip 
  \paragraph{Some examples.} 
  $$
  \footnotesize{
    \begin{tabular}{lccccc}
    Network & Marriage & Business & Karate & Tree & Blog \\ \hline
    $n$ & 16 & 16 & 34 & 51 & 196 \\
    $d$ & 3 & 3 & 8 & 3 & 3 \\
    $p_{VB}(K=1|Y)$ & $9.54 \; 10^{-1}$ & $7.04 \; 10^{-1}$ & $2.56 \; 10^{-1}$ & $4.83 \; 10^{-153}$ & $8.63 \; 10^{-174}$ \\
    $p_{SMC}(K=1|Y)$ & $1.00$ & $1.00$ & $7.07 \; 10^{-3}$ & $1.06 \; 10^{-161}$ & $4.04 \; 10^{-290}$   
    \end{tabular}
  }
  $$
  \begin{itemize}
   \item Similar conclusions with VB and SMC
   \item But the estimated residual graphon may be different
  \end{itemize}
}
  
%====================================================================
%====================================================================
\section{Discussion}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}
%====================================================================
\frame{\frametitle{Conclusion}

  \paragraph{Summary.}
  \begin{itemize}
   \item A generic framework to get an exact sample from the posterior
   \item Taking advantage of fast preliminary inference (VB, ML, ...)
   \item No convergence issue (as opposed to MCMC)
  \end{itemize}

  \bigskip \bigskip \pause
  \paragraph{Some limitations.}
  \begin{itemize}
   \item Large number of iterations when starting far from the target
   \item Requires a model-specific Gibbs sampler
   \item Suffer general issues in Bayesian inference (e.g. label switching)
  \end{itemize}


}
  
%====================================================================
\frame{ \frametitle{References}
{\tiny
  \bibliography{/home/robin/Biblio/BibGene}
%   \bibliographystyle{/home/robin/LATEX/Biblio/astats}
  \bibliographystyle{alpha}
  }
}


%====================================================================
%====================================================================
\end{document}
%====================================================================
%====================================================================

