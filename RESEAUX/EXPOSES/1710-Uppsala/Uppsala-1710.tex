\documentclass[10pt]{beamer}

% Beamer style
%\usetheme[secheader]{Madrid}
% \usetheme{CambridgeUS}
\useoutertheme{infolines}
\usecolortheme[rgb={0.65,0.15,0.25}]{structure}
% \usefonttheme[onlymath]{serif}
\beamertemplatenavigationsymbolsempty
%\AtBeginSubsection

% Packages
%\usepackage[french]{babel}
\usepackage[latin1]{inputenc}
\usepackage{color}
\usepackage{xspace}
\usepackage{dsfont, stmaryrd}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{epsfig}
\usepackage{tikz}
\usepackage{url}
\usepackage{/home/robin/LATEX/Biblio/astats}
%\usepackage[all]{xy}
\usepackage{graphicx}

% Not numbering backup slides
\newcommand{\backupbegin}{
   \newcounter{finalframe}
   \setcounter{finalframe}{\value{framenumber}}
}
\newcommand{\backupend}{
   \setcounter{framenumber}{\value{finalframe}}
}

% Commands
\input{TikZcommands.tex}
\definecolor{darkred}{rgb}{0.65,0.15,0.25}
\newcommand{\emphase}[1]{\textcolor{darkred}{#1}}
% \newcommand{\emphase}[1]{{#1}}
\newcommand{\paragraph}[1]{\textcolor{darkred}{#1}}
\newcommand{\refer}[1]{{\small{\textcolor{gray}{{[\cite{#1}]}}}}}
% \newcommand{\Refer}[1]{{\small{\textcolor{gray}{{[#1]}}}}}
\renewcommand{\newblock}{}

% Symbols
\newcommand{\Abf}{{\bf A}}
\newcommand{\Beta}{\text{B}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\BIC}{\text{BIC}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\dd}{\text{~d}}
\newcommand{\dbf}{{\bf d}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Esp}{\mathbb{E}}
\newcommand{\Ebf}{{\bf E}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Gam}{\mathcal{G}\text{am}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Ibb}{\mathbb{I}}
\newcommand{\Ibf}{{\bf I}}
\newcommand{\ICL}{\text{ICL}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\Corr}{\mathbb{C}\text{orr}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Vsf}{\mathsf{V}}
\newcommand{\pen}{\text{pen}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Hbf}{{\bf H}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kbf}{{\bf K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\mbf}{{\bf m}}
\newcommand{\mum}{\mu(\mbf)}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Nbf}{{\bf N}}
\newcommand{\Nm}{N(\mbf)}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Obf}{{\bf 0}}
\newcommand{\Omegas}{\underset{s}{\Omega}}
\newcommand{\Pbf}{{\bf P}}
\newcommand{\Pt}{\widetilde{P}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\BP}{\text{BP}}
\newcommand{\EM}{\text{EM}}
\newcommand{\VEM}{\text{VEM}}
\newcommand{\VBEM}{\text{VBEM}}
\newcommand{\cst}{\text{cst}}
\newcommand{\obs}{\text{obs}}
\newcommand{\ra}{\emphase{\mathversion{bold}{$\rightarrow$}~}}
%\newcommand{\transp}{\text{{\tiny $\top$}}}
\newcommand{\transp}{\text{{\tiny \mathversion{bold}{$\top$}}}}
\newcommand{\logit}{\text{logit}\xspace}

% Directory
\newcommand{\fignet}{/home/robin/Bureau/RECHERCHE/RESEAUX/EXPOSES/FIGURES}
\newcommand{\figchp}{/home/robin/Bureau/RECHERCHE/RUPTURES/EXPOSES/FIGURES}


%====================================================================
%====================================================================

%====================================================================
%====================================================================
\begin{document}
%====================================================================
%====================================================================

%====================================================================
\title[Detecting change-points in network structure]{Detecting change-points in the structure of a network: Exact Bayesian inference}

\author[S. Robin]{S. Robin \\ ~\\
    Joint work with \underline{L. Schwaller}
  }

\institute[INRA / AgroParisTech]{~ \\%INRA / AgroParisTech \\
  \vspace{-.1\textwidth}
  \begin{tabular}{ccc}
    \includegraphics[height=.25\textheight]{\fignet/LogoINRA-Couleur} & 
    \hspace{.02\textheight} &
    \includegraphics[height=.06\textheight]{\fignet/logagroptechsolo} % & 
%     \hspace{.02\textheight} &
%     \includegraphics[height=.09\textheight]{\fignet/logo-ssb}
    \\ 
  \end{tabular} \\
  \bigskip
  }

\date[Oct. 2017, Uppsala]{Emerging Topics in Biological Networks and Systems Biology,\\ Oct. 2017, Uppsala}

%====================================================================
%====================================================================
\maketitle
%====================================================================

%====================================================================
%====================================================================
\section*{Motivating example}
%====================================================================
\frame{\frametitle{Example: Gene regulatory network along time} 

  \paragraph{Data:} \refer{AFI02}
  $$
  Y_{jt} = \text{expression of   $j$ at time $t$}
  $$
  
  \bigskip \pause
  \paragraph{'Model':} 
  \begin{eqnarray*}
    G_t & = & \text{gene regulatory network at time $t$} \\
    & = & \text{graphical model of $Y_t = (Y_{jt})_j$}
  \end{eqnarray*}

  \bigskip \pause
  \paragraph{Questions:} 
  \begin{itemize}
   \item Is $G_t$ constant along time or is there some 'gene rewiring'? \\ ~
   \item If not, when does it change? \\ ~
   \item And what is the network within each period of time?
  \end{itemize}
}

%====================================================================
\frame{\frametitle{Example of output} 

  \paragraph{Data:} $n = 67$ time points, $p = 11$ genes, four expected regions

  \pause \bigskip
  Posterior probability of change-points:
  $$
  \includegraphics[width=.7\textwidth]{\figchp/ScR16-Fig8-chgpt_EMP_ap10_au1}
  $$
  \pause
  Inferred networks:
  $$
  \includegraphics[width=.8\textwidth]{\figchp/ScR16-Fig9-network_seuil02}
  $$

}

%====================================================================
\frame{\frametitle{Similar problems} 

  \paragraph{Ecology:} 
  $$
  Y_{jt} = \text{abundance of species $j$ at time $t$ in a given medium}
  $$
  $G_t =$ interaction structure between species at time $t$. \\ ~
  \begin{itemize}
   \item Time-evolving species interaction network?
  \end{itemize}

  \pause \bigskip \bigskip
  \paragraph{Neuroscience:} 
  $$
  Y_{jt} = \text{activity of brain region $j$ at time $t$}
  $$
  $G_t =$ connectivity structure between regions at time $t$. \\ ~
  
  \begin{itemize}
   \item Time-evolving connectivity network?
  \end{itemize}

}

%====================================================================
%====================================================================
\section{Bayesian inference with discrete parameters}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}

%====================================================================
\frame{\frametitle{A reminder on Bayesian inference}

  \paragraph{Typical Bayesian model:}
  \begin{eqnarray*}
   \text{Parameter} \quad \Theta & \sim & p(\Theta) \qquad = \text{{\sl prior} distribution} \\
   \text{Data} \quad Y & \sim & p(Y | \Theta) \quad = \text{\sl likelihood} \\ ~\\
   \text{\emphase{Aim:} evaluate} & & p(\Theta | Y) \quad = \text{{\sl posterior} distribution}
  \end{eqnarray*}
  where
  $$
  p(\Theta | Y) = p(\Theta) p(Y | \Theta) / p(Y)
  $$

  
  \bigskip \bigskip 
  Requires to integrate over the whole parameter space:
  $$
  p(Y) = \int p(\Theta) p(Y | \Theta) \dd \Theta
  $$
}

%====================================================================
\frame{\frametitle{Bayesian inference with mixed parameters}

  \paragraph{Mixed parameter:} $\Theta = (\theta, T)$
  \begin{itemize}
   \item $\theta =$ (means, variances, correlations): \emphase{continuous} parms, 
   \item $T =$ (segmentation, graph): \emphase{discrete} parms, 
  \end{itemize}
  
  \bigskip \bigskip \pause
  Even if $\int \text{d} \theta$ raises no issue, we are left with 
  $$
  p(Y) 
  = \emphase{\sum_{T \in \Tcal}} \int p(Y, \theta, T) \dd \theta 
  = \emphase{\sum_{T \in \Tcal}} p(Y, T) 
  $$
  \begin{itemize}
   \item Intractable when $\# \Tcal$ grows (super-)exponentially with $n$ or $p$
%    \item Need to find algorithmic or algebraic shortcuts
  \end{itemize}
}

%====================================================================
\frame{\frametitle{Bayesian inference with discrete parameters}

  \paragraph{Examples.}
  \begin{itemize}
   \item Change-point detection:    
   \item Network inference = structure inference
   \item Combination of both
  \end{itemize}
  
  \bigskip \bigskip \pause
  \paragraph{Main approaches for Bayesian inference.}
  \begin{itemize}
   \item Stochastic: Monte-Carlo sampling, MCMC, SMC, ...
   \item Approximation: variational Bayes, INLA, ...
   \item Hard headed: 'exact' computation
  \end{itemize}

}

%====================================================================
%====================================================================
\section{Change-point detection}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}

%====================================================================
\subsection*{Change-point detection model}
%====================================================================
\frame{\frametitle{A change-point detection model} 

  \paragraph{Segmentation $T$ =} set of adjacent segments. 
  $\Tcal^K = \Tcal^K_{1:n}$ set of all possible segmentations. 
  $$
  \begin{overprint}
    \onslide<1>
	 \begin{tikzpicture}
	   \node[empty] (Y1lu) at (-.25*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (Y1lb) at (-.25*\edgeunit, -.5*\edgeunit) {};
	   \node[observed] (Y1l) at (0*\edgeunit, 0) {$Y_1$};
	   \node[empty] (Y1m) at (.75*\edgeunit, 0) {};
	   \node[observed] (Y1r) at (1.5*\edgeunit, 0) {$Y_s$};
	   \node[empty] (Y1ru) at (1.75*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (Y1rb) at (1.75*\edgeunit, -.5*\edgeunit) {};
	   
	   \node[empty] (Yrlu) at (2.75*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (Yrlb) at (2.75*\edgeunit, -.5*\edgeunit) {};
	   \node[observed] (Yrl) at (3*\edgeunit, 0) {$Y_t$};
	   \node[empty] (Yrm) at (3.5*\edgeunit, 0) {};
	   \node[observed] (Yrr) at (4*\edgeunit, 0) {$Y_u$};
	   \node[empty] (Yrru) at (4.25*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (Yrrb) at (4.25*\edgeunit, -.5*\edgeunit) {};

	   \node[empty] (YKlu) at (5.25*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (YKlb) at (5.25*\edgeunit, -.5*\edgeunit) {};
	   \node[observed] (YKl) at (5.5*\edgeunit, 0) {$Y_v$};
	   \node[empty] (YKm) at (6.5*\edgeunit, 0) {};
	   \node[observed] (YKr) at (7.5*\edgeunit, 0) {$Y_n$};
	   \node[empty] (YKru) at (7.75*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (YKrb) at (7.75*\edgeunit, -.5*\edgeunit) {};

	   \draw[edge] (Y1lu) to (Y1lb); \draw[dashededge] (Y1l) to (Y1r); 
	   \draw[edge] (Y1ru) to (Y1rb); \draw[dashededge] (Y1r) to (Yrl); 
	   \draw[edge] (Yrlu) to (Yrlb); \draw[dashededge] (Yrl) to (Yrr); 
	   \draw[edge] (Yrru) to (Yrrb); \draw[dashededge] (Yrr) to (YKl); 
	   \draw[edge] (YKlu) to (YKlb); \draw[dashededge] (YKl) to (YKr); 
	   \draw[edge] (YKru) to (YKrb); 
	 \end{tikzpicture}
    \onslide<2->
	 \begin{tikzpicture}
	   \node[hidden] (theta1) at (.75*\edgeunit, \edgeunit) {$\theta_1$};
	   \node[hidden] (thetar) at (3.5*\edgeunit, \edgeunit) {$\theta_r$};
	   \node[hidden] (thetaK) at (6.5*\edgeunit, \edgeunit) {$\theta_K$};

	   \node[empty] (Y1lu) at (-.25*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (Y1lb) at (-.25*\edgeunit, -.5*\edgeunit) {};
	   \node[observed] (Y1l) at (0*\edgeunit, 0) {$Y_1$};
	   \node[empty] (Y1m) at (.75*\edgeunit, 0) {};
	   \node[observed] (Y1r) at (1.5*\edgeunit, 0) {$Y_s$};
	   \node[empty] (Y1ru) at (1.75*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (Y1rb) at (1.75*\edgeunit, -.5*\edgeunit) {};
	   
	   \node[empty] (Yrlu) at (2.75*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (Yrlb) at (2.75*\edgeunit, -.5*\edgeunit) {};
	   \node[observed] (Yrl) at (3*\edgeunit, 0) {$Y_t$};
	   \node[empty] (Yrm) at (3.5*\edgeunit, 0) {};
	   \node[observed] (Yrr) at (4*\edgeunit, 0) {$Y_u$};
	   \node[empty] (Yrru) at (4.25*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (Yrrb) at (4.25*\edgeunit, -.5*\edgeunit) {};

	   \node[empty] (YKlu) at (5.25*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (YKlb) at (5.25*\edgeunit, -.5*\edgeunit) {};
	   \node[observed] (YKl) at (5.5*\edgeunit, 0) {$Y_v$};
	   \node[empty] (YKm) at (6.5*\edgeunit, 0) {};
	   \node[observed] (YKr) at (7.5*\edgeunit, 0) {$Y_n$};
	   \node[empty] (YKru) at (7.75*\edgeunit, 1.5*\edgeunit) {};
	   \node[empty] (YKrb) at (7.75*\edgeunit, -.5*\edgeunit) {};

	   \draw[edge] (Y1lu) to (Y1lb); \draw[dashededge] (Y1l) to (Y1r); 
	   \draw[edge] (Y1ru) to (Y1rb); \draw[dashededge] (Y1r) to (Yrl); 
	   \draw[edge] (Yrlu) to (Yrlb); \draw[dashededge] (Yrl) to (Yrr); 
	   \draw[edge] (Yrru) to (Yrrb); \draw[dashededge] (Yrr) to (YKl); 
	   \draw[edge] (YKlu) to (YKlb); \draw[dashededge] (YKl) to (YKr); 
	   \draw[edge] (YKru) to (YKrb); 

	   \draw[arrow] (theta1) to (Y1l); \draw[arrow] (theta1) to (Y1r); \draw[dashedarrow] (theta1) to (Y1m); 
	   \draw[arrow] (thetar) to (Yrl); \draw[arrow] (thetar) to (Yrr); \draw[dashedarrow] (thetar) to (Yrm); 
	   \draw[arrow] (thetaK) to (YKl); \draw[arrow] (thetaK) to (YKr); \draw[dashedarrow] (thetaK) to (YKm); 
	 \end{tikzpicture}
  \end{overprint}
  $$
  \onslide+<3->{
    \vspace{-.05\textheight}
    $$
    p(Y \, | \, T) % & = & \prod_{r \in T} \underbrace{\int p(\theta_r) \prod_{t \in r} p(Y_t \, | \, \theta_r) \dd \theta_r} \\
% 	 & = & \prod_{r \in T} \qquad \qquad p(Y^r), \qquad \qquad Y^r = (Y_t)_{t \in r}
	 = \prod_{r \in T} \int p(Y^r | \theta_r) p(\theta_r) \dd \theta_r %, \qquad \qquad Y^r = (Y_t)_{t \in r}
	 = \prod_{r \in T} p(Y^r), \qquad \qquad Y^r = (Y_t)_{t \in r}
    $$
    }
    
  \onslide+<4>{
    \paragraph{Segmentation space:}
    $$
    \# \Tcal_{1:n}^K = {{n-1}\choose{K-1}} \approx (n/K)^K
    $$
    }
  }
  
%====================================================================
\frame{ \frametitle{Some quantities of interest}

  Under mild assumptions (incl. $p(T) \propto \prod_{r \in T} a_r$).

  \bigskip \bigskip 
  \paragraph{Marginal likelihood:}
  $$
  p(Y\, | \,K) = \sum_{T \in \Tcal^K} p(T\, | \,K) p(Y\, | \,T) \propto \sum_{T \in \Tcal^K} \prod_{r \in T} a_r p(Y^r)
  $$
  with normalizing constant $\sum_{T \in \Tcal^K} \prod_{r \in T} a_r$.

  \pause \bigskip \bigskip 
  \paragraph{Posterior distribution of a change-point.}
  $$
  \Pr\{\tau_k = t \, | \, Y, K\} \propto \left(\sum_{T \in \Tcal^k_{1:t}} \prod_{r \in T} a_r p(Y^r) \right) \left(\sum_{T \in \Tcal^{K-k}_{t+1:n}} \prod_{r \in T} a_r p(Y^r) \right)
  $$
}

%====================================================================
\frame{ \frametitle{Some simple algebra}

  Computing the sum of 
  $$
  f_{1, \tau_1} \times f_{\tau_1+1, \tau_2} \times \dots \times f_{\tau_{K-1}+1, n}
  $$
  for all $1 \leq \tau_1 < \tau_2 < \dots < \tau_{K-1} < n$ is the same as ... 
  
  \bigskip \pause 
  Computing the $K$-th power of the $(n+1)\times(n+1)$ upper-triangular matrix (with zero diagonal) $A$:
  $$
  [A]_{i, j+1} = f_{i, j} 
  \qquad \Rightarrow \qquad 
  \sum_{T \in \Tcal^K_{1:n}} \prod_{r \in T} f_r = \left[A^K\right]_{1,n+1} 
  $$
  
  \bigskip \pause
  \begin{itemize}
  \item All terms are computed in $O(K n^2)$. 
  \item To compute $p(Y\, | \,K)$, take $f_r = a_r p(Y^r)$. 
  %    \item \pause 'sum-product' = counterpart of 'max-sum' in the dynamic programming algorithm.
  \item Similar ideas in \refer{Fea06}. 
  \item R package EBS (exact Bayesian segmentation) %\Refer{Cleynen and R. (2014)} \nocite{ClR14} 
  \refer{ClR14}
  \end{itemize}
  }

%====================================================================
\frame{\frametitle{Gene regulatory network} 

  \paragraph{Data:} $n = 67$ time points, $p = 11$ genes, four expected regions \refer{AFI02}

  \bigskip
  \paragraph{Model:} 
  $$
  t \in r: \qquad Y_t \, | \, \theta_r = (\mu_r, \Sigma_r) \sim \Ncal(\mu_r, \Sigma_r)
  $$
  \ra Saturated graphical model.
  
  \pause \bigskip \bigskip
  Posterior probability of change-points: \emphase{dotted line}
  $$
  \includegraphics[width=.7\textwidth]{\figchp/ScR16-Fig8-chgpt_EMP_ap10_au1}
  $$
}

%====================================================================
%====================================================================
\section{Network inference}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}
%====================================================================
\frame{\frametitle{A reminder on (undirected) graphical model} 

  \begin{tabular}{cc}
    \begin{tabular}{p{.5\textwidth}}
      \begin{tikzpicture}
      \node[observed] (Y1) at (0, 0) {$Y_1$};
      \node[observed] (Y2) at (\edgeunit, 0) {$Y_2$};
      \node[observed] (Y3) at (.5*\edgeunit, .87*\edgeunit) {$Y_3$};
      \node[observed] (Y4) at (-.5*\edgeunit, -.87*\edgeunit) {$Y_4$};
      \node[observed] (Y5) at (.5*\edgeunit, -.87*\edgeunit) {$Y_5$};
      \node[observed] (Y8) at (-.5*\edgeunit, .87*\edgeunit) {$Y_8$};
      \node[observed] (Y6) at (1.5*\edgeunit, -.87*\edgeunit) {$Y_6$};
      
      \draw[edge] (Y1) to (Y2);  \draw[edge] (Y1) to (Y3);  \draw[edge] (Y2) to (Y3);
      \draw[edge] (Y1) to (Y4);  \draw[edge] (Y1) to (Y5);
      \draw[edge] (Y2) to (Y6);  \draw[edge] (Y3) to (Y8);
      \end{tikzpicture} \\~
    \end{tabular}
    & 
    \hspace{-.2\textwidth}
    \begin{tabular}{p{.5\textwidth}}
    Means that
    \begin{eqnarray*}
      p(Y_1, \dots, Y_8) & \propto & \psi_1(Y_1, Y_2, Y_3) \\
      &  & \psi_2(Y_1, Y_4) \ \psi_3(Y_1, Y_5) \\
      &  & \psi_4(Y_2, Y_6) \ \psi_5(Y_3, Y_8)
    \end{eqnarray*}
    which implies %\footnote{Under fairly general assumptions on $p$} 
    that
    \begin{eqnarray*}
     Y_4 \perp Y_3 & | & Y_1 \\
     (Y_6, Y_7) \perp Y_3 & | & Y_2 \\
     \dots
    \end{eqnarray*}
    \end{tabular}
  \end{tabular}
  
  \pause
  \begin{itemize}
   \item $G$ reveals the structure of conditional independences between the variables $Y_1, \dots Y_p$
   \item 'Network inference' problem: Based on $\{(Y_{i1}, \dots Y_{ip})\}_i$ iid $\sim p$, infer $G$.
  \end{itemize}
}

%====================================================================
\frame{\frametitle{Tree-structures network} 

  \paragraph{Tree assumption:} the graph $G$ is a spanning tree $T$.
  
  \begin{tabular}{cc}
    \hspace{.1\textwidth}
    \begin{tabular}{p{.5\textwidth}}
      \begin{tikzpicture}
      \node[observed] (Y1) at (0, 0) {$Y_1$};
      \node[observed] (Y2) at (\edgeunit, 0) {$Y_2$};
      \node[observed] (Y3) at (.5*\edgeunit, .87*\edgeunit) {$Y_3$};
      \node[observed] (Y4) at (-.5*\edgeunit, -.87*\edgeunit) {$Y_4$};
      \node[observed] (Y5) at (.5*\edgeunit, -.87*\edgeunit) {$Y_5$};
      \node[observed] (Y8) at (-.5*\edgeunit, .87*\edgeunit) {$Y_8$};
      \node[observed] (Y6) at (1.5*\edgeunit, -.87*\edgeunit) {$Y_6$};
      \draw[dashededge] (Y1) to (Y2);  \draw[dashededge] (Y1) to (Y3);  \draw[dashededge] (Y2) to (Y3);
      \draw[edge] (Y1) to (Y4);  \draw[edge] (Y1) to (Y5);
      \draw[edge] (Y2) to (Y6);  \draw[edge] (Y3) to (Y8);
      \end{tikzpicture}
    \end{tabular}
    & 
    \hspace{-.2\textwidth}
    \begin{tabular}{p{.5\textwidth}}
	 \begin{overprint}
	  \onslide<1>
	   \begin{tikzpicture}
	   \node[observed] (Y1) at (0, 0) {$Y_1$};
	   \node[observed] (Y2) at (\edgeunit, 0) {$Y_2$};
	   \node[observed] (Y3) at (.5*\edgeunit, .87*\edgeunit) {$Y_3$};
	   \node[observed] (Y4) at (-.5*\edgeunit, -.87*\edgeunit) {$Y_4$};
	   \node[observed] (Y5) at (.5*\edgeunit, -.87*\edgeunit) {$Y_5$};
	   \node[observed] (Y8) at (-.5*\edgeunit, .87*\edgeunit) {$Y_8$};
	   \node[observed] (Y6) at (1.5*\edgeunit, -.87*\edgeunit) {$Y_6$};
	   \draw[edge] (Y1) to (Y3);  \draw[edge] (Y2) to (Y3);
	   \draw[edge] (Y1) to (Y4);  \draw[edge] (Y1) to (Y5);
	   \draw[edge] (Y2) to (Y6);  \draw[edge] (Y3) to (Y8);
	   \end{tikzpicture}
	  \onslide<2>
	   \begin{tikzpicture}
	   \node[observed] (Y1) at (0, 0) {$Y_1$};
	   \node[observed] (Y2) at (\edgeunit, 0) {$Y_2$};
	   \node[observed] (Y3) at (.5*\edgeunit, .87*\edgeunit) {$Y_3$};
	   \node[observed] (Y4) at (-.5*\edgeunit, -.87*\edgeunit) {$Y_4$};
	   \node[observed] (Y5) at (.5*\edgeunit, -.87*\edgeunit) {$Y_5$};
	   \node[observed] (Y8) at (-.5*\edgeunit, .87*\edgeunit) {$Y_8$};
	   \node[observed] (Y6) at (1.5*\edgeunit, -.87*\edgeunit) {$Y_6$};
	   \draw[edge] (Y1) to (Y2);  \draw[edge] (Y2) to (Y3);
	   \draw[edge] (Y1) to (Y4);  \draw[edge] (Y1) to (Y5);
	   \draw[edge] (Y2) to (Y6);  \draw[edge] (Y3) to (Y8);
	   \end{tikzpicture}
  	  \onslide<3->
	   \begin{tikzpicture}
	   \node[observed] (Y1) at (0, 0) {$Y_1$};
	   \node[observed] (Y2) at (\edgeunit, 0) {$Y_2$};
	   \node[observed] (Y3) at (.5*\edgeunit, .87*\edgeunit) {$Y_3$};
	   \node[observed] (Y4) at (-.5*\edgeunit, -.87*\edgeunit) {$Y_4$};
	   \node[observed] (Y5) at (.5*\edgeunit, -.87*\edgeunit) {$Y_5$};
	   \node[observed] (Y8) at (-.5*\edgeunit, .87*\edgeunit) {$Y_8$};
	   \node[observed] (Y6) at (1.5*\edgeunit, -.87*\edgeunit) {$Y_6$};
	   \draw[edge] (Y1) to (Y2);  \draw[edge] (Y1) to (Y3);  
	   \draw[edge] (Y1) to (Y4);  \draw[edge] (Y1) to (Y5);
	   \draw[edge] (Y2) to (Y6);  \draw[edge] (Y3) to (Y8);
	   \end{tikzpicture}
	  \end{overprint}
    \end{tabular}
  \end{tabular}
  
  \onslide+<4>{
  \bigskip \bigskip 
%     \paragraph{Tree structure assumption.}
    \begin{itemize}
    \item Consistent with the usual assumption that the graph is sparse (although much stronger).
    \\~
    \item Not true in general, but may be sufficient for the \emphase{inference on local
    structures}, such as the existence of a given edge.
    \end{itemize}
    }
    
}

%====================================================================
\frame{\frametitle{Bayesian inference for tree-structured network \refer{SRS15}} 

  $p(Y\, | \,T)$ Markov wrt $T$
%   \begin{eqnarray*}
%    p(Y\, | \,T) 
%    & = & \prod_j p(Y_j) \prod_{(j, k) \in T} \frac{p(Y_j, Y_k)}{p(Y_j)p(Y_k)} \\
%    & = & \prod_{(j, k) \in T} p(Y_j, Y_k) \left/ \prod_j p^{d_j-1}(Y_j) \right.
%   \end{eqnarray*}
  $$
   p(Y\, | \,T) 
   = \prod_j p(Y_j) \prod_{(j, k) \in T} \frac{p(Y_j, Y_k)}{p(Y_j)p(Y_k)}
   \propto \prod_{(j, k) \in T} \psi_{jk}
  $$
  where $d_j$ is the degree (number of neighbors in $T$) of node $j$.
  
  \bigskip \bigskip \pause
  \paragraph{Assumptions:}
  \begin{itemize}
   \item The prior on $T$ factorizes over the edges: $p(T) \propto \prod_{(j, k) \in T} b_{jk}$
   \item The prior on $\theta$ is hyper-Markov (see \refer{DaL93})
   \item Holds for multinomial-Dirichlet, normal-Wishart, Gaussian copulas.
  \end{itemize}


}

%====================================================================
\frame{\frametitle{Quantities of interest}

  \paragraph{Marginal distribution.}
  $$
  p(Y) 
  = \sum_{T \in \Tcal} p(T) p(Y\, | \,T)
  \propto 
%   \sum_{T \in \Tcal} \prod_{j, k} \frac{b_{jk} \int p(Y_j, Y_k, \theta_{jk}) \dd \theta_{jk}}{\int p(Y_j, \theta_j) \dd \theta_j \times \int p(Y_k, \theta_k) \dd \theta_k}
%   \sum_{T \in \Tcal} \prod_{j, k} \frac{b_{jk} p(Y_j, Y_k)}{p(Y_j) p(Y_k)}
  \sum_{T \in \Tcal} \prod_{j, k} b_{jk} \psi_{jk}
  $$
  where $\Tcal$ stands for the set of all spanning trees. 
  
  \bigskip \bigskip \pause
  \paragraph{Posterior probability for an edge to be absent.}
  $$
  \Pr\{(j, k) \notin T \, | \,Y\} \propto 
%   \sum_{T \in \Tcal: (j, k) \notin T} \prod_{j, k} \frac{b_{jk} \int p(Y_j, Y_k, \theta_{jk}) \dd \theta_{jk}}{\int p(Y_j, \theta_j) \dd \theta_j \times \int p(Y_k, \theta_k) \dd \theta_k}
  \sum_{T \in \Tcal: (j, k) \notin T} \prod_{j, k} b_{jk} \psi_{jk}
  $$

  \bigskip \pause
  \paragraph{Typical form:} 
  $$
  \sum_{T \in \Tcal} \prod_{(j, k) \in T} f_{jk},
  \qquad \qquad \text{with} \quad \# \Tcal = p^{p-2}.
  $$

}

%====================================================================
\frame{\frametitle{Summing over spanning trees} 

  \paragraph{Matrix-tree theorem.} \refer{Cha82} {\sl
  \begin{itemize}
   \item $F = [f_{jk}]$: a symmetric matrix with $f(j, j) = 0, f_{jk} > 0$;
   \item $\Delta = [\Delta_{jk}]$ its Laplacian: $\Delta_{jj} = \sum_k f_{jk}, \Delta_{jk} = -f_{jk}$. 
  \end{itemize} \pause
  Then the minors $|\Delta^{uv}|$ of $\Delta$ are equal and
  $$
  |\Delta^{uv}| = \sum_{T \in \Tcal} \prod_{(j, k) \in T} f_{jk}.
  $$}
  
  \pause \bigskip
  \begin{itemize}
   \item Quantities of interest can be computed at the cost of computing a determinant, ie \emphase{$O(p^3)$}. 
   \item Already used in \refer{MeJ06,Kir07} for tree learning.
%    \item Again 'sum-product' in place of 'max-sum'.
   \item For edge probability, set $f_{jk} = 0$ (see \refer{Kir07})
   \item R package Saturnin (spanning trees used for network inference) \refer{SRS15}
  \end{itemize}
}

%====================================================================
\frame{\frametitle{Tree averaging} 

%   \begin{tabular}{cc}
%     \begin{tabular}{p{.3\textwidth}}
% %     \paragraph{Sum over all trees}
%     \end{tabular}
%     & 
%     \hspace{-.1\textwidth}
    \begin{tabular}{p{.4\textwidth}}
      \begin{tabular}{cccc}
	   \begin{tabular}{c}
		\begin{tikzpicture}
		\node[observed] (Y1) at (0*\edgeunit, 0*\edgeunit) {$Y_1$};
		\node[observed] (Y2) at (1*\edgeunit, 0*\edgeunit) {$Y_2$};
		\node[observed] (Y3) at (1*\edgeunit, 1*\edgeunit) {$Y_3$};
		\node[observed] (Y4) at (0*\edgeunit, 1*\edgeunit) {$Y_4$};
		\draw[edge] (Y1) to (Y2); \draw[edge] (Y1) to (Y3); \draw[edge] (Y2) to (Y4); 
		\end{tikzpicture} \\
		\footnotesize{$P\{T = T_1 | Y\}$}
	   \end{tabular}
	   & 
	   \hspace{-.05\textwidth} \pause
	   \begin{tabular}{c}
		\begin{tikzpicture}
		\node[observed] (Y1) at (0*\edgeunit, 0*\edgeunit) {$Y_1$};
		\node[observed] (Y2) at (1*\edgeunit, 0*\edgeunit) {$Y_2$};
		\node[observed] (Y3) at (1*\edgeunit, 1*\edgeunit) {$Y_3$};
		\node[observed] (Y4) at (0*\edgeunit, 1*\edgeunit) {$Y_4$};
		\draw[edge] (Y1) to (Y2); \draw[edge] (Y1) to (Y3); \draw[edge] (Y1) to (Y4); 
		\end{tikzpicture} \\
		\footnotesize{$P\{T = T_2 | Y\}$}
	   \end{tabular}
	   &
	   \hspace{-.05\textwidth} \pause
	   \begin{tabular}{c}
		\begin{tikzpicture}
		\node[observed] (Y1) at (0*\edgeunit, 0*\edgeunit) {$Y_1$};
		\node[observed] (Y2) at (1*\edgeunit, 0*\edgeunit) {$Y_2$};
		\node[observed] (Y3) at (1*\edgeunit, 1*\edgeunit) {$Y_3$};
		\node[observed] (Y4) at (0*\edgeunit, 1*\edgeunit) {$Y_4$};
		\draw[edge] (Y1) to (Y2); \draw[edge] (Y2) to (Y3); \draw[edge] (Y2) to (Y4); 
		\end{tikzpicture}\\
		\footnotesize{$P\{T = T_3 | Y\}$}
	   \end{tabular}
	   &
	   \hspace{-.05\textwidth} \pause
	   \begin{tabular}{c}
		\begin{tikzpicture}
		\node[observed] (Y1) at (0*\edgeunit, 0*\edgeunit) {$Y_1$};
		\node[observed] (Y2) at (1*\edgeunit, 0*\edgeunit) {$Y_2$};
		\node[observed] (Y3) at (1*\edgeunit, 1*\edgeunit) {$Y_3$};
		\node[observed] (Y4) at (0*\edgeunit, 1*\edgeunit) {$Y_4$};
		\draw[edge] (Y1) to (Y2); \draw[edge] (Y2) to (Y4); \draw[edge] (Y3) to (Y4); 
		\end{tikzpicture} \\
		\footnotesize{$P\{T = T_4 | Y\}$}
	   \end{tabular}
	   \\ \\
	   \\ \pause
	   \begin{tabular}{l}
		Edge posterior\\
		probabilities:
	   \end{tabular}
	   &
	   \hspace{-.05\textwidth}
	   \begin{tabular}{c}
		\begin{tikzpicture}
		\node[observed] (Y1) at (0*\edgeunit, 0*\edgeunit) {$Y_1$};
		\node[observed] (Y2) at (1*\edgeunit, 0*\edgeunit) {$Y_2$};
		\node[observed] (Y3) at (1*\edgeunit, 1*\edgeunit) {$Y_3$};
		\node[observed] (Y4) at (0*\edgeunit, 1*\edgeunit) {$Y_4$};
		\draw [line width=5pt] (Y1) -- (Y2); 
		\draw [line width=3pt] (Y1) -- (Y3); 
		\draw [line width=.5pt] (Y1) -- (Y4); 
		\draw [line width=2pt] (Y2) -- (Y3); 
		\draw [line width=.5pt] (Y2) -- (Y4); 
% 		\draw [line width=.5pt] (Y3) -- (Y4); 
		\end{tikzpicture}\\
		\emphase{$P\{(j, k) \in T | Y\}$}
	   \end{tabular}
	   &
	   \hspace{-.05\textwidth} \pause
	   \begin{tabular}{l}
		Thresholding\\
		probabilities:
	   \end{tabular}
	   &
	   \hspace{-.05\textwidth}
	   \begin{tabular}{c}
		\begin{tikzpicture}
		\node[observed] (Y1) at (0*\edgeunit, 0*\edgeunit) {$Y_1$};
		\node[observed] (Y2) at (1*\edgeunit, 0*\edgeunit) {$Y_2$};
		\node[observed] (Y3) at (1*\edgeunit, 1*\edgeunit) {$Y_3$};
		\node[observed] (Y4) at (0*\edgeunit, 1*\edgeunit) {$Y_4$};
		\draw [line width=1pt] (Y1) -- (Y2); 
		\draw [line width=1pt] (Y1) -- (Y3); 
% 		\draw [line width=1pt] (Y1) -- (Y4); 
		\draw [line width=1pt] (Y2) -- (Y3); 
% 		\draw [line width=.1pt] (Y2) -- (Y4); 
% 		\draw [line width=1pt] (Y3) -- (Y4); 
		\end{tikzpicture}\\
		\emphase{$P\{(j, k) \in T | Y\}$}
	   \end{tabular}
	 \end{tabular}
    \end{tabular}
%   \end{tabular}
}

%====================================================================
\frame{\frametitle{Simulations: Comparison with sampling among DAGs} 

\refer{NPK11}: MCMC sampling over the directed acyclic graphs (multinomial case) 
$$
\begin{tabular}{cccc}
\includegraphics[width=0.18\linewidth]{\fignet/boxplot_tree.pdf}
& \includegraphics[width=0.18\linewidth]{\fignet/boxplot_ER2p.pdf}
& \includegraphics[width=0.18\linewidth]{\fignet/boxplot_ER4p.pdf}
& \includegraphics[width=0.18\linewidth]{\fignet/boxplot_ER8p.pdf} \\
Tree & Erd\"{o}s-R\'{e}nyi & Erd\"{o}s-R\'{e}nyi & Erd\"{o}s-R\'{e}nyi \\
& $p_c = 2/p$ & $p_c = 4/p$ &  $p_c = 8/p$ 
\end{tabular} 
$$
Area under the curves: top=ROC, bottom=PR\\
light grey = multinomial trees (\emphase{2.2''}), dark grey: multinomial DAGs (\emphase{1393''}) 

}	

%====================================================================
\frame{\frametitle{Illustration: Raf pathway}

Flow cytometry data for $p = 11$ proteins from the Raf signaling pathway \refer{SPP05} \\ ~

\begin{tabular}{cc}
  \includegraphics[trim = 12mm 35mm 12mm 18mm, clip,width=0.45\linewidth]{\fignet/RAF.pdf}
  & 
  \includegraphics[width=0.45\linewidth]{\fignet/RAF_edge_prob_graph_q0_05.pdf} \\
  'ground truth' & posterior probabilities \\ 
  ~\\
  \includegraphics[width=0.45\linewidth]{\fignet/best_1.pdf} 
  &
  \includegraphics[width=0.45\linewidth]{\fignet/best_2.pdf} \\
  most likely tree & second most likely tree
\end{tabular}

}

%====================================================================
%====================================================================
\section{Detecting changes in a graphical model}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}

%====================================================================
\frame{\frametitle{Change-point in a graphical model} 

  \pause \paragraph{Problem:} \refer{ScR16} %\Refer{Schwaller and R. (2016)} \nocite{ScR16} 
  \begin{itemize}
  \item Consider $p$ variables observed along time; 
  \item Consider the graph $G_t$ supporting the graphical model at time $t$; 
  \item Does the graph $G_t$ remain the same along time? 
  \end{itemize}
  $$
  \includegraphics[height=.3\textheight]{\figchp/ChangePoint-Tree.pdf}
  $$

 \pause \paragraph{Examples:}
  \begin{enumerate}
   \item Gene regulatory network along the {\sl Drosophila} life cycle? \\~
   \item Connections between brain regions along different tasks?
  \end{enumerate}
%   }
  
}

%====================================================================
\frame{\frametitle{Model} 

%   $$
%   \begin{array}{rcll}
%    K & = & \text{number of segments} & p(K); \\ ~\\ \pause
%    R = (r_k)_k & = & \text{segmentation} & p(R|K) \propto \prod_{r \in R} a_r; \\ ~\\ \pause
%    T = (T_r)_r & = & \text{set of trees} & p(T|R) = \prod_{r \in R} p(T_r|R), \\ 
%    T_r & = & \text{graphical model in segment $r$} & \\ ~\\ \pause
%    \theta = (\theta_r)_ r & = & \text{parameter in each segment} & p(\theta | T) = \prod_{r \in R} p(\theta_r | T_r) \\
%    & & \text{typically: support$(\Sigma_r^{-1}) = T_r$} \\ ~\\ \pause 
%    Y_t = (Y_{jt}) & = & \text{data collected at time $t$} & p(Y | R, \theta) = \prod_{r \in R} \prod_{t \in r} p(Y_t | \theta_r) 
%   \end{array}
%   $$
  
  $$
  \begin{array}{rcl}
   K & = & \text{number of segments}  \\ ~\\ 
   R = (r_k)_k & = & \text{segmentation}  \\ ~\\ 
   T = (T_r)_r & = & \text{set of trees}  \\ 
   T_r & = & \text{graphical model in segment $r$} \\ ~\\ 
   \theta = (\theta_r)_ r & = & \text{parameter in each segment}  \\
   & & \text{typically: support$(\Sigma_r^{-1}) = T_r$} \\ ~\\ 
   Y_t = (Y_{jt}) & = & \text{data collected at time $t$} 
  \end{array}
  $$
  
  \bigskip \bigskip \pause
  \paragraph{Typical quantity of interest:} (under factorisable prior assumptions)
%   $$
%   p(Y | K) = \sum_R \sum_{T = (T_r)_{r\in R}} p(R|K) \prod_{r \in R} p(T_r | R) p(Y^r | T_r)
%   $$
  $$
  p(Y | K) \propto \sum_R \sum_{T = (T_r)_{r\in R}} \prod_{r \in R} a_r \prod_{jk \in T_r} b_{jk} p(Y_{jk}^r)
  $$
  }

%====================================================================
\frame{\frametitle{Handling two sums} 

  $$
  \begin{tabular}{lcc}
    & Space size & Complexity \\ ~\\
    \hline ~\\
    Segmentation & $\approx (n/K)^K$ & $O(Kn^2)$ \\ ~\\
    Spanning trees & $p^{(p-2)}$ & $O(p^3)$ \\ ~\\
    \hline ~\\
    Both & $\approx (n/K)^K p^{K(p-2)}$ & $O(\max\{K, p^3\}n^2)$
  \end{tabular}
  $$

  \bigskip
  \paragraph{Quantities of interest} can be computed in $O(p^3n^2)$: \\
  \begin{itemize}
   \item $P(\text{change-point at time $t$} \, | \, K, Y)$ 
   \item $P(\text{edge $(j, k)$ present at time $t$} \, | \, K, Y)$
   \item $P(\text{edge $(j, k)$ present at all $t$} \, | \, Y)$ 
   \item $P(\text{$K$ segments} \, | \, Y)$
   \item + Network comparison $P(T_1 = T_2 | Y_1, Y_2)$
  \end{itemize}
}

%====================================================================
\frame{\frametitle{Gene regulatory network} 

  \pause
  \paragraph{Data:} $n = 67$ time points, $p = 11$ genes, four expected regions

  \pause \bigskip
  Posterior probability of change-points:
  $$
  \includegraphics[width=.7\textwidth]{\figchp/ScR16-Fig8-chgpt_EMP_ap10_au1}
  $$

  \pause
  Inferred networks:
  $$
  \includegraphics[width=.8\textwidth]{\figchp/ScR16-Fig9-network_seuil02}
  $$

}

%====================================================================
\frame{\frametitle{FMRI data \refer{CHA12}} 

  \begin{tabular}{cc}
    \begin{tabular}{p{.3\textwidth}} 
	 FMRI data collected on $20$ patients: \\
	 $p=5$ brain regions,\\
	 $n = 215$ time-points. \\ ~
	 
	 Task changes at\\
	 $t = 60$ and $120$. \\ ~
	 
	 Top: 5 patients analyzed separately. \\ ~
	 
	 Bottom: joint analysis of the same 5 patients

    \end{tabular}
    & 
    \begin{tabular}{l}
    \includegraphics[width=.5\textwidth]{\figchp/ScR16-Fig10a} \\
    \includegraphics[width=.56\textwidth]{\figchp/ScR16-Fig10c}
    \end{tabular}
  \end{tabular}

}

%====================================================================
%====================================================================
\section{Discussion}
\frame{\frametitle{Outline} \tableofcontents[currentsection]}

%====================================================================
\frame{\frametitle{Summary} 

  \paragraph{To summarize.} \\ ~
  \begin{itemize}
   \item Exact Bayesian inference can be achieved for some fairly complex models with discrete parameter. \\ ~
   \item Do not have to care about sampling and convergence. \\ ~
   \item No systematic way to check when similar algebraic shortcuts exist \\
   \ra ad-hoc developments.
  \end{itemize}

}

%====================================================================
\frame{\frametitle{Future works} 

  \begin{itemize}
%    \item Combining the two problems: finding change-points in a network structure.
   \item Dealing with dependency along time. 
   $$
   \begin{tikzpicture}
   \node[observed] (Y1t_2) at (0*\edgeunit, 0*\edgeunit) {$Y^1_{t-2}$};
   \node[observed] (Y2t_2) at (0*\edgeunit, .66*\edgeunit) {$Y^2_{t-2}$};
   \node[observed] (Y3t_2) at (0*\edgeunit, 1.33*\edgeunit) {$Y^3_{t-2}$};
   \node[observed] (Y4t_2) at (0*\edgeunit, 2*\edgeunit) {$Y^4_{t-2}$};
   \node[observed] (Y1t_1) at (1.5*\edgeunit, 0*\edgeunit) {$Y^1_{t-1}$};
   \node[observed] (Y2t_1) at (1.5*\edgeunit, .66*\edgeunit) {$Y^2_{t-1}$};
   \node[observed] (Y3t_1) at (1.5*\edgeunit, 1.33*\edgeunit) {$Y^3_{t-1}$};
   \node[observed] (Y4t_1) at (1.5*\edgeunit, 2*\edgeunit) {$Y^4_{t-1}$};
   \draw[arrow] (Y1t_2) to (Y2t_1);  \draw[arrow] (Y2t_2) to (Y4t_1);
   \draw[arrow] (Y3t_2) to (Y1t_1);  \draw[arrow] (Y4t_2) to (Y3t_1);
   \node[observed] (Y1t) at (3*\edgeunit, 0*\edgeunit) {$Y^1_{t}$};
   \node[observed] (Y2t) at (3*\edgeunit, .66*\edgeunit) {$Y^2_{t}$};
   \node[observed] (Y3t) at (3*\edgeunit, 1.33*\edgeunit) {$Y^3_{t}$};
   \node[observed] (Y4t) at (3*\edgeunit, 2*\edgeunit) {$Y^4_{t}$};
   \draw[arrow] (Y1t_1) to (Y2t);  \draw[arrow] (Y2t_1) to (Y4t);
   \draw[arrow] (Y3t_1) to (Y1t);  \draw[arrow] (Y4t_1) to (Y3t);
   \node[empty] (Ytru) at (3.75*\edgeunit, 2.5*\edgeunit) {};  
   \node[empty] (Ytrb) at (3.75*\edgeunit, -.5*\edgeunit) {};  
   \draw[dashededge] (Ytru) to (Ytrb);
   \node[observed] (Y1t1) at (4.5*\edgeunit, 0*\edgeunit) {$Y^1_{t+1}$};
   \node[observed] (Y2t1) at (4.5*\edgeunit, .66*\edgeunit) {$Y^2_{t+1}$};
   \node[observed] (Y3t1) at (4.5*\edgeunit, 1.33*\edgeunit) {$Y^3_{t+1}$};
   \node[observed] (Y4t1) at (4.5*\edgeunit, 2*\edgeunit) {$Y^4_{t+1}$};
%    \draw[arrow] (Y1t) to (Y2t1);  \draw[arrow] (Y2t) to (Y4t1);
%    \draw[arrow] (Y3t) to (Y1t1);  \draw[arrow] (Y4t) to (Y3t1);
   \node[observed] (Y1t2) at (6*\edgeunit, 0*\edgeunit) {$Y^1_{t+2}$};
   \node[observed] (Y2t2) at (6*\edgeunit, .66*\edgeunit) {$Y^2_{t+2}$};
   \node[observed] (Y3t2) at (6*\edgeunit, 1.33*\edgeunit) {$Y^3_{t+2}$};
   \node[observed] (Y4t2) at (6*\edgeunit, 2*\edgeunit) {$Y^4_{t+2}$};
   \draw[arrow] (Y1t1) to (Y4t2);  \draw[arrow] (Y2t1) to (Y3t2);
   \draw[arrow] (Y3t1) to (Y1t2);  \draw[arrow] (Y4t1) to (Y2t2);
   \end{tikzpicture} 
   $$
   \item Influence of the prior: $p(T)$ depends on $n$ and/or $p$. \\ ~
   \item Solve numerical issues raised by the exact evaluation of all probabilities.
  \end{itemize}
}


%====================================================================
\frame[allowframebreaks]{ \frametitle{References}
{\tiny
  \bibliography{/home/robin/Biblio/BibGene}
%   \bibliographystyle{/home/robin/LATEX/Biblio/astats}
  \bibliographystyle{alpha}
  }
}


%====================================================================
%====================================================================
\backupbegin
\appendix
\section*{Backup}
%====================================================================
\subsection*{Graphical model framework}
\frame{\frametitle{Graphical model framework} 

  \paragraph{Property [Hammersley-Clifford].} $p(Y) = p(Y_1, \dots Y_p)$ is Markov wrt the (decomposable) graph $G$ iff it factorizes wrt the maximal cliques of $G$:
  $$
  p(Y) \propto \prod_{C \in \Ccal(G)} \psi_c(Y^c), 
  \qquad Y^c = (Y_j)_{j \in C}.
  $$
  
  \bigskip 
  \ra $G$ reveals the structure of conditional independences between the variables $Y_1, \dots Y_p$.
  
}

%====================================================================
\subsection*{Bayesian inference}
\frame{\frametitle{Bayesian inference} 

  \paragraph{Factorability assumptions} 
  \begin{itemize}
  \item Independent parameters in each segment:
  $$
  p(\theta\, | \,T) = \prod_{r \in T} p(\theta_r)
  $$
  ~
  \item Data are independent from one segment to another
  $$
  p(Y \, | \, T, \theta) = \prod_{r \in T} p(Y^r \, | \, \theta_r) 
  $$
  ~
  \item Prior distribution for the segmentation:
  $$
  p(T\, | \,K) = \prod_{r \in T} a_r , \qquad \text{e.g. } a_r = n_r^\alpha
  $$
  \end{itemize}

}

%====================================================================
\frame{\frametitle{Hyper-Markov prior} 

  \begin{tabular}{cc}
    \begin{tabular}{p{.5\textwidth}}
	 \onslide+<1->{
	   \paragraph{Graphical model:} \\
	   $p(Y \, | \, \theta, T)$ factorizes wrt edges of $T$ \\
	   (Markov wrt $T$) \\
	   ~\\
	   }
	 \onslide+<2->{
	   \paragraph{Desirable prior:} \\
	   $p(\theta\, | \, T)$ factorizes wrt edges of $T$ as well \\
	   (Hyper-Markov wrt $T$) \\
	   ~\\
	   }
	 \onslide+<3->{
	   \paragraph{Averaging over $T$:} \\
	   This should hold fot any tree $T$
	   }
    \end{tabular}
    & 
    \hspace{-.02\textwidth}
    \begin{tabular}{p{.5\textwidth}}
	 \begin{overprint}
	 \onslide<1>
	   \hspace{.01\textwidth} \vspace{-.15\textheight}
	   \begin{tikzpicture}
	   \node[observed] (Y1) at (0, 0) {$Y_1$};
	   \node[observed] (Y2) at (\edgeunit, 0) {$Y_2$};
	   \node[observed] (Y3) at (.5*\edgeunit, .87*\edgeunit) {$Y_3$};
	   \node[observed] (Y4) at (-.5*\edgeunit, -.87*\edgeunit) {$Y_4$};
	   \node[observed] (Y5) at (.5*\edgeunit, -.87*\edgeunit) {$Y_5$};
	   \draw[edge] (Y1) to (Y3);  \draw[edge] (Y2) to (Y3);
	   \draw[edge] (Y1) to (Y4);  \draw[edge] (Y1) to (Y5);
	   \node[empty] (theta1) at (-1*\edgeunit, 0) {};
	   \node[empty] (theta2) at (2*\edgeunit, 0) {};
	   \node[empty] (theta3) at (.5*\edgeunit, 1.74*\edgeunit) {};
	   \end{tikzpicture}
	 \onslide<2>
	   \begin{tikzpicture}
	   \node[observed] (Y1) at (0, 0) {$Y_1$};
	   \node[observed] (Y2) at (\edgeunit, 0) {$Y_2$};
	   \node[observed] (Y3) at (.5*\edgeunit, .87*\edgeunit) {$Y_3$};
	   \node[observed] (Y4) at (-.5*\edgeunit, -.87*\edgeunit) {$Y_4$};
	   \node[observed] (Y5) at (.5*\edgeunit, -.87*\edgeunit) {$Y_5$};
	   \draw[edge] (Y1) to (Y3);  \draw[edge] (Y2) to (Y3);
	   \draw[edge] (Y1) to (Y4);  \draw[edge] (Y1) to (Y5);
	   \node[hidden] (theta1) at (-1*\edgeunit, 0) {$\theta_1$};
	   \node[hidden] (theta2) at (2*\edgeunit, 0) {$\theta_2$};
	   \node[hidden] (theta3) at (.5*\edgeunit, 1.74*\edgeunit) {$\theta_3$};
	   \node[empty] (Ye13) at (.25*\edgeunit, .435*\edgeunit) {};
	   \node[empty] (Ye23) at (.75*\edgeunit, .435*\edgeunit) {};
	   \node[hidden] (theta13) at (-.62*\edgeunit, .935*\edgeunit) {$\theta_{13}$};
	   \node[hidden] (theta23) at (1.87*\edgeunit, .935*\edgeunit) {$\theta_{23}$};
	   \draw[arrow] (theta1) to (Y1);  \draw[arrow] (theta2) to (Y2);
	   \draw[arrow] (theta3) to (Y3);  
	   \draw[arrow] (theta13) to (Ye13); \draw[arrow] (theta23) to (Ye23);
	   \end{tikzpicture} 
	 \onslide<3->
	   \begin{tikzpicture}
	   \node[observed] (Y1) at (0, 0) {$Y_1$};
	   \node[observed] (Y2) at (\edgeunit, 0) {$Y_2$};
	   \node[observed] (Y3) at (.5*\edgeunit, .87*\edgeunit) {$Y_3$};
	   \node[observed] (Y4) at (-.5*\edgeunit, -.87*\edgeunit) {$Y_4$};
	   \node[observed] (Y5) at (.5*\edgeunit, -.87*\edgeunit) {$Y_5$};
	   \draw[edge] (Y1) to (Y3);  \draw[edge] (Y1) to (Y2);
	   \draw[edge] (Y1) to (Y4);  \draw[edge] (Y1) to (Y5);
	   \node[hidden] (theta1) at (-1*\edgeunit, 0) {$\theta_1$};
	   \node[hidden] (theta2) at (2*\edgeunit, 0) {$\theta_2$};
	   \node[hidden] (theta3) at (.5*\edgeunit, 1.74*\edgeunit) {$\theta_3$};
	   \node[empty] (Ye13) at (.25*\edgeunit, .435*\edgeunit) {};
	   \node[empty] (Ye12) at (.5*\edgeunit, 0) {};
	   \node[hidden] (theta13) at (-.62*\edgeunit, .935*\edgeunit) {$\theta_{13}$};
	   \node[hidden] (theta12) at (1.5*\edgeunit, .935*\edgeunit) {$\theta_{12}$};
	   \draw[arrow] (theta1) to (Y1);  \draw[arrow] (theta2) to (Y2);
	   \draw[arrow] (theta3) to (Y3);  
	   \draw[arrow] (theta13) to (Ye13); \draw[arrow] (theta12) to (Ye12);
	   \end{tikzpicture} 
	   
	   \bigskip
	   ($\theta_4$, $\theta_5$, $\theta_{14}$, $\theta_{15}$ not drawn.)
	 \end{overprint}
    \end{tabular}
  \end{tabular}

  \bigskip 
  \onslide<4>{
    \emphase{Compatible family of strong Markov hyper-dist. \refer{DaL93}:} \\
    \ra multinomial-Dirichlet (conjugacy), \\
    \ra normal-Wishart (conjugacy), \\
    \ra Gaussian copulas (numerical integration), ...?
    }
  }
  
%====================================================================
\frame{\frametitle{Posterior probability of an edge} 

  The existence of an edge between variables $Y_j$ and $Y_k$ can be assessed by
  $$
  \Pr\{(j, k) \in T \, | \, Y\} \propto \sum_{T \ni (j, k)} p(T) p(Y\, | \,T)
  $$
  which depends on the prior $p(T)$.
  
  \bigskip \bigskip
  The prior probability $\Pr\{(j, k) \in T\}$ can be tuned
  \begin{itemize}
   \item with the prior coefficient $b_{jk}$ 
   \item or set to an arbitrary value using an edge-specific probability change.
  \end{itemize}
  

  \pause \bigskip \bigskip
  All posterior probabilities can \emphase{still be computed in $O(p^3)$} \refer{Kir07}. \\ ~\\
  \ra R package Saturnin (spanning trees used for network inference) \refer{SRS15}

}

%====================================================================
\frame{\frametitle{Simulations: ROC curves for edge detection} 

For various graph topologies ($p=25$, $n = 25, 50, 200$, $B = 100$ simulations)
$$%\\
\begin{tabular}{cccc}
\includegraphics[width=0.18\linewidth]{\fignet/roc_curves_tree_multinomial.pdf}
& \includegraphics[width=0.18\linewidth]{\fignet/roc_curves_ER2p_multinomial.pdf}
& \includegraphics[width=0.18\linewidth]{\fignet/roc_curves_ER4p_multinomial.pdf}
& \includegraphics[width=0.18\linewidth]{\fignet/roc_curves_ER8p_multinomial.pdf} \\
Tree & Erd\"{o}s-R\'{e}nyi & Erd\"{o}s-R\'{e}nyi & Erd\"{o}s-R\'{e}nyi \\
& $p_c = 2/p$ & $p_c = 4/p$ &  $p_c = 8/p$ 
\end{tabular}
$$
}

%====================================================================
\frame{\frametitle{Some simulations} 

  \begin{tabular}{ccc}
   Tree & Erd\"os ($\pi = 2/p$) & Erd\"os ($\pi = 4/p$) \\
   \begin{tabular}{c}
    \includegraphics[width=.3\textwidth]{\figchp/ScR16-Fig4-Tree_N_p10_Tree}
   \end{tabular}
   &
   \hspace{-.05\textwidth}
   \begin{tabular}{c}
    \includegraphics[width=.3\textwidth]{\figchp/ScR16-Fig4-ER2p_N_p10_Tree}
   \end{tabular}
   &
   \hspace{-.05\textwidth}
   \begin{tabular}{c}
    \includegraphics[width=.3\textwidth]{\figchp/ScR16-Fig4-ER4p_N_p10_Tree}
   \end{tabular}
   \\
   \begin{tabular}{c}
    \includegraphics[width=.3\textwidth]{\figchp/ScR16-Fig4-Tree_N_p10_Complete}
   \end{tabular}
   &
   \hspace{-.05\textwidth}
   \begin{tabular}{c}
    \includegraphics[width=.3\textwidth]{\figchp/ScR16-Fig4-ER2p_N_p10_Complete}
   \end{tabular}
   &
   \hspace{-.05\textwidth}
   \begin{tabular}{c}
    \includegraphics[width=.3\textwidth]{\figchp/ScR16-Fig4-ER4p_N_p10_Complete}
   \end{tabular}
  \end{tabular}
  Top to bottom: n = 70, 140, 210. %\\
   \textcolor{blue}{Tree-structured} network. \textcolor{red}{Complete} network.

}

%====================================================================
\frame{\frametitle{Algebraic properties} 

  \begin{tabular}{ccc}
    \begin{tabular}{p{.3\textwidth}}
    \end{tabular}
    & 
    \hspace{-.05\textwidth}
    \begin{tabular}{p{.3\textwidth}}
	 Bayesian inference 
    \end{tabular} 
    & 
    \hspace{-.05\textwidth}
    \begin{tabular}{p{.3\textwidth}}
	 Maximum likelihood 
    \end{tabular} \pause
    \\ ~\\ \hline ~\\
    \begin{tabular}{p{.3\textwidth}}
	 Change-point detection
    \end{tabular}
    & 
    \hspace{-.05\textwidth}
    \begin{tabular}{p{.3\textwidth}}
	 $\sum_m \prod_{r \in m} p_r$ \\ ~\\
	 \ra Matrix power 
    \end{tabular} 
    & 
    \hspace{-.05\textwidth}
    \begin{tabular}{p{.3\textwidth}}
	 $\max_m \sum_{r \in m} \log p_r$ \\ ~\\
	 \ra Dynamic programing 
    \end{tabular} \pause
    \\ ~\\ \hline ~\\
    \begin{tabular}{p{.3\textwidth}}
	 Tree-structured network inference
    \end{tabular}
    & 
    \hspace{-.05\textwidth}
    \begin{tabular}{p{.3\textwidth}}
	 $\sum_T \prod_{(jk) \in T} p_{jk}$ \\ ~\\
	 \ra Matrix-tree theorem 
    \end{tabular} 
    & 
    \hspace{-.05\textwidth}
    \begin{tabular}{p{.3\textwidth}}
	 $\max_T \sum_{(jk) \in T} \log p_{jk}$ \\ ~\\
	 \ra Max. spanning tree 
    \end{tabular} \pause
    \\ ~\\ \hline ~\\ 
    \begin{tabular}{p{.3\textwidth}}
	 Algebra
    \end{tabular}
    & 
    \hspace{-.05\textwidth}
    \begin{tabular}{p{.3\textwidth}}
	 sum-product
    \end{tabular}
    & 
    \hspace{-.05\textwidth}
    \begin{tabular}{p{.3\textwidth}}
	 max-sum
    \end{tabular}
  \end{tabular}
  
  \pause \bigskip \bigskip 
  Any other example?

}

%====================================================================
%====================================================================
\backupend
\end{document}
%====================================================================
%====================================================================

  \begin{tabular}{cc}
    \begin{tabular}{p{.5\textwidth}}
    \end{tabular}
    & 
    \hspace{-.02\textwidth}
    \begin{tabular}{p{.5\textwidth}}
    \end{tabular}
  \end{tabular}

