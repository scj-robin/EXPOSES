\documentclass[dvips, lscape]{foils}
%\documentclass[dvips, french]{slides}
\textwidth 19cm
\textheight 25cm 
\topmargin -1cm 
\oddsidemargin  -1cm 
\evensidemargin  -1cm

% Maths
\usepackage{amsfonts, amsmath, amssymb, url}
\usepackage{/Latex/astats}

\newcommand{\coefbin}[2]{\left( 
    \begin{array}{c} #1 \\ #2 \end{array} 
  \right)}
\newcommand{\bbullet}{\bullet\bullet}
\newcommand{\bbbullet}{\bbullet\bullet}
\newcommand{\bbbbullet}{\bbbullet\bullet}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Ccal}{\mathcal{C}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Zcal}{\mathcal{Z}}
\newcommand{\dd}{\text{d}}
\newcommand{\etabar}{\overline{\eta}}
\newcommand{\pibar}{\overline{\pi}}
\newcommand{\alphabf}{\mbox{\mathversion{bold}{$\alpha$}}}
\newcommand{\betabf}{\mbox{\mathversion{bold}{$\beta$}}}
\newcommand{\gammabf}{\mbox{\mathversion{bold}{$\gamma$}}}
\newcommand{\mubf}{\mbox{\mathversion{bold}{$\mu$}}}
\newcommand{\nubf}{\mbox{\mathversion{bold}{$\nu$}}}
\newcommand{\phibf}{\mbox{\mathversion{bold}{$\phi$}}}
\newcommand{\pibf}{\mbox{\mathversion{bold}{$\pi$}}}
\newcommand{\Pibf}{\mbox{\mathversion{bold}{$\Pi$}}}
\newcommand{\psibf}{\mbox{\mathversion{bold}{$\psi$}}}
\newcommand{\Sigmabf}{\mbox{\mathversion{bold}{$\Sigma$}}}
\newcommand{\taubf}{\mbox{\mathversion{bold}{$\tau$}}}
\newcommand{\thetabf}{\mbox{\mathversion{bold}{$\theta$}}}
\newcommand{\Abf}{{\bf A}}
\newcommand{\Ebf}{{\bf E}}
\newcommand{\Hbf}{{\bf H}}
\newcommand{\Ibf}{{\bf I}}
\newcommand{\Obf}{{\bf 0}}
\newcommand{\Sbf}{{\bf S}}
\newcommand{\mbf}{{\bf m}}
\newcommand{\ubf}{{\bf u}}
\newcommand{\vbf}{{\bf v}}
\newcommand{\xbf}{{\bf x}}
\newcommand{\Xbf}{{\bf X}}
\newcommand{\ybf}{{\bf y}}
\newcommand{\Zbf}{{\bf Z}}
\newcommand{\Esp}{{\mathbb E}}
\newcommand{\Corr}{{\mathbb C}\mbox{orr}}
\newcommand{\Nbb}{\mathbb{N}}
\newcommand{\Nm}{N(\mbf)}
\newcommand{\mum}{\mu(\mbf)}
\newcommand{\obs}{\text{obs}}
\newcommand{\Ibb}{{\mathbb I}}
\newcommand{\Omegas}{\underset{s}{\Omega}}
\newcommand{\Var}{{\mathbb V}}
\newcommand{\Pro}{\mathbb{P}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\RX}{R_{\Xbf}}
\newcommand{\Vsf}{\mathsf{V}}
\newcommand{\Starsf}{\mathsf{*}}
\newcommand{\QZ}{Q_{\Zbf}}
\newcommand{\QZi}{Q_{\Zbf_i}}
\newcommand{\Qt}{Q_{\thetabf}}


% Couleur et graphiques
\usepackage{color}
\usepackage{graphics}
\usepackage{epsfig} 
\usepackage{pstcol}

% Texte
\usepackage{lscape}
\usepackage{../../../../Latex/fancyheadings, rotating, enumerate}
%\usepackage[french]{babel}
\usepackage[latin1]{inputenc}
%\definecolor{darkgreen}{cmyk}{0.5, 0, 0.5, 0.5}
%\definecolor{green}{cmyk}{0.5, 0, 0.5, 0.5}
\definecolor{orange}{cmyk}{0, 0.6, 0.8, 0}
\definecolor{jaune}{cmyk}{0, 0.5, 0.5, 0}
\newcommand{\textblue}[1]{\textcolor{blue}{#1}}
\newcommand{\textred}[1]{\textcolor{red}{#1}}
\newcommand{\textgreen}[1]{\textcolor{green}{ #1}}
\newcommand{\textlightgreen}[1]{\textcolor{green}{#1}}
%\newcommand{\textgreen}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\textorange}[1]{\textcolor{orange}{#1}}
\newcommand{\textyellow}[1]{\textcolor{yellow}{#1}}
\newcommand{\emphase}[1]{\textblue{\sl #1}}
\newcommand{\refer}[1]{\textgreen{\sl \cite{#1}}}

% Listes
\newcommand{\itemv}{\item \vspace{-0.5cm}}

% Sections
%\newcommand{\chapter}[1]{\centerline{\LARGE \textblue{#1}}}
% \newcommand{\section}[1]{\centerline{\Large \textblue{#1}}}
% \newcommand{\subsection}[1]{\noindent{\Large \textblue{#1}}}
% \newcommand{\subsubsection}[1]{\noindent{\large \textblue{#1}}}
% \newcommand{\paragraph}[1]{\noindent {\textblue{#1}}}
% Sectionsred
\newcommand{\chapter}[1]{
  \addtocounter{chapter}{1}
  \setcounter{section}{0}
  \setcounter{subsection}{0}
  {\centerline{\LARGE \textblue{\arabic{chapter} - #1}}}
%  {\centerline{\LARGE \textblue{#1}}}
  }
\newcommand{\section}[1]{
  \addtocounter{section}{1}
  \setcounter{subsection}{0}
  {\noindent {\Large \textblue{\arabic{chapter}.\arabic{section} - #1}}}
%  {\noindent {\Large \textblue{#1}}}
  }
\newcommand{\subsection}[1]{
  \addtocounter{subsection}{1}
%   {\noindent{\large
%       \textblue{\arabic{chapter}.\arabic{section}.\arabic{subsection}
%         - #1}}}
  {\noindent{\large \textblue{#1}}}
  }
\newcommand{\paragraph}[1]{\noindent{\textblue{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\landscape
\newcounter{chapter}
\newcounter{section}
\newcounter{subsection}
\setcounter{chapter}{0}
\headrulewidth 0pt 
\pagestyle{fancy} 
\cfoot{}
\rfoot{
% \begin{rotate}{90}{
%       \hspace{1cm} \tiny S. Robin: Mixture model for valued graphs  
%       }\end{rotate}
  }
\rhead{\begin{rotate}{90}{
      \hspace{-.5cm} \tiny \thepage
      }\end{rotate}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}
  \textblue{\LARGE Uncovering latent structure in valued graphs:}

  \textblue{\LARGE A variational approach}
  
  ~\\
  \textblue{\large S. Robin} \\
  ~\\
  {\tt robin@agroparistech.fr} \\
  ~\\
  {\large Joint work with \textblue{J.-J. Daudin, M. Mariadassou,
  F. Picard, C. Vacher}}
\end{center}

\noindent UMR AgroParisTech / INRA, Paris, Mathématique et Informatique Appliquées~: \\
\centerline{\url{www.agroparistech.fr/mia/}}

\noindent {Statistics for Systems Biology (SSB) group:} \\
\centerline{\url{genome.jouy.inra.fr/ssb/}} \\

\paragraph{Research report:} \url{genome.jouy.inra.fr/ssb/preprint/}
\begin{itemize}
\item \vspace{-0.5cm} url{SSB-RR-4.ermg.pdf} + Stat. Comput.,
  18(2):173-83, Jun 2008. 
\item \vspace{-0.5cm} \url{SSB-RR-10.valued-graphs.pdf}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Looking for structure in networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bigskip
% \section{}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\begin{tabular}{cc}
  \begin{tabular}{p{12cm}}
    Networks \dots
    \begin{itemize}
    \item Arise in many fields:
      \begin{itemize}
      \item[\bf{$\rightarrow$}] Biology, Chemistry
      \item[\bf{$\rightarrow$}] Physics, Internet.
      \end{itemize}
    \item Represent an interaction pattern:
      \begin{itemize}
      \item[\bf{$\rightarrow$}] $\mathcal{O}(n^2)$ interactions
      \item[\bf{$\rightarrow$}] between $n$ elements.
      \end{itemize}
    \item Have a topology which:
      \begin{itemize}
      \item[\bf{$\rightarrow$}] reflects the structure / function
        relationship
      \end{itemize}
    \end{itemize}
  \end{tabular}
  &
  \begin{tabular}{c}
    \psfig{file=../Figures/caida2_lg(brown).ps, angle=270, width=11cm} \\
    \begin{tiny} From Barab\'asi website \end{tiny}
  \end{tabular}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Uncovering structure in networks:} A simple example

\vspace{-2cm}
$$
\begin{tabular}{c}
  \epsfig{file = ../Figures/Karate-Graph.eps, clip=, width=7cm,
    height=20cm, angle=270}
  \\
  \epsfig{file = ../Figures/Karate-Dotplot.eps, clip=, width=7cm,
    height=20cm, angle=270}
\end{tabular}
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Heterogeneity in random graphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Nodes may have different connectivity behaviour.

\paragraph{Looking for connected sub-groups:}
\begin{itemize}
\item \vspace{-0.5cm} Detection of cliques or groups of highly
  connected nodes: \refer{GeD04} 
\item \vspace{-0.5cm} Edge betweenness: \refer{GiN02}
\item \vspace{-0.5cm} Spectral clustering: \refer{LBB07}
\end{itemize}

\paragraph{Model based:}
\begin{itemize}
\item \vspace{-0.5cm} Underlying topology: \refer{HRH02}
  (Latent space)
\item \vspace{-0.5cm} Mixture model \refer{NoS01}
  (Block structure), \refer{DPR08} (Mixture for graphs)
\item \vspace{-0.5cm} General model for heterogeneous networks:
  \refer{BJR07} (Topological properties: Giant component,
  diameter, degree distribution = compound Poisson, {\it etc.}).
\item \vspace{-0.5cm} General review on random graph models:
  \refer{PaR07}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Inhomogeneous random graphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{General definition for binary graphs.} (\refer{BJR07})
\begin{itemize}
\item \vspace{-0.5cm} $n$ nodes $(i = 1 \dots n$)
\item \vspace{-0.5cm} $n(n-1)/2$ possible edges: $X_{ij} = \Ibb\{ i \sim j\}$
\item \vspace{-0.5cm} Each $i$ is characterised by a \emphase{latent
    variable} $Z_i$ sampled in some space $\Zcal$ with distribution
  $\alpha$:
  $$
  \{Z_i\}_i \mbox{ i.i.d.}, \qquad Z_i \sim \alpha
  $$
\item \vspace{-0.5cm} Edge $(i, j)$ is present with probability
  $\pi(Z_i, Z_j)$, where $\pi$ is a \emphase{kernel function}:
  $$
  \{X_{ij}\}_{i, j} \mbox{ independent given } \{Z_i\}_i, \qquad X_{ij}
  \sim \Bcal[\pi(Z_i, Z_j)].
  $$
\end{itemize}
\paragraph{Latent space:} 
$\displaystyle{
\Zcal = \Rbb^k, \qquad \pi(z, z') = \frac{\exp(a - |z-z'|)}{1 + \exp(a
  - |z-z'|)}.
}$

\paragraph{Mixture model:} 
$\displaystyle{
\Zcal = \{1, \dots, Q\}, \qquad \pi(z, z') = \pi_{q\ell} \mbox{ for } z
= q, z' = \ell.
}$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Mixture model for valued graphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
\paragraph{Our approach}
\begin{itemize}
\item \vspace{-0.5cm} is model based: 
  $$
  \mbox{Mixture model}
  $$
\item \vspace{-0.5cm} deals with valued graphs:
  $$
  \mbox{$X_{ij} \in \{0, 1\}, \Nbb, \Rbb, \Rbb^d, {\it etc.}$}
  $$
\item \vspace{-0.5cm} and makes frequentist inference using a
  variational method:
  $$
  \mbox{Approximate maximum likelihood.}
  $$
\end{itemize}

\paragraph{Available software.} \\
\centerline{\tt http://stat.genopole.cnrs.fr/software/mixnet/}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
\item \vspace{-0.5cm} $n$ nodes $(i = 1 \dots n$);
\item \vspace{-0.5cm} each node $i$ belong to class $q$ with
  probability $\alpha_q$:
  $$
  \{Z_i\}_i \mbox{ i.i.d.}, \qquad Z_i \sim \Mcal(1; \alphabf)
  $$
  where $\alphabf = (\alpha_1, \dots \alpha_Q)$;
\item \vspace{-0.5cm} The values of the edges $\{X_{ij}\}_{i,
      j}$ are conditionally independent given the $Z_i$'s:
  $$
  (X_{ij} \;|\; Z_i = q, Z_j = \ell) \sim f_{q\ell}(\cdot).
  $$
  where $f_{q\ell}(\cdot)$ is some parametric distribution
  $f_{q\ell}(x) = f(x; \theta_{q\ell})$. \\ 
\end{itemize}
We denote: $\Zbf = \{Z_i\}_i$, $\Xbf = \{X_{ij}\}_{i, j}$, $\thetabf =
\{\theta_{q\ell}\}_{q, \ell}$, $\gammabf = (\alphabf, \thetabf)$.

\paragraph{Remark.} Similar to the block structure model (\refer{NoS01})
%$$
% \paragraph{Scale free network model }
%  (Barabasi \& Albert, 99) can also be expressed in terms of ERMG.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Some distributions $f_{q\ell}$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Bernoulli $\Bcal(\pi_{ql})$.} Binary oriented or
non-oriented \emphase{interaction graphs}: \\
Relation network, protein-protein interaction, gene regulation.

\paragraph{Multinomial $\Mcal(\pibf_{ql})$.} \emphase{Labelled edges}: \\
Social networks ('friend', 'lover', colleague'), Directed graphs with
correlated edges (' ', '$\rightarrow$', '$\leftarrow$',
'$\leftrightarrow$').

\paragraph{Poisson $\Pcal(\lambda_{ql})$.} The edge value is a \emphase{count}:
\\
Number of co-publications of two authors, Number of times two species
were observed in the same place, Number of alleles shared by two
species.

\paragraph{Gaussian $\Ncal(\mu_{q\ell}, \sigma^2)$.} \emphase{Traffic
  intensity}: \\
Airport network, Electric network.

\paragraph{Linear regression.} If \emphase{covariates} $\ybf_{ij}$ are
available for each couple of nodes:
$$
X_{ij} = \ybf_{ij} \betabf_{q\ell} + E_{ij}, \qquad \{E_{ij}\}_{i, j}
\mbox{ independent, } E_{ij} \sim \Ncal(0, \sigma^2).
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Binary case} \label{Sec:ErdosMixture}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Erdös-Rényi (ER) model.} $\{X_{ij}\}$ i.i.d.,  $\Pr\{ i
\leftrightarrow j\} = \pi$
$$
\Rightarrow \quad K_i \sim \Bcal(n-1, \pi) \approx \Pcal[(n-1)\pi],
\quad c = \pi.
$$
This model fits poorly many real-world networks, probably because
of some heterogeneity between the vertices.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{An explicit random graph model} 

\paragraph{Mixture population of edges.} We suppose that the
edges belong to $Q$ groups:
$$
\alpha_q = \Pr\{i \in q\}, 
\qquad
Z_{iq} = \Ibb\{i \in q\}.
$$
\paragraph{Conditional distribution of the edges.}
The edges $\{X_{ij}\}$ are conditionally independent given the group of
the vertices:
$$
X_{ij} \; |\; \{i \in q, j \in \ell \} \sim \Bcal(\pi_{q\ell}).
$$
$\pi_{q\ell} = \pi_{\ell q}$ is the connection probability between
groups $q$ and $\ell$. \\
A high value of $\pi_{q\ell}$ reveals a preferential connectivity
between groups $q$ and $\ell$. 

% \paragraph{Erdös-Rényi model.} The popular Erdös-Rényi fits
% poorly many real-world networks. It actually corresponds to $Q = 1$
% group. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%\subsection{Examples}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%$$
\vspace{-4cm}\hspace{-1cm} 
\begin{tabular}{lcccc}
  %\vspace{-3cm}
  \subsection{Examples} & Network & $Q$ & $\pi$ & Cluster. coef. \\
  \hline
  \begin{tabular}{p{2cm}} Random \end{tabular}
  & \begin{tabular}{c} \epsfig{file = ../figures/FigNetworks-Erdos-Col.eps,
      width=3.6cm, height=3.6cm, clip=, angle=270} 
  \end{tabular}
  & 1
  &  $p$ & $p$ \\
  \hline
  \begin{tabular}{p{2cm}} Independent model (product connectivity) \end{tabular}
  & \begin{tabular}{c} \epsfig{file = ../figures/FigNetworks-Indep-Col.eps,
      width=3.6cm, clip=, angle=270}
  \end{tabular}
  & 2
  & $\left( \begin{array}{cc} a^2 &ab\\ ab&b^2\\ \end{array}
  \right)$
  & $\displaystyle{\frac{(a^2+b^2)^2}{(a+b)^2}}$ \\
  \hline
  \begin{tabular}{p{2cm}} Stars \end{tabular}
  & \begin{tabular}{c} \epsfig{file = ../figures/FigNetworks-Star-Col.eps,
      width=3.6cm, clip=, angle=270}
  \end{tabular}
  & 4
  & $\left( \begin{array}{cccc} 0&1&0&0\\ 1&0&1&0\\0&1&0&1\\0&0&1&0\\
    \end{array} \right)$
  & 0 \\
  \hline \begin{tabular}{p{2cm}} Clusters (affiliation networks)
  \end{tabular} 
  & \begin{tabular}{c} \epsfig{file = ../figures/FigNetworks-Clusters-Col.eps,
      width=3.6cm, clip=, angle=270}
  \end{tabular}
  & 2
  & $\left(\begin{array}{cc} 1&\varepsilon\\ \varepsilon&1\\
    \end{array} \right)$ &
  $\displaystyle{\frac{1+3\varepsilon^2}{(1+\varepsilon)^2}}$ \\ 
\end{tabular} \\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Scale free network model.} \refer{BaA99}

The network is build iteratively: the $i$-th vertex joining the
network connects one of the $(i-1)$ preceeding ones with probability
proportional to their current degree (\paragraph{'busy gets busier'}):
$$
\forall j < i, \qquad \Pr\vspace{0cm}^i\{ i \leftrightarrow j\}
\propto K_j^i. 
$$
The limit marginal distribution of the degree is then scale free
(Zipf): $p(k) \propto k^{-3}$.

                                %\bigskip\bigskip
\hspace{-1.75cm}
\begin{tabular}{cc}
  \begin{tabular}{p{12cm}}
    \paragraph{Mixture with 'product probabilities'.} At time $q$,
    $n_q = n \alpha_q$ vertices join the net work. They preferentially
    connect the oldest vertices:
    $$
    \pi_{q\ell} = \eta_q \eta_{\ell}, 
    \qquad \eta_1 \geq \eta_2 \geq \dots \geq \eta_q \geq \dots 
    $$
    The decreasing speed of the $\{\eta_q\}$ gives the tail of the degree
    distribution. 
  \end{tabular}
  &
  \begin{tabular}{c}
    \epsfig{file = ../figures/FigNetworks-PrefAtt-Col.eps, 
      height=10cm, width=11cm, clip=,}% angle=270}
  \end{tabular}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Some properties}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Conditional distribution of the degree:} Denoting $
\pibar_q = \sum_{\ell} \alpha_{\ell} \pi_{q\ell}$, $\lambda_q = (n-1)
\pibar_q$
$$
K_i \;|\; \{i \in q \} \sim \Bcal(n-1, \pibar_q) \approx
\Pcal(\lambda_q).
$$

\bigskip
\paragraph{Marginal distribution of the degree:} we get a Poisson mixture
$$
K_i \sim \sum_q \Bcal(n-1, \pibar_q) \approx \sum_q \alpha_q \Pcal(\lambda_q).
$$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage
% \paragraph{Between-group connectivity.}
% Let $A_{q\ell}$ denote the number of edges between groups $q$ and
% $\ell$. Its expectation in the ERMG model is
% $$
% \Esp (A_{q\ell})  = \frac{n(n-1)}2 \alpha_q \alpha_{\ell} \pi_{q\ell}.
% $$

\bigskip
\paragraph{Clustering coefficient:}  
$$
c = \Pr\{\nabla \;|\; \Vsf\} = \Pr\{\nabla\} / \Pr\{\Vsf\} 
= \frac{\sum_{q, \ell, m} \alpha_q \alpha_{\ell} \alpha_m \pi_{q\ell}
\pi_{qm} \pi_{\ell m}}{\sum_{q, \ell, m} \alpha_q
  \alpha_{\ell} \alpha_m \pi_{q\ell} \pi_{qm}}.
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Variational inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip
\section{Maximum Likelihood Inference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Likelihoods.} The log-likelihood of the complete dataset
($\Xbf, \Zbf$) is
\begin{eqnarray*}
  \log{\Pro(\Zbf,\Xbf; \alphabf, \thetabf)} & = & \log\Pro(\Zbf; \alphabf) +
    \log{\Pro(\Xbf | \Zbf; \thetabf)} \\
  & = & \sum_i \sum_q Z_{iq}\log{\alpha_q} + \sum_{i \neq j}
  \sum_{q,\ell} Z_{iq}Z_{j\ell}\log{f_{q\ell}(X_{ij})}.
\end{eqnarray*}
The log-likelihood of the observed dataset ($\Xbf$) is
$$
\log{\Pro(\Xbf; \alphabf, \thetabf)} = \sum_{\Zbf} \log{\Pro(\Zbf,\Xbf;
  \alphabf, \thetabf)}
$$
and cannot be evaluated since $\Zbf$ may take $Q^n$ different
values. \\

\paragraph{Most popular solution:} E-M algorithm that aims at
\emphase{restoring the unobserved $Z_i$'s}
.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{E-M algorithm.} To achieve the E-step, we need to calculate
the \emphase{conditional distribution of the unobserved data} given
the observed ones: \emphase{$\log{\Pro(\Zbf|\Xbf)}$}.

\paragraph{Due to intricate dependencies} this distribution is
\emphase{intractable}:
$$
\begin{tabular}{p{11cm}p{11cm}}
  \paragraph{Dependency graph} (oriented) & \paragraph{Moral graph}
  (parents are married) \\  
  \\
  Edge $X_{ij}$ only depends on its two parents $Z_1$ and $Z_2$
  &
  Conditional on the edges, labels $Z_i$'s all depend on each others \\
                                %\\
  \hspace{-1cm}\epsfig{file = ../figures/FigNetworks-DepGraph.eps, clip=,
    angle=270, width=11cm}
  &
  \hspace{-1cm}\epsfig{file = ../figures/FigNetworks-DepGraph-Moral.eps, clip=,
    angle=270, width=11cm} \\
  \multicolumn{2}{c}{\vspace{-0.5cm}$\Rightarrow$ All edges are
    actually \emphase{'neighbours'} (unlike in Bayesian networks).}
\end{tabular}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Variational strategy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Variational trick:} Maximise a \emphase{lower bound} of the
incomplete likelihood
$$
\Jcal(\RX,\alphabf, \thetabf) = \log{\Pro(\Xbf;\alphabf, \thetabf)} -
KL[\RX(\cdot),\Pro(\cdot|\Xbf;\alphabf, \thetabf)]
$$
where 
\begin{itemize}
\item \vspace{-0.5cm} $KL$ denotes the Kullback-Leibler divergence 
\item \vspace{-0.5cm} $\RX$ is some distribution for $\Zbf$.
\end{itemize}
Thanks to the definition of $KL$, we get for any $\RX$
(\refer{Jaa00})
\begin{eqnarray*}
  \Jcal(\RX,\alphabf, \thetabf) & = & \log{\Pro(\Xbf)} -
  \sum_{\Zbf} \log[\RX(\Zbf)] \RX(\Zbf) + \sum_{\Zbf}
  \log[P(\Zbf|\Xbf)] \RX(\Zbf) \\
  & = & \Hcal(\RX) + \sum_{\Zbf}
  \RX(\Zbf)\log{\Pro(\Xbf, \Zbf;\alphabf, \thetabf)}
\end{eqnarray*}
where $\Hcal(\RX)$ stands for the entropy of distribution $\RX$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Choice of $\RX$.} $\RX$ approximates the conditional
distribution $\Pro(\Zbf|\Xbf)$. We want it to be
\begin{itemize}
\item \vspace{-0.5cm} tractable (e.g. factorised):
  $$    
  \RX(\Zbf) = \prod_i h(\Zbf_i, \taubf_i)
  $$
  where $h(\cdot, \taubf)$ denotes the multinomial distribution;
\item \vspace{-0.5cm} as close to $\Pro(\Zbf|\Xbf)$ as possible:
  $$
  \widehat{\taubf} = \arg \min
  KL[\RX(\cdot),\Pro(\cdot|\Xbf;\alphabf, \thetabf)].
  $$
\end{itemize}
We get
$$
\Jcal(\RX,\alphabf, \thetabf) = - \sum_i \sum_q \tau_{iq} \log{\tau_{iq}} +
\sum_i \sum_q \tau_{iq} \log{\alpha_q} + \sum_{i \neq j} \sum_{q,\ell}
\tau_{iq} \tau_{j\ell} \log{f_{q\ell}(X_{ij})}.
$$
The $\tau_i$'s are interpreted as \emphase{approximate posterior
  probabilities $\Pro\{Z_i = q | \Xbf\}$};

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Estimation algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The optimisation of $\Jcal(\RX,\alphabf, \thetabf)$ is achieved via two
alternative steps.

\paragraph{M-step:} Maximises $\Jcal(\RX,\alphabf, \thetabf)$ w.r.t. $\alphabf, \thetabf =
(\alphabf, \thetabf)$ given $\taubf$. We get
$$
\widehat{\alpha}_q = \frac1n \sum_i \tau_{iq}, \qquad
\widehat{\theta}_{q\ell} = \arg\underset{\theta_{q\ell}}{\max} \sum_{i
  \neq j} \tau_{iq}\tau_{j\ell}\log{f(X_{ij}; \theta_{q\ell})}.
$$

\paragraph{Pseudo E-step:} Finds the optimal $\taubf$ given
$(\alphabf, \thetabf)$. We end up with a \emphase{fix point relation}.
\begin{itemize}
\item \vspace{-0.5cm} Oriented graphs:
  $$
  \log \widehat{\tau}_{iq} = \mbox{cst} + \log \alpha_q + \sum_{j
    \neq i} \sum_{\ell} \widehat{\tau}_{j\ell} \left[\log
    f(X_{ij}; \theta_{q\ell}) \log f(X_{ji}; \theta_{\ell q})\right].
  $$
\item \vspace{-0.5cm} Non-oriented graphs:
  $$
  \log \widehat{\tau}_{iq} = \mbox{cst} + \log \alpha_q + \sum_{j
    \neq i} \sum_{\ell} \widehat{\tau}_{j\ell} \log f(X_{ij};
  \theta_{q\ell}).
  $$
\end{itemize}
\paragraph{$\rightarrow$ 'Variational E-M':} \refer{JGJ98}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Interpretation of the pseudo E-step}

\paragraph{Mean field approximation:} Given the classes of the other
nodes, $Z_i$ only depends on the edges $X_{ij}$:
\begin{eqnarray*}
\Pr\{Z_{iq} = 1 | \Xcal, \Zcal_{\setminus i}\} & \propto & \alpha_q
\prod_{j \neq i} \prod_{\ell} f(X_{ij};  \theta_{q\ell})^{Z_{j\ell}} \\
\Rightarrow \quad \log \Pr\{Z_{iq} = 1 | \Xcal, \Zcal_{\setminus i}\}
& = & \mbox{cst} + \log \alpha_q + \sum_{j \neq i} \sum_{\ell}
Z_{j\ell} \log f(X_{ij}; \theta_{q\ell}).
\end{eqnarray*}
The unknown classes $Z_{j}$ of nodes $j \neq i$ are
\emphase{replaced by their (estimated) mean}.

\bigskip\bigskip
\paragraph{'Message passing' algorithm:} Each node $j \neq i$ gives
its opinion (i.e. sends a message) about the class $q$ to which node
$i$ belongs
$$
m_{j \rightarrow i}(q) = \alpha_q \prod_{\ell} f(X_{ij};
\theta_{q\ell})^{\widehat{\tau}_{j\ell}}.
$$
The fix point algorithm can be viewed as a \emphase{'belief
  propagation'}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Model selection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Penalised likelihood.} Standard criteria, such as BIC or
AIC are based on the log-likelihood of observed data $\log
\Pro(\Xbf)$, so they can not be used here.

\paragraph{Integrated Classification Likelihood (ICL).} The ICL
criterion (\refer{BCG00}) is an approximation of the complete-data
integrated log-likelihood:
$$
\log \Pro(\Xbf, \Zbf| m_Q) = \int \log \Pro(\Xbf, \Zbf |
\gammabf,m_Q) g(\gammabf|m_Q) d\gammabf,
$$
where $\log \Pro(\Xbf, \Zbf | \gammabf,m_Q)$ is the log-likelihood
of model $m_Q$ with $Q$ classes.

We get
$$
ICL(m_Q) = \underset{\gammabf}{\max} \log \Pro(\Xbf,
\widehat{\Zbf} |\gammabf, m_Q) - \frac{1}{2} \left\{ P_Q \log
  [n(n-1)] - (Q-1) \log(n) \right\}.
$$
where $P_Q$ denotes the number of parameters in $\thetabf$ and
$\widehat{\Zbf}$ can be replaced by $\widehat{\taubf}$ or by the
Maximum A posteriori (MAP) prediction of $\Zbf$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip
\section{Karate club}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hspace{-2cm}
\begin{tabular}{cc}
  \begin{tabular}{p{12cm}}
    \paragraph{Data.} Social binary network of friendship within a
    sport club. \\
    \\
    \paragraph{Results.} 
    The split is recovered and the role of the leaders is underlined. 
  \end{tabular}
  &
  \begin{tabular}{p{12cm}}
    \begin{tabular}{c|rrrr}
      & \multicolumn{4}{c}{$\widehat{\pi}_{q\ell}$ (\%)} \\
      $q / \ell$ &  {1} & 2 & 3 &  4 \\
      \hline
       {1} &  {100} &   {53} &  {16} & {16} \\  
      {2} & - &  {12} & {0} & {7}  \\  
      3 & - & - & 8 & 73 \\
      4 & - & - & - & 100\\
      \hline
      $n\alpha_{\ell}$        & 3 &  13       & 16    & 2     \\
    \end{tabular}
  \end{tabular}
  \\
  \\
  \begin{tabular}{p{12cm}}
    \epsfig{file=../Figures/graphe_karate.ps, clip=, bbllx=93,
    bblly=140, bburx=550, bbury=440, width=10cm}
  \end{tabular}
  &
  \begin{tabular}{p{12cm}}
    \epsfig{file=../Figures/karate-simple.ps, width=10cm, clip=,
    bbllx=117, bblly=70, bburx=505, bbury=324}
  \end{tabular}
\end{tabular}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \section{Metabolic network of {\sl E. coli}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph{Dataset.}
% \begin{itemize}
% \item \vspace{-0.5cm} The network is made of 605 reaction (nodes) and
%   1782 edges (\refer{LFS06}).
% \item \vspace{-0.5cm} Reactions $i$ and $j$ are connected if the
%   compound of $i$ is the substrate of $j$.
% \item \vspace{-0.5cm} Because most reactions are reversible, the
%   network is not oriented.
% \item \vspace{-0.5cm} The only information about edges is terms of
%   presence/absence.
% \end{itemize}
% \paragraph{Results} 
% \begin{itemize}
% \item \vspace{-0.5cm} The ICL criterion applied to a mixture with
%   Bernoulli edge values select $\widehat{Q} = 21$ classes.
%     \item \vspace{-0.5cm} Groups 1 to 20 gather reactions involving
%       all the \emphase{same compound} either as a substrate or as a
%       product.
%     \item \vspace{-0.5cm} A compound (chorismate, pyruvate,
%       ATP,\emph{etc}) can be associated to each group.
% \end{itemize}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \paragraph{Dot-plot representation.} \\
% \begin{tabular}{cc}
%   \begin{tabular}{p{12cm}}
%     \begin{itemize}
%     \item Classes 1 and 16 constitute a \emphase{single clique}
%       corresponding to a single compound (pyruvate),
%     \item They are split into two classes because they
%       \emphase{interact differently with classes 7} (CO2) and 10
%       (AcetylCoA)
%     \item Connectivity matrix (sample): 
%       $$
%       \begin{array}{c|cccc}
%         q, \ell & 1 & 7 & 10 & 16 \\
%         \hline
%         1  & \textcolor{red}{1.0} & & & \\
%         7  & \textcolor{green}{.11} & .65 & &  \\
%         10 & \textcolor{green}{.43} & & .67 & \\
%         16 & \textcolor{red}{1.0} & \textcolor{yellow}{.01} &
%         \textcolor{yellow}{\epsilon} & \textcolor{red}{1.0}
%       \end{array}
%       $$
%     \end{itemize}    
%   \end{tabular}
%   &
%   \begin{tabular}{c}
%       Adjacency matrix \\
%       (zoom on the \emphase{first 20 classes}) \\
%       \\
%     \epsfig{file = ../Figures/Ecoli-Complet-ERMG-Ward-Q21_class3.ps,
%      height=12cm, width=12cm, clip=, angle=90, bbllx=33, bblly=64,
%     bburx=565, bbury=389} 
%    \end{tabular}
% \end{tabular}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \section{Gene regulations in {\sl A. Thaliana}}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \paragraph{Dataset.} \emphase{Partial correlations} between the expression
% levels of 800 genes in various conditions (\refer{OpS06}). \\
% ~\\
% \begin{tabular}{cc}
%   \hspace{-1cm}
%   \begin{tabular}{p{12cm}}
%     \paragraph{Dot-plot.} Dot size = absolute correlation, Color =
%     sign (\textred{$-$}, \textblue{$+$}). \\
%     ~\\
%     \paragraph{Results.} 
%     \begin{itemize}
%     \item \vspace{-0.5cm} Using a Gaussian model, we get $\widehat{Q}
%       = 7$ classes.
%     \item \vspace{-0.5cm} Groups are made of positively correlated
%       genes. 
%     \item \vspace{-0.5cm} Between group correlations are weaker than
%       within-group correlation and have different signs (see classes
%       3/4 with class 7).
%     \item \vspace{-0.5cm} Total computational time for $Q=1..15$
%       classes on a standard PC: 1h.
%     \end{itemize}
%   \end{tabular}
%   &
%   \begin{tabular}{c}
%     \hspace{-1cm}
%     \epsfig{file = ../Figures/GauHo_Q7_dotplot.eps, width=13cm,
%     height=13cm, clip=}
%   \end{tabular}
% \end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Macaque cortex network}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip    
\paragraph{Data.} Binary non-oriented graphs between cortical
regions ($n = 47, x_{++} = 505$).
   
\bigskip
\paragraph{Expected properties:} Highly connected network, central
and "provincial hubs".

\bigskip
\paragraph{Results.}  Central and provincial hubs well identified.
$$
\begin{tabular}{ccc}
  \begin{tabular}{c}
    \epsfig{file=../Figures/macaque_matrix_Q8.ps, clip=, bbllx=95,
      bblly=43, bburx=518, bbury=388, angle=270, width=9cm}
  \end{tabular}
  &
  \qquad
  &
  \begin{tabular}{p{12cm}}
%     \noindent
%     {\small
%       \begin{tabular}{c|rrrrrrrr}
%         & \multicolumn{8}{c|}{$\widehat{\pi}_{q\ell}$ (\%)} \\
%         $q / \ell$& 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
%         \hline
%         1 & 75.0 & 58.9 & 100.0 & 43.7 & 2.8 & 3.6 & 10.0 &   . \\
%         2 & 44.7 & 76.1 & 71.4 & 85.7 & 3.2 & 12.2 & 25.7 &   . \\
%         3 & 100.0 & 42.9 & 45.7 & 50.0 & 55.5 & 28.6 & 20.0 &   . \\
%         4 & 6.2 & 92.8 & 50.0 & 100.0 & 11.1 & 42.9 & 100.0 &   . \\
%         5 & 4.2 & 6.4 & 66.6 & 27.8 & 23.6 & 4.8 & 4.4 &   . \\
%         6 & 8.9 & 12.2 & 28.6 & 42.9 & 12.7 & 76.2 & 31.4 & 1.8 \\
%         7 & 15.0 & 45.7 &   . & 80.0 & 6.7 & 42.9 & 100.0 & 45.0 \\
%         8 &   . &   . &   . & 18.7 &   . & 7.1 & 62.5 & 57.1 \\
%         \hline
%         $\alpha_{\ell}$ & 17.0 & 14.9 & 2.1 & 4.3 & 19.2 & 14.9 & 10.6 & 17.0 \\
%       \end{tabular}
%       }\\
    \paragraph{'Meta-graph':}
    $$
    \epsfig{file=../Figures/cortex_Q8_summary_lim32.ps, clip=,
      bbllx=35, bblly=45, bburx=570, bbury=400, width=12cm}
    $$
    \emphase{Groups $\{4, 7\}$ =  split clique}. 
  \end{tabular}
\end{tabular}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Transcriptional regulatory network of {\sl E. Coli} } 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hspace{-2cm}
\begin{tabular}{cc}
  \begin{tabular}{p{12cm}}
    \paragraph{Data.} Oriented binary network of operons:
    '$i \rightarrow j' = 'i \text{ regulates } j'$. \\
    \smallskip
    \paragraph{Expected properties:}
    sparseness, no feed-back circuits, hierarchical organisation. \\ 
    \smallskip
    \paragraph{Results.}
    Meta Hierarchical structure, Meta Single Input Modules and Feed
    Forward Loops. 
    $$
    \epsfig{file = ../Figures/TRN_Q5_summary.ps, clip=, width=10cm,
      bbllx=30, bblly=55, bburx=570, bbury=370}
    $$
  \end{tabular}
  &
  \begin{tabular}{p{12cm}}
    {\tiny
      \begin{tabular}{c|rrrrr}
        & \multicolumn{5}{c}{$\widehat{\pi}_{q\ell}$ (\%)} \\
        $q / \ell$ & 1 & 2 & 3 & 4 & 5 \\
        \hline
        1         &   .   &   .   &   .   &   .   &   .   \\
        2         & 6.40  & 1.50  & 1.34  &   .   &   .   \\
        3         & 1.21  &   .   &   .   &   .   &   .   \\
        4         &   .   &   .   &   .   &   .   &   .   \\
        5         & 8.64  & 17.65 &   .   & 72.87 & 11.01 \\
        \hline
        $\alpha_{\ell}$  & 65.49 & 5.18  & 7.92  & 21.10 & 0.30  \\
      \end{tabular}
      }
    $$
    \epsfig{file = ../Figures/Colinet_Q5.ps, clip=, width=11cm,
      bbllx=100, bblly=20, bburx=520, bbury=400}
    $$
    $n = 328, \quad x_{++} = 456$.
  \end{tabular}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Fungus - Tree interactions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Dataset.} Interactions between 154 fungi and 51
trees European species. Fungus $f$ is connected to tree $t$ if it has
been collected on it (Data from \refer{VPD08}).

\paragraph{Projected graphs.} For each species we define the projected
graph:
$$
\begin{array}{lrcl}
  \mbox{for trees} \quad & X_{tt'} & = & \mbox{Number of common
  fungi,} \\ 
\\
  \mbox{for fungi} \quad & X_{ff'} & = & \mbox{Number of common trees.}
\end{array}  
$$

\paragraph{Poisson model.} For both species, we assume that the
intensities have Poisson distributions: $X
\sim\Pcal(\lambda_{q\ell})$.

\bigskip
\paragraph{Number of classes.} The ICL criterion selects 
\begin{itemize}
\item \vspace{-0.5cm} 5 classes for trees
\item \vspace{-0.5cm} and 6 classes for fungi.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\noindent
\begin{tabular}{cc}
  \begin{tabular}{p{11cm}}
    \subsection{Fungus network}
    \\ 
    \epsfig{file = ../Figures/Fungus_Poiss_Q6_dotplot.eps,
      width=12cm, height=12cm, clip=} \\
    \begin{itemize}
    \item \vspace{-0.5cm} A group of generalist fungi is detected.
    \item \vspace{-0.5cm} Others are more specific.
    \end{itemize}
  \end{tabular}
  &
  \begin{tabular}{p{11cm}}
    \subsection{Tree network}
    \\
    \epsfig{file = ../Figures/Tree_Poiss_Q5_dotplot.eps,
      width=8cm, height=8cm, clip=}
    \\ 
    \begin{itemize}
    \item \vspace{-0.5cm} Trees are mainly grouped according to
      the number of fungi they host.
    \item \vspace{-0.5cm} Tree groups are less contrasted.
    \end{itemize}
    \\ \\ \\ \\ 
  \end{tabular}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Crossed clusterings}

%\bigskip
\noindent
\begin{tabular}{cc}
  \begin{tabular}{p{16cm}}
    The comparison of the two clusterings exhibits \emphase{specific
    correspondences} between groups of fungi (rows) and trees (columns). \\
    \\ \\
    \paragraph{Interpretation.}
    \begin{itemize}
    \item Group 1 gathers \emphase{generalist fungi}.
    \itemv Tree groups are related to \emphase{phylogenetic origin}.
    \itemv Fungal groups mostly refer to \emphase{nutritional type}.
    \end{itemize}
    \\ \\
    \paragraph{Biclustering.} A direct clustering could be performed
    on the interaction matrix Fungi $\times$ Tree. The method
    proposed by \refer{GoN05} also relies on a
    variational approach. \\ \\
  \end{tabular}
  &
  \begin{tabular}{p{10cm}}
    \epsfig{file = ../Figures/CompCluster_QF6_QT5.eps,
    width=7cm, height=15cm, clip=}
  \end{tabular}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Accounting for covariates (Trees)}

\paragraph{Model.} $d_{ij} =$ taxonomic distance between species $i$
and $j$.
$$
(X_{ij} | Z_{iq}
Z_{j\ell} = 1) \sim \Pcal(\lambda_{q\ell} \gamma^{ d_{ij}})
$$
We get 
$$
\log \widehat{\gamma} = -.317 ~:
\qquad \widehat{\gamma}^{\overline{d}} = .298, 
\qquad \text{for } \overline{d} = 3.82
$$
(i.e. redcution of 70\% of the connexion probability at the mean
distance.)

\paragraph{Parameter estimates (Trees).} \\
\begin{tabular}{cc}
  Without covariate &   With covariate \\
  {\small
    \begin{tabular}{cccccccc}
      $\widehat{\lambda}_{q\ell}$ & T1 & T2 & T3 & T4 & T5 & T6 &
      T7 \\ 
      \hline
      T1 & 14.46 & 4.19 & 5.99 & 7.67 & 2.44 & 0.13 & 1.43 \\
      T2 & 4.19 & 14.13 & 0.68 & 2.79 & 4.84 & 0.53 & 1.54 \\
      T3 & 5.99 & 0.68 & 3.19 & 4.10 & 0.66 & 0.02 & 0.69 \\
      T4 & 7.67 & 2.79 & 4.10 & 7.42 & 2.57 & 0.04 & 1.05 \\
      T5 & 2.44 & 4.84 & 0.66 & 2.57 & 3.64 & 0.23 & 0.83 \\
      T6 & 0.13 & 0.53 & 0.02 & 0.04 & 0.23 & 0.04 & 0.06 \\
      T7 & 1.43 & 1.54 & 0.69 & 1.05 & 0.83 & 0.06 & 0.27 \\
      \hline
      $\widehat{\alpha}_q$ & 7.8 & 7.8 & 13.7 & 13.7 & 15.7 & 19.6 &
      21.6  
    \end{tabular} 
    }
  &
  {\small
    \begin{tabular}{ccccc}
      $\widehat{\lambda}_{q\ell}$ & T'1 & T'2 & T'3 & T'4 \\ 
      \hline
      T'1 & 0.75 & 2.46 & 0.40 & 3.77 \\
      T'2 & 2.46 & 4.30 & 0.52 & 8.77 \\ 
      T'3 & 0.40 & 0.52 & 0.080 & 1.05 \\ 
      T'4 & 3.77 & 8.77 & 1.05 & 14.22 \\
      \hline
      $\widehat{\alpha}_q$ & 17.7 & 21.5 & 23.5 & 37.3
    \end{tabular}
    }
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Cross classification of the groups without and with the
  taxonomic distance as a covariate.} 

\hspace{-2cm} 
\begin{tabular}{ll}
  \begin{tabular}{p{15cm}}
    Groups with covariates: \\
    no group has species belonging exclusively to one or the other of the 
    taxonomic classes (Magnoliophyta or Conipherophyta): \\
    \\
    the association between group of trees and taxonomy was cropped
    out by the covariate. 
  \end{tabular}
  &
  \begin{tabular}{p{10cm}}
    {\small
      \begin{tabular}{c|cccc}
        & T'1 & T'2 & T'3 & T'4 \\
        \hline
        T1 & - & -   &  -  &  4  \\
        T2 & - & -   &  -  &  4  \\
        T3 & 2 & 5   &  -  &  -  \\
        T4 & - & 2   &  -  &  5  \\
        T5 & - & 2   &  -  &  6  \\
        T6 & - & -   & 10  &  -  \\
        T7 & 7 & 2   &  2  &  -
      \end{tabular} 
      }
  \end{tabular} 
\end{tabular} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Discussion \& Work in progress}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inference for heterogeneous valued graphs}

\begin{itemize}
\item Mixture models constitutes a \emphase{natural
    way to describe heterogeneity} in a network.
\item The variational approach is a general and
  \emphase{efficient alternative to MCMC} algorithms.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip\bigskip
\subsection{Applications of the mixture model}

\begin{itemize}
\item 'Realistic' heterogeneous networks can be
  \emphase{simulated} according to mixture models with given
  parameters.
\item Once fitted to a given network, the mixture
  model \emphase{allows to detect unexpetedly frequent motifs} in
  biological (binary) networks (see \emphase{5.1}).
% \item The \emphase{Expected Degree Distribution (EDD)}
%   model (\refer{Park \& Newman, 03}, \refer{Chung \& Lu, 02}) is
%   another special case of heterogeneous binary network:
%   \begin{itemize} 
%   \item For each node $i$, draw its \emphase{expected degree $D_i$} in
%     the empirical degree distribution $F$ of a given network.
%     %: $\{D_i\} \mbox{ i.i.d. }  \sim F$.
%   \item For each couple of nodes $(i, j)$, the edge \emphase{$i \sim
%       j$ exists with probability $D_i D_j/K$}.
%     %: $(X_{ij} |D_i, D_j) \sim \Bcal(D_i D_j /K)$.
%   \end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Extension}
\begin{itemize}
\item The statistical propoerties of variational EM
  are still unclear: '{\sl ... fixed points under variational EM
    cannot be stationary points in likelihood ...}' (\refer{GuB05}).
\item The variational approach \emphase{does not
    provide any measure of the precision} of the estimates. \\
  $\rightarrow$ A \emphase{variational Bayes} approach would provide
  the (approximate) posterior distribution of the parameters (see
  \emphase{5.2}).
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Mixture model as a null model for heterogeneous networks}

\noindent
\begin{tabular}{cc}
  \begin{tabular}{p{15cm}}
    \paragraph{Looking for over-represented motifs in {\sl E. coli}
    transcriptional network.} \\
    \\
    \paragraph{Strategy proposed by \refer{SMM02}.} 
    \begin{enumerate}
    \item Count the number of occurrences $N_{\obs}(\mbf)$;
    \item Resample a \textblue{large number of random networks}
      similar to {\sl E.coli}'s one (using the edge swapping
      algorithm);
    \item Estimate $\Esp \Nm$ and $\Var \Nm$;
%     \item Calculate a $Z$-score: 
%       $Z = (N_{\obs}(\mbf) - \Esp \Nm)/ \sqrt{\Var \Nm}$;
    \item \textblue{Derive a $p$-value} implicitly based on a Gaussian
      approximation. 
    \end{enumerate}
  \end{tabular}
  &
  \begin{tabular}{c}
    \epsfig{file=../FIGURES/RegulationMotifs.ps, bbllx=82, bblly=89,
    bburx=289, bbury=600, clip=, height=16cm}
  \end{tabular}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Direct computation using heterogenous models}

\noindent \hspace{-1cm}
\begin{tabular}{cc}
  \begin{tabular}{p{11cm}}
    \paragraph{Exact moments.} For several heterogeneous models
    (mixture, EDD), we can get the exact formula for the mean $\Esp N$
    and variance $\Var N$ of the count (\refer{PDK08}). \\
    \\ \\
    \paragraph{Distribution.} Based on theoretical results (Erdös)
    and an analogy with sequence motifs, we fit a \emphase{compound
    Poisson} distribution to derive a $p$-value. \\ \\
  \end{tabular}
  &
  \begin{tabular}{p{10cm}}
    {\small \hspace{-1cm}
    \begin{tabular}{crrrr}
      Motif & $N_{\obs}(\mbf)$ & $\lambda$ & $\displaystyle{\frac1{(1-a)}}$ & $p$-value  \\
      \hline
      \epsfig{file = ../figures/Vmotif.eps, width=1cm, clip=} & 14\,113 & 25.5 & 514.9 & 3.36$\,10^{-1}$ \\
      \epsfig{file = ../figures/trianglemotif.eps, width=1cm, clip=} & 75 & 10.4 & 6.2 & 2.87$\,10^{-1}$ \\
      \epsfig{file = ../figures/chainmotif.eps, width=1cm, clip=} & 98\,697 & 11.9 & 7\,543.2 & 3.46$\,10^{-1}$ \\
      \epsfig{file = ../figures/starmotif.eps, width=1cm, clip=} & 112\,490 & 11.4 & 7\,812.0 & 1.85$\,10^{-1}$ \\
      \epsfig{file = ../figures/squaremotif.eps, width=1cm, clip=} & 1\,058 & 5.9 & 82.9 & \emphase{9.34$\,10^{-3}$} \\
      \epsfig{file = ../figures/whisker.eps, width=1cm, clip=} & 3\,535 & 6.4 & 428.7 & 2.22$\,10^{-1}$ \\
      \epsfig{file = ../figures/halfclique.eps, width=1cm, clip=} & 79 & 2.9 & 11.5 & \emphase{2.56$\,10^{-2}$} \\
      \epsfig{file = ../figures/clique.eps, width=1cm, clip=} & 0 & 0.1 & 1.1 & 1.00 \\
    \end{tabular}
    }
  \end{tabular}
\end{tabular}

\paragraph{Results for \emphase{E. coli}'s network.} 2 motifs
appear to be unexpectedly frequent. \\
\\
According to the permutation-based strategy, all of them are
significantly over-represented!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Variational Bayes approach}

\refer{BeG03} propose a 
\begin{itemize}
\item \vspace{-0.5cm} variational
\item \vspace{-0.5cm} Bayes 
\item \vspace{-0.5cm} E-M algorithm 
\end{itemize}
to deal with for incomplete data models in the exponential family context.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \bigskip\bigskip
% \subsection{3-step approximation}

\bigskip\bigskip
\subsection{1 - Variational approximation} 

Denoting $\thetabf$ the set of parameters, for any distribution $Q$,
we have
$$
\log P(\Xbf) 
\geq
\int Q(\Zbf, \thetabf) \log
    \frac{P(\Xbf, \Zbf, \thetabf)}{Q(\Zbf, \thetabf)} \dd \Zbf \dd
    \thetabf 
=: \Fcal(\Xbf, Q).
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{2 - Optimal approximate distribution}

If we choose $Q = \Qt \QZ$, the optimal $\QZ$ and $\Qt$ must satisfy
\begin{eqnarray*}
  \QZ(\Zbf) & \propto & \exp \int \Qt(\thetabf) \log
  P(\Xbf, \Zbf, \thetabf) \dd \thetabf, \\
  \Qt(\thetabf) & \propto & \exp \int \QZ(\Zbf) \log P(\Xbf, \Zbf,
  \thetabf) \dd \Zbf.
\end{eqnarray*}
This can be viewed as a \emphase{mean field} approximation.

\bigskip\bigskip
\paragraph{Proof: Euler's problem.} We look for the function
$Q$ that maximises
$$
\Fcal(Q) = \int L(x, Q(x)) \dd x.
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Idea:} $Q$ maximises $\Fcal(Q)$ if, for any direction $H$,
the directional derivative in direction $H$ is zero, i.e.
$$
\forall H, \qquad \left. \frac{\partial \Fcal(Q +
    tH)}{\partial t} \right|_{t = 0} = 0.
$$
Under regularity conditions, %we can move the derivative into the integral so
\begin{eqnarray*}
  \left. \frac{\partial \Fcal(Q + tH)}{\partial t} \right|_{t = 0} 
  & =
  & \frac{\partial}{\partial t} \int L(x, Q(x) + t H(x)) \dd x \\
  & =
  & \int \left[ \frac{\partial L}{\partial Q(x)} L(x, Q(x))\right]
  H(x) \dd x.
\end{eqnarray*}
The \emphase{fundamental lemma of calculus of variations} states that,
$$
\forall H, \int G(x) H(x) \dd x = 0 \qquad \Rightarrow \qquad G(x) \equiv
0
$$
so the optimal function $Q$ must satisfy the
\emphase{Euler-Lagrange differential equation}:
$$
\frac{\partial L}{\partial Q(x)} L(x, Q(x)) = 0.
$$
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{3 - Exponential family}

Suppose the complete likelihood belongs to the exponential family and
that parameter prior is conjugate
\begin{eqnarray*}
  P(\Xbf, \Zbf | \thetabf) &= & f(\Xbf, \Zbf) g(\thetabf)
  \exp\{\phibf(\thetabf)' \ubf(\Xbf, \Zbf)\}, \\
  \\
  P(\thetabf |\eta, \nubf) & = & h(\eta, \nubf) g(\thetabf)^{\eta} \exp
  \{\phibf(\thetabf)' \nubf\}.
\end{eqnarray*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Variational Bayes E-M algorithm}

The optimal approximate conditional distribution $\Qt$ and $\QZ$ must
satisfy
$$
\begin{array}{rclcrcl}
  \Qt(\thetabf) & \propto & g(\thetabf)^{\tilde{\eta}}
  \exp\{\phibf(\thetabf)' \tilde{\nubf}\},  
  & \quad & \tilde{\eta} & = & \eta + 1, \\
  \\
  \overline{\ubf}(\Xbf) & = & \int \QZ(\thetabf) \ubf(\Xbf, \Zbf) \dd \Zbf;
  & \quad & \tilde{\nubf} & = & \nubf + \overline{\ubf}(\Xbf, \Zbf),
  \\ 
  \\
  \QZ(\Zbf) & \propto & f(\Xbf, \Zbf) \exp \left\{\overline{\phibf}'
    \ubf(\Xbf, \Zbf) \right\}, 
  & \quad &  \overline{\phibf} & = & \int \Qt(\thetabf) \phibf(\thetabf)
  \dd \thetabf. 
\end{array}
$$

\bigskip\bigskip
\paragraph{Iterative algorithm.}
The variational Bayes E-M algorithm consists in alternative updates of
$\Qt$ ('E-step') and $\QZ$ ('M-step'):
$$
\begin{array}{rrcl}
  \text{\bf E-step:} &   \Qt^{t+1}(\thetabf) & = & h(\tilde{\eta},
  \tilde{\nubf}^t) g(\thetabf)^{\tilde{\eta}}
  \exp\{[\phibf(\thetabf)]' \tilde{\nubf}^t\}; \\ 
  \\
  \text{\bf M-step:} &   \QZ^{t+1}(\Zbf) & \propto & f(\Xbf, \Zbf)
  \exp \left\{\left[\overline{\phibf}^{t+1}\right]' \ubf(\Xbf, \Zbf)
  \right\}. 
\end{array}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Application to mixture in networks?}

\paragraph{Interest.}
\begin{itemize}
\item \vspace{-0.5cm} Get \emphase{(approximate) prediction intervals}
  for the parameter;
\item \vspace{-0.5cm} Still avoids costly MCMC algorithms.
\end{itemize}

\bigskip\bigskip
\paragraph{Problems.}
\begin{itemize}
\item \vspace{-0.5cm} The approximate distribution $\QZ$ still needs
  to be restricted (e.g. $\QZ = \prod_i \QZi$);
\item \vspace{-0.5cm} Initialisation (same as E-M);
\item \vspace{-0.5cm} Uniqueness of the fix point?
\item \vspace{-0.5cm} The \emphase{intrinsic identifiability problem}
  of mixture models...
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
{\small
  \bibliography{/Biblio/ARC}
  \bibliographystyle{/Latex/astats}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


