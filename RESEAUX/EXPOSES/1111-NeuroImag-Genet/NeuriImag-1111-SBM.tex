\documentclass{beamer}

% Beamer style
% \usetheme[secheader]{Madrid}
\usetheme{CambridgeUS}
\usecolortheme[rgb={0.65,0.15,0.25}]{structure}
%\usefonttheme[onlymath]{serif}
\beamertemplatenavigationsymbolsempty
%\AtBeginSubsection

% Packages
%\usepackage[french]{babel}
\usepackage[latin1]{inputenc}
\usepackage{color}
\usepackage{dsfont, stmaryrd}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{stmaryrd}
\usepackage{epsfig}
\usepackage{url}
\usepackage{/media/donnees/LATEX/astats}
%\usepackage[all]{xy}
\usepackage{graphicx}

% Commands
\definecolor{darkred}{rgb}{0.65,0.15,0.25}
\newcommand{\emphase}[1]{\textcolor{darkred}{#1}}
%\newcommand{\emphase}[1]{{#1}}
\newcommand{\paragraph}[1]{\textcolor{darkred}{#1}}
\newcommand{\refer}[1]{\textcolor{black}{\sl \cite{#1}}}
\newcommand{\Refer}[1]{\textcolor{black}{\sl #1}}
\newcommand{\newblock}{}

% Symbols
\newcommand{\Abf}{{\bf A}}
\newcommand{\Beta}{\text{B}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\BIC}{\text{BIC}}
\newcommand{\dd}{\text{d}}
\newcommand{\dbf}{{\bf d}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Esp}{\mathbb{E}}
\newcommand{\Ebf}{{\bf E}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Gam}{\mathcal{G}\mbox{am}}
\newcommand{\Ibb}{\mathbb{I}}
\newcommand{\Ibf}{{\bf I}}
\newcommand{\ICL}{\text{ICL}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\Corr}{\mathbb{C}\text{orr}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Vsf}{\mathsf{V}}
\newcommand{\pen}{\text{pen}}
\newcommand{\Fcal}{\mathcal{F}}
\newcommand{\Hbf}{{\bf H}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Kbf}{{\bf K}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Mcal}{\mathcal{M}}
\newcommand{\mbf}{{\bf m}}
\newcommand{\mum}{\mu(\mbf)}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Nbf}{{\bf N}}
\newcommand{\Nm}{N(\mbf)}
\newcommand{\Ocal}{\mathcal{O}}
\newcommand{\Obf}{{\bf 0}}
\newcommand{\Omegas}{\underset{s}{\Omega}}
\newcommand{\Pbf}{{\bf P}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Qcal}{\mathcal{Q}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\sbf}{{\bf s}}
\newcommand{\Sbf}{{\bf S}}
\newcommand{\Scal}{\mathcal{S}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Tbf}{{\bf T}}
\newcommand{\ubf}{{\bf u}}
\newcommand{\Ubf}{{\bf U}}
\newcommand{\Wbf}{{\bf W}}
\newcommand{\xbf}{{\bf x}}
\newcommand{\Xbf}{{\bf X}}
\newcommand{\ybf}{{\bf y}}
\newcommand{\Ybf}{{\bf Y}}
\newcommand{\zbf}{{\bf z}}
\newcommand{\Zbf}{{\bf Z}}
\newcommand{\betabf}{\mbox{\mathversion{bold}{$\beta$}}}
\newcommand{\pibf}{\mbox{\mathversion{bold}{$\pi$}}}
\newcommand{\Sigmabf}{\mbox{\mathversion{bold}{$\Sigma$}}}
\newcommand{\gammabf}{\mbox{\mathversion{bold}{$\gamma$}}}
\newcommand{\mubf}{\mbox{\mathversion{bold}{$\mu$}}}
\newcommand{\nubf}{\mbox{\mathversion{bold}{$\nu$}}}
\newcommand{\Thetabf}{\mbox{\mathversion{bold}{$\Theta$}}}
\newcommand{\thetabf}{\mbox{\mathversion{bold}{$\theta$}}}
\newcommand{\BP}{\text{BP}}
\newcommand{\EM}{\text{EM}}
\newcommand{\VEM}{\text{VEM}}
\newcommand{\VBEM}{\text{VB}}
\newcommand{\cst}{\text{cst}}
\newcommand{\obs}{\text{obs}}
\newcommand{\ra}{\emphase{\mathversion{bold}{$\rightarrow$}~}}
\newcommand{\QZ}{Q_{\Zbf}}
\newcommand{\Qt}{Q_{\theta}}

% Directory
\newcommand{\fignet}{/media/donnees/RECHERCHE/RESEAUX/EXPOSES/FIGURES}
\newcommand{\figmotif}{/media/donnees/RECHERCHE/RESEAUX/MOTIFS/FIGURES}


%--------------------------------------------------------------------
\title[Deciphering Network Structure]{Deciphering Network's Structure
  via the Stochastic Block-Model}   

\author{S. Robin}

\institute[AgroParisTech / INRA]{AgroParisTech / INRA \\
  \bigskip
  \begin{tabular}{ccccc}
    % \epsfig{file=\fignet/LogoINRA-Couleur.ps, width=2.5cm} & 
    % \hspace{.5cm} &
    % \epsfig{file=\fignet/logagroptechsolo.eps, width=3.75cm} & 
    % \hspace{.5cm} &
    % \epsfig{file=\fignet/logo-ssb.eps, width=2.5cm} \\ 
    \epsfig{file=\fignet/LogoINRA-Couleur.ps, width=1.5cm} & 
    \hspace{.5cm} &
    \epsfig{file=\fignet/logagroptechsolo.eps, width=2.25cm} & 
    \hspace{.5cm} &
    \epsfig{file=\fignet/logo-ssb.eps, width=1.5cm} \\ 
  \end{tabular} \\
  \bigskip
  }

\date[Neuroimaging, Paris, 2011]{Neuroimaging - Genetics Workshop, Paris,
  November 2011} 
%--------------------------------------------------------------------

%--------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{document}
%--------------------------------------------------------------------
%--------------------------------------------------------------------

%--------------------------------------------------------------------
\frame{\titlepage}

%--------------------------------------------------------------------
\section{Stochastic block-model}
\frame{\frametitle{Stochastic block-model (SBM)}}

%--------------------------------------------------------------------
\subsection{Understanding network structure}
%--------------------------------------------------------------------
\frame{\frametitle{Understanding network structure}

  \begin{itemize}
  \item Networks constitute a natural way to depict interactions
    between entities.
  \item They are present in many scientific fields (biology,
    sociology, communication, economics, ...).
  \item Most observed networks display an heterogeneous topology, that
    one would like to decipher and better understand. \pause
  \end{itemize}
  \begin{tabular}{ll}
    %\hspace{-.5cm}
    \begin{tabular}{p{.5\textwidth}}
      \paragraph{Dolphine social network.} \\
      %\epsfig{file=../FIGURES/NeG04-Fig11.ps, clip=, width=.5\textwidth}
      \epsfig{file=../FIGURES/NeG04-PhysRevE-Fig11.ps, clip=,
        width=.4\textwidth, bbllx=60, bblly=590, bburx=295,bbury=745}\\
      \refer{NeG04}
    \end{tabular}
    & 
    \hspace{-.5cm}
    \begin{tabular}{p{.5\textwidth}}
      \paragraph{Hyperlink network.} \\
      %\epsfig{file=../FIGURES/NeG04-Fig13.ps, clip=, width=.5\textwidth}
      \epsfig{file=../FIGURES/NeG04-PhysRevE-Fig13.ps, clip=,
        width=.4\textwidth, bbllx=50, bblly=545, bburx=300,bbury=745} 
    \end{tabular} 
  \end{tabular} 
  }

%--------------------------------------------------------------------
\frame{\frametitle{Stochastic Block-Model (SBM)}

  \paragraph{Model based approach} 

  \bigskip\bigskip\pause
  \paragraph{Latent labels:}
  each node $i$ belong to class $k$ with probability $\pi_k$:
  $$
  \Pr\{ i \in k\} = \Pr\{ Z_i = k\} = \pi_k
  $$
  $Z_i$ = label of node $i$.

  \bigskip\pause
  \paragraph{Observed edges} $X_{ij} = \Ibb(i \sim j)$ are conditionally independent given the $Z_i$'s:
  % $$
  % (X_{ij} \;|\; Z_i = k, Z_j = \ell) \sim f_{k\ell}(\cdot)
  % $$
  % where $f_{k\ell}(\cdot)$ is some parametric distribution
  % $f_{k\ell}(x) = f(x; \gamma_{k\ell})$, e.g.
  $$
  \Pr\{i \sim j \;|\; i \in k, j \in \ell\} = 
  \Pr\{X_{ij} = 1 \;|\; Z_i = k, Z_j = \ell\} = \gamma_{k\ell}
  $$
  
  \bigskip\pause
  \paragraph{Inference:} We want to 
  \begin{itemize}
  \item estimate the parameters $\thetabf = (\pibf, \gammabf)$ and
  \item classify the nodes: $\tau_{ik} = \Pr\{Z_i = k \;|\; \Xbf = \xbf\}.$
  \end{itemize}
  }

%-------------------------------------------------------------------- 
\frame{ \frametitle{SBM for a binary social network}

  \vspace{-.5cm}
  \begin{tabular}{ll}
%      \paragraph{An example.}
%      & 
%      \\
      \epsfig{file = \fignet/Karate-Graph.eps, clip=, width=3.5cm,
        height=5cm, angle=270, bllx=50, bblly=80, bburx=530,
        bbury=420} 
      &
      \onslide+<2->{
        \epsfig{file = \fignet/Karate-Graph.eps, clip=, width=3.5cm,
          height=5cm, angle=270, bllx=70, bblly=490, bburx=530,
          bbury=770} 
        }
      \\     
      ~\\
      \begin{tabular}{p{.4\textwidth}}
        \paragraph{Left. Unstructured social network} of friendship
        within a sport club (Zachary data). \\ 
        \\ 
        \onslide+<2->{
          \paragraph{Right. Structured network} in 4 groups: 2 groups
          of leaders and 2 groups of followers. 
          }
      \end{tabular}
      & 
      \begin{tabular}{p{.5\textwidth}}
        \onslide+<3->{
          {\small
            \begin{tabular}{c|rrrr}
              (\%) & \multicolumn{4}{c}{$\widehat{\gamma}_{k\ell}$} \\
              $k / \ell$ &  {1} & 2 & 3 &  4 \\
              \hline
              {1} &  {100} &   {53} &  {16} & {16} \\  
              {2} & - &  {12} & {0} & {7}  \\  
              3 & - & - & 8 & 73 \\
              4 & - & - & - & 100\\
              \hline
              $\widehat{\pi}_{\ell}$        & 9 &  38       & 47    & 6     \\
            \end{tabular}
            }
          }
    \end{tabular}
  \end{tabular}
  }

%--------------------------------------------------------------------
\subsection{Alternatives, extensions and variations}
%-------------------------------------------------------------------- 
\frame{ \frametitle{Extensions and variations}
  \paragraph{Algorithmic approaches:} Looking for communities
  \begin{itemize}
  \item Graph clustering (\refer{GiN02}, \refer{New04}); 
  \item Spectral clustering (\refer{LBB08}).
  \end{itemize}

  \bigskip\pause
  \paragraph{Variations around SBM:} Model-based graph clustering
    \begin{itemize}
    \item Valued (weighted) graphs (\refer{MRV10})
    \item Community structure (\refer{HoW08}),
    \item Mixed-membership (\refer{ABF08}), overlapping groups (\refer{LBA11a})
    \item Continuous version (\refer{DPV10}),
    \item SBM = Step-function version of $W$-random graphs (\refer{LoS06})
    \end{itemize}

  \bigskip\pause
  \paragraph{In this talk:}
  \begin{itemize}
  \item Variational (Bayes) inference for SBM;
  \item Biological and ecological networks, \emphase{including covariates}.
  \end{itemize}
  }


%--------------------------------------------------------------------
\section{Variational inference}
\frame{\frametitle{Variational inference}}
%--------------------------------------------------------------------
\subsection{Maximum likelihood inference}
%-------------------------------------------------------------------- 
\frame{ \frametitle{Maximum likelihood inference}
  \paragraph{Maximum likelihood estimate:} We are looking for
  $$
  \widehat{\thetabf} = \arg\max_{\thetabf} \log P(\Xbf; \thetabf)
  $$
  but $P(\Xbf; \thetabf) = \sum_{\Zbf} P(\Xbf, \Zbf; \thetabf)$ is
  \emphase{not tractable}.

  \bigskip\pause
  \paragraph{Classical strategy:} Based on the decomposition
  $$
  \log P(\Xbf; \thetabf) = \Esp[\log P(\Xbf, \Zbf; \thetabf) | \Xbf] -
  \Esp[\log P(\Zbf|\Xbf; \thetabf) | \Xbf],
  $$
  \pause
  the EM algorithm aims at retrieving the maximum likelihood estimates via
  the alternation of 2 steps.
  \begin{description}
  \item[E-step:] calculation of $P(\Zbf|\Xbf; \widehat{\thetabf})$
    \qquad ('data augmentation').
  \item[M-step:] maximisation of $\Esp[\log P(\Xbf, \Zbf; \thetabf) |
    \Xbf]$ in $\thetabf$. 
  \end{description}
  }

%-------------------------------------------------------------------- 
\frame{ \frametitle{Case of the Stochastic Block-Model}

  \begin{tabular}{cc}
    \hspace{-.5cm}
    \begin{tabular}{p{.5\textwidth}}
      \vspace{-1.5cm}
      \begin{itemize}
      \item \onslide+<1->{Labels are independent;}
      \item \onslide+<2->{Each edge depends on 2 labels;}
      \item \onslide+<3->{These 2 labels are \emphase{dependent
            conditional} on the edge;}
      \item \onslide+<4->{This holds for all pairs;}
      \item \onslide+<5->{The conditional dependency graphs of the
        $Z_i$'s is \emphase{a clique}.}
      \end{itemize}
    \end{tabular}
    & 
    \begin{tabular}{p{.6\textwidth}}
      \begin{overprint}
        \onslide<1>
        \epsfig{file=../FIGURES/FigSBM-Z.eps, clip=, width=0.6\textwidth}
        \onslide<2>
        \epsfig{file=../FIGURES/FigSBM-Z-X12.eps, clip=, width=0.6\textwidth}
        \onslide<3>
        \epsfig{file=../FIGURES/FigSBM-Z-X12-Moral.eps, clip=,
          width=0.6\textwidth} 
%        \onslide<3>
%        \epsfig{file=../FIGURES/FigSBM-Z-X.eps, clip=, width=0.6\textwidth}
        \onslide<4>
        \epsfig{file=../FIGURES/FigSBM-Z-X-Moral.eps, clip=,
          width=0.6\textwidth} 
        \onslide<5->
        \epsfig{file=../FIGURES/FigSBM-ZcondX.eps, clip=,
          width=0.6\textwidth}
      \end{overprint}
    \end{tabular}
  \end{tabular}

  \vspace{-1cm}
  \onslide+<6->{
    % The conditional dependency graph of $\Zbf$ is a \emphase{clique}. \\
    \ra No factorization can be hoped to calculate $P(\Zbf|\Xbf)$.

    \bigskip
    \ra Need to sum over the $K^n$ possible values of $\Zbf$:
    \emphase{intractable}.
    % , so we
    % approximate it with 
    % $$
    % Q^* = \arg\min_{Q \in \Qcal} KL[Q(\Zbf); P(\Zbf|\Xbf)]
    % $$ where $\Qcal$ is a class of \emphase{'manageable'
    %   distributions}, i.e. $\prod_i Q_i(Z_i)$  \\
    % \ra \emphase{'Mean-field'} approximation.  
  } 
}


%--------------------------------------------------------------------
\subsection{Variational inference}
%-------------------------------------------------------------------- 
\frame{ \frametitle{Variational approximation}
  
  As $P(\Zbf|\Xbf)$ can not be calculated, we need to find some
  {approximate} distribution $Q(\Zbf)$ chosen
  % \bigskip \pause
  % \paragraph{Lower bound of the log-likelihood:}
  % For any distribution $Q(\Zbf)$, we have (\refer{Jaa00}, \refer{WaJ08}) 
  % \begin{eqnarray*}
  %   \log P(\Xbf) & \geq & \log P(\Xbf) - \emphase{KL[Q(\Zbf); P(\Zbf|\Xbf)]} 
  %   \pause \\
  %   & = & \Esp_Q[\log \emphase{P(\Xbf, \Zbf)}] -\Esp_Q[\log
  %   Q(\Zbf)]
  % \end{eqnarray*}
  in a class of \emphase{'manageable' distributions}, e.g.
  $$
  Q(Z) = \prod_i Q_i(Z_i)
  $$
  \ra \emphase{'Mean-field'} approximation. 

  \bigskip 
  \begin{itemize}
  \item \pause\emphase{Often used in (neuro-)imaging:} hidden Markov
    fields, conditional random fields (CRF).
  \item \pause See \refer{Ker09} for a review of \emphase{variational
      approximations in neuro-imaging}.
  \item \pause (Approximate) \emphase{Bayesian inference} can be achieved in
    the same framework \\
    \ra 'Variational Bayes' (\refer{BeG03} + \refer{LBA10} for SBM).
  \end{itemize}
  
}

% %-------------------------------------------------------------------- 
% \frame{ \frametitle{Variational EM}
%   \paragraph{'Expectation' step (pseudo E-step):}
%   find the best lower bound of $\log P(\Xbf)$, i.e. the best
%   approximation of $P(\cdot|\Xbf)$ as
%   $$
%   Q^* = \arg\min_{Q \in \Qcal} KL[Q(\Zbf); P(\Zbf|\Xbf)]
%   $$
%   where $\Qcal$ is a class of \emphase{'manageable' distributions}. 
%   \\
%   \ra \emphase{Mean-field} approximation.

%   \bigskip\bigskip\pause
%   \paragraph{Maximisation step (M-step):} estimate $\thetabf$ as
%   $$
%   \widehat{\thetabf} = \arg\max_{\thetabf} \Esp_{\emphase{Q^*}}[\log P(\Xbf,
%   \Zbf; \thetabf)] 
%   $$
%   which maximises the lower bound of $\log P(\Xbf)$.  
%   }

% %--------------------------------------------------------------------
% \frame{ \frametitle{Approximation of $P(\Zbf|\Xbf)$ for SBM}

%   We are looking for
%   $$
%   Q^* = \arg\min_{Q \in \Qcal} KL[Q(\Zbf); P(\Zbf|\Xbf)].
%   $$

%   \pause
%   \begin{itemize}
% %   \item The optimum over all possible distributions is
% %     \emphase{$Q^*(\Zbf) = P(\Zbf|\Xbf)$} ... which can no be
% %     calculated.
%   \item We restrict ourselves to the set of \emphase{factorisable
%       distributions}:
%     $$
%     \Qcal = \left\{Q: Q(\Zbf) = \prod_i Q_i(Z_i) = \prod_i \prod_k
%       \tau_{ik}^{Z_{ik}}\right\}, 
%     \quad
%     \emphase{\tau_{ik} \approx \Pr\{Z_i=k|\Xbf\}}. \pause
%     $$
%   \item The optimal $\tau_{ik}^*$'s satisfy the {fix-point relation}:
%   $$
%   \tau_{ik}^* \propto \pi_k \prod_{j \neq i} \prod_\ell
%   f_{k\ell}(X_{ij})^{\emphase{\tau^*_{j\ell}}}
%   $$
%   also known as \emphase{mean-field} approximation in physics
%   (\refer{Par88}).
%   \end{itemize}
%   }

% %--------------------------------------------------------------------
% \section{Variational Bayes inference}
% \frame{ \frametitle{Variational Bayes inference}}
% %--------------------------------------------------------------------
% \subsection{Variational Bayes approximation}
% %-------------------------------------------------------------------- 
% \frame{ \frametitle{Variational Bayes inference} 

%   \paragraph{Bayesian setting:} Both $\thetabf$ and $\Zbf$ are
%   random and unobserved and we want to retrieve \emphase{$P(\Zbf,
%     \thetabf|\Xbf)$} 
%   \pause 
%   so we look for 
%   $$
%   Q^* = \arg\min_{Q \in \Qcal} KL[Q(\Zbf, \thetabf); P(\Zbf,
%   \thetabf|\Xbf)]
%   $$
%   within $\Qcal = \{Q: Q(\Zbf, \thetabf) =
%   \emphase{Q_Z}(\Zbf)\emphase{Q_\theta}(\thetabf)\}$ \qquad \qquad (... and $\QZ
%   \in \Qcal_{\Zbf}$). 

%   \bigskip\pause
%   \paragraph{VB-EM algorithm:} In the exponential family / conjugate
%   prior context
%   % $$
%   % P(\Xbf, \Zbf, \thetabf) \propto \exp\{\phi(\thetabf)' [u(\Xbf,
%   % \Zbf) + \nubf]\} \pause
%   % $$
%   the optimal $Q^*(\Zbf, \thetabf)$ is recovered (\refer{BeG03}) via
%   \begin{eqnarray*}
%     \emphase{\text{pseudo-M:} \quad \Qt}(\thetabf) 
%     & \propto 
%     & \exp \left(\phi(\thetabf)'
%       \left\{\Esp_{\emphase{Q_Z}}[u(\Xbf, \Zbf)] + \nubf \right\}
%       \right) \\ 
%       \\
%     \emphase{\text{pseudo-E:} \quad \QZ}(\Zbf)     
%     & \propto     
%     & \exp \{ \Esp_{\emphase{Q_\theta}}[\phi(\thetabf)]' u(\Xbf, \Zbf) \}   
%     \pause
%   \end{eqnarray*}
%   See \refer{LBA10} for binary SBM inference.
%   }

%--------------------------------------------------------------------
\section{Regulatory network}
\frame{\frametitle{Regulatory network}}

%--------------------------------------------------------------------
\subsection{Operon network}
%-------------------------------------------------------------------- 
\frame{ \frametitle{Operon network}
  \vspace{-.5cm}\hspace{-.5cm}
  \begin{tabular}{ll}
    \begin{tabular}{p{6cm}}
      \paragraph{Regulatory network =} directed graph where
      \begin{itemize}
      \item \emphase{Nodes =} operons (or groups of genes collocated
        on the genome)
      \item \emphase{Edges =} regulations:
        $$
        \emphase{\{i \rightarrow j\}}
        \quad \Leftrightarrow \quad 
        \emphase{i \text{ regulates } j}
        $$
      \end{itemize}

      \onslide+<3->{
        \paragraph{Questions}
        \begin{itemize}
        \item Do some nodes share similar connexion profiles?
        \item Is there a 'macroscopic' organisation of the network?
        \end{itemize}    
        }
    \end{tabular}
    &
    \begin{tabular}{l}
      \onslide+<2->{
        \hspace{-.75cm}
        \epsfig{file=\fignet/im_EcoliVEM_NB.ps,
          width=.45\textwidth, clip=} 
        }
    \end{tabular}
  \end{tabular}
  }

%-------------------------------------------------------------------- 
\frame{ \frametitle{SBM analysis}

  \vspace{-0.5cm}
  \hspace{-0.5cm}
  \begin{tabular}{cc}      
    \begin{tabular}{p{0.45\textwidth}}
      \paragraph{Parameter estimates.} $K = 5$     \\
      \tiny{$\begin{array}{cccccc}
          \widehat{\gamma}_{k\ell}~(\%) & 1 & 2 & 3 & 4 & 5 \\
          \hline
          1 & . & . & . & . & . \\
          2 & 6.40 & 1.50 & 1.34 & . & . \\
          3 & 1.21 & . & . & . & . \\
          4 & . & . & . & . & . \\
          5 & 8.64 & 17.65 & . & 72.87 & 11.01 \\
          \hline
          \widehat{\pi}~(\%) & 65.49 & 5.18 & 7.92 & 21.10 & 0.30
        \end{array}$}
      \\ 

      \onslide+<3->{
        \paragraph{Meta-graph representation.} \\
        \epsfig{file=\fignet/VEMmetagraphe.ps,
          width=.4\textwidth, clip=}  \\
        }
      \\ 
      \refer{PMD09}
    \end{tabular}
    &
    \begin{tabular}{p{0.45\textwidth}}
      \onslide+<2->{
        \epsfig{file=\fignet/im_EcoliVEM_2.ps,
          width=.45\textwidth, clip=} 
        }
    \end{tabular}
  \end{tabular}
  }

% %-------------------------------------------------------------------- 
% \frame{ \frametitle{Operon network: Comparison of {\VEM} and {\VBEM}}
% %   Network are n = 338 operons, linked if one encodes a
% %   transcription factor that directly regulates the other one. 
%   {\VEM} estimates for the $K=5$ group model lie within the {\VBEM}
%   approximate 90\% credibility intervals (\refer{GDR11}). 

%   {\footnotesize
%     $$
%     \begin{tabular}{cccccc}
%       $\gamma_{k\ell}$ & 1 & 2 & 3 & 4 & 5 \\
%       \hline 
%       1 & 0.03 & 0.00 & 0.03 & 0.00 & 0.00 \\
%       2 & 6.40 & 1.50 & 1.34 & 0.44 & 0.00 \\
%       3 & 1.21 & 0.89 & 0.58 & 0.00 & 0.00 \\
%       4 & 0.00 & 0.09 & 0.00 & 0.95 & 0.00 \\
%       5 & 8.64 & 17.65 & 0.05 & 72.87 & 11.01 \\
%       \hline
%       $\pi$ & 65.49 & 5.18 & 7.92 & 21.10 & 0.30 \\
%       \hline \hline 
%       \onslide+<2->{ 
%         1 & [0.02;0.04] & [0.00;0.10] & [0.01;0.08] & [0.00;0.03] &
%       [0.02;1.34] \\  
%         2 & [6.14;7.60] & [0.61;3.68] & [1.07;3.50] & [0.05;0.54] &
%       [0.33;17.62] \\  
%         3 & [1.20;1.72] & [0.35;2.02] & [0.56;1.92] & [0.03;0.30] &
%       [0.19;10.57] \\  
%         4 & [0.01;0.07] & [0.04;0.51] & [0.01;0.20] & [0.76;1.27] &
%       [0.08;4.43] \\  
%         5 & [6.35;12.70] & [4.60;33.36] & [4.28;24.37] & [63.56;81.28]
%       & [5.00;95.00] \\  
%         \hline 
%         $\pi$ & [59.65;74.38]&  [2.88;6.74] & [5.68;10.77] &
%       [16.02;24.04] & [0.11;1.42] 
%       }
%       \end{tabular} 
%     $$
%     }
  
%   \onslide+<2->{
%     {\VEM} and {\VBEM} estimates for the $K=5$ group model (approximate
%     90\% credibility intervals).
%     }
%   }

%--------------------------------------------------------------------
  \frame{ \frametitle{Approximate posterior $\Qt^*$ distribution via
      $\VBEM$}

  \vspace{-0.5cm}
  $$
  \begin{tabular}{c}
    \includegraphics[width=.7\textwidth]{../FIGURES/im-pi1BVEM}\\        
    \includegraphics[width=.7\textwidth]{../FIGURES/im-pi2BVEM}\\
    \includegraphics[width=.7\textwidth]{../FIGURES/im-pi3BVEM}\\
    \includegraphics[width=.7\textwidth]{../FIGURES/im-pi4BVEM}\\
    \includegraphics[width=.7\textwidth]{../FIGURES/im-pi5BVEM}\\
    \hline 
    \includegraphics[width=.7\textwidth]{../FIGURES/im-alphaBVEM}\\
  \end{tabular}
  $$
  }

%--------------------------------------------------------------------
  \subsection{Macaque cortex}
  \frame{ \frametitle{Another example: Macaque cortex}

  \begin{tabular}{cc}
    \hspace{-.5cm}
    \begin{tabular}{p{.5\textwidth}}
      \paragraph{Nodes:} cortical regions.

      \bigskip
      \psfig{file=../FIGURES/PMD09-BMCbioinfo-Fig3b.ps, clip=,
        width=.5\textwidth} 

      \bigskip 
      \refer{PMD09}
    \end{tabular}
    & 
    \hspace{-1cm}
    \begin{tabular}{p{.5\textwidth}}
      \psfig{file=../FIGURES/PMD09-BMCbioinfo-Fig3a.ps, clip=,
        width=.5\textwidth}        
    \end{tabular}
  \end{tabular}

    
  }

% %--------------------------------------------------------------------
% \frame{ \frametitle{Why does {\VBEM} work so well? (off the record)}
% %-------------------------------------------------------------------- 
%   \emphase{Work in progress:} Daudin \& Celisse are about to prove the 
%   concentration of $P(Z|X)$ around the true value $z^*$, i.e.
%   $$
%   P(Z|X) \underset{n \rightarrow \infty}{\longrightarrow} \delta_{z^*}(Z)
%   $$
%   \bigskip
%   \emphase{Intuition:} If this holds, 
%   \begin{enumerate}[($i$)]
%   \item The limit distribution $\delta_{z^*}(Z)$ belongs to the
%     distribution class over which {\VBEM} approximation achieves
%     maximisation, so it is reached;
%   \item The joint conditional distribution $P(\theta, Z|X) =
%     P(\theta|Z, X) P(Z|X)$ tends to $P(\theta|Z, X) \delta_{z^*}(Z)$.
%     Again $P(\theta|Z, X)$ belongs to the distribution class of
%     {\VBEM}, so it is also reached;
%   \end{enumerate}
%   And the variational approximation tends to be ... exact.
%   }

%--------------------------------------------------------------------
\section{Ecological network (with covariate)}
\frame{\frametitle{Ecological network (with covariate)}}

%--------------------------------------------------------------------
\subsection{Accouting for covariates}
%--------------------------------------------------------------------
%--------------------------------------------------------------------
\frame{ \frametitle{Tree interaction network}
  \begin{tabular}{cc}
    \hspace{-.5cm}
    \begin{tabular}{p{.4\textwidth}}
      \paragraph{Data:} $n = 51$ tree species, \\
      $X_{ij}=$ number of shared parasites (\refer{VPD08}).

      \onslide+<2->{
        \bigskip
        \paragraph{Model:}
        $$
        X_{ij} \sim \Pcal(\lambda_{k\ell}),
        $$
        $\lambda_{k\ell} =$ mean number of shared parasites.
        
        \bigskip
        \paragraph{Results:} ICL selects $K=7$ groups}
      \onslide+<3->{
        that are \emphase{strongly related with phylums}. 
        }
      \end{tabular}
    & 
    \hspace{-.75cm}
    \begin{tabular}{c}
      \onslide+<2->{
        {\tiny
          \begin{tabular}{c|ccccccc}
            $\widehat{\lambda}_{k\ell}$ & T1 & T2 & T3 & T4 & T5 & T6 &
            T7 \\ 
            \hline
            T1 & 14.46 & 4.19 & 5.99 & 7.67 & 2.44 & 0.13 & 1.43 \\
            T2 &  & 14.13 & 0.68 & 2.79 & 4.84 & 0.53 & 1.54 \\
            T3 &  &  & 3.19 & 4.10 & 0.66 & 0.02 & 0.69 \\
            T4 &  &  &  & 7.42 & 2.57 & 0.04 & 1.05 \\
            T5 &  &  &  &  & 3.64 & 0.23 & 0.83 \\
            T6 &  &  &  &  &  & 0.04 & 0.06 \\
            T7 &  &  &  &  &  &  & 0.27 \\
            \hline \hline
            $\widehat{\pi}_k$ & 7.8 & 7.8 & 13.7 & 13.7 & 15.7 & 19.6 &
            21.6  
          \end{tabular}
          }\\
        }
      \onslide+<3->{
        \epsfig{file=\fignet/MRV10_AoAS_Q7_group.eps, width=.6\textheight,
          height=.6\textwidth, clip=, angle=270}
        }
    \end{tabular}
  \end{tabular}
  }

%--------------------------------------------------------------------
\frame{ \frametitle{Including covariates}
  \paragraph{Understanding the mixture components:} Observed clusters
  may be related to exogenous covariates. 
%   \ra Including such covariates enlightens \emphase{residual
%     heterogeneity}.

  \bigskip
  \paragraph{Model-based clustering}  (such as SBM) provide a
  comfortable set-up to account for covariates.

  \bigskip\bigskip\pause
  \paragraph{Generalised linear model.} In the context of exponential
  family, covariates $\ybf$ can be accounted for via a regression term
  $$
  g[\Esp(X_{ij}\;|\;Z_{ik} Z_{j\ell} = 1)] = \gamma_{k\ell} + \ybf_{ij}
  \emphase{\betabf} 
  $$
  where 
  \begin{itemize}
  \item the intercept $\gamma_{k\ell}$ depends on the groups,
  \item the regression parameters \emphase{$\betabf$ does not depend
      on the group} (\refer{MRV10}).
  \end{itemize}
  
  \bigskip\pause 
  Both VEM or VBEM inference can be performed
  (with only a (slight) modification of the M-step).  
  }

%--------------------------------------------------------------------
\frame{ \frametitle{Accounting for the taxonomic distance $d_{ij}$}
  \begin{tabular}{cc}
    \hspace{-.5cm}
    \begin{tabular}{p{.4\textwidth}}
      \paragraph{Model:}
      $$
      X_{ij} \sim \Pcal[\lambda_{k\ell} \; \emphase{\exp(\beta d_{ij})}].
      $$

      \bigskip\pause
      \paragraph{Results:} $\widehat{\beta} = -0.317$. \\
      \ra for $\overline{d} = 3.82$, 
      $$
      e^{\widehat{\beta}\overline{d}} = .298
      $$
      \ra The mean number of shared parasites \emphase{decreases as
        the distance increases}.
      \pause
    \end{tabular}
    & 
    \hspace{-.75cm}
    \begin{tabular}{c}
      {\tiny
        \begin{tabular}{c|cccc}
          $\widehat{\lambda}_{k\ell}$ & T'1 & T'2 & T'3 & T'4 \\ 
          \hline
          T'1 & 0.75 & 2.46 & 0.40 & 3.77 \\
          T'2 &  & 4.30 & 0.52 & 8.77 \\ 
          T'3 &  &  & 0.080 & 1.05 \\ 
          T'4 &  &  &  & 14.22 \\
          \hline \hline
          $\widehat{\pi}_k$ & 17.7 & 21.5 & 23.5 & 37.3 \\
          \hline \hline
          $\widehat{\beta}$ & \multicolumn{4}{c}{-0.317}
        \end{tabular}
        } \\ 
      \pause
      \epsfig{file=\fignet/MRV10_AoAS_Q4_group.eps, width=.5\textheight,
        height=.5\textwidth, clip=, angle=270}
    \end{tabular}
  \end{tabular}
  
  \ra Groups are no longer associated with the phylogenetic
  structure. \\
  \ra Mixture $=$ \emphase{residual heterogeneity} of the regression.
  }

%--------------------------------------------------------------------
\frame{ \frametitle{Comparison of classifications and G-O-F}
  \begin{tabular}{cc}
    \hspace{-.5cm}
    \begin{tabular}{p{.5\textwidth}}
      Accounting for taxonomy deeply modifies the group structure: 
      {\footnotesize
        $$
        \begin{tabular}{c|cccc}
          & T'1 & T'2 & T'3 & T'4 \\
          \hline
          T1 & - & -   &  -  &  4  \\
          T2 & - & -   &  -  &  4  \\
          T3 & 2 & 5   &  -  &  -  \\
          T4 & - & 2   &  -  &  5  \\
          T5 & - & 2   &  -  &  6  \\
          T6 & - & -   & 10  &  -  \\
          T7 & 7 & 2   &  2  &  -
        \end{tabular} 
        $$
        }\pause

      \bigskip
      \paragraph{Goodness of fit} can be assessed via the predicted intensities
      $\widehat{X}_{ij}$ or degrees $\widehat{K}_{i}$.
    \end{tabular}
    & 
    \hspace{-.5cm}
    \begin{tabular}{p{.5\textwidth}}
      \paragraph{Edges' value $X_{ij}$:} \\
      \epsfig{file=\fignet/MRV10_AoAS_FitX_Q4.eps, width=.35\textwidth,
        clip=}

      \paragraph{Degrees $K_i$:} \\
      \epsfig{file=\fignet/MRV10_AoAS_FitK_Q4.eps, width=.35\textwidth,,
        clip=}
    \end{tabular}
  \end{tabular}
  }

%--------------------------------------------------------------------
\section{Conclusion}
\frame{ \frametitle{Conclusion}}
%--------------------------------------------------------------------
\subsection{Conclusion}
%--------------------------------------------------------------------
\frame{ \frametitle{Conclusion}

  \paragraph{Stochastic block-model:} flexible and already widely used
  mixture model to uncover some underlying heterogeneity in networks.

  \bigskip\pause
  \paragraph{Variational inference}
  \begin{itemize}
  \item \emphase{Efficient and scalable} (\Refer{Daudin (2011)}: $n >
    2000$) in terms of computation times (as opposed to MCMC).
  \item Seems to work well, because the conditional distribution
    $P(\Zbf|\Xbf)$ (and therefore $P(\Zbf, \thetabf|\Xbf)$)
    \emphase{asymptotically belongs to the class $\Qcal$} within which
    the optimisation if achieved.
  \item Due to the \emphase{specific asymptotic framework} of
      networks.
  \end{itemize}

  \bigskip\pause
  \paragraph{Alternative methods.} Faster algorithms do exist for
  large graphs:
  \begin{itemize}
  \item Based on the degree distribution (\Refer{Channarond (2011))}
  \item Based on spectral clustering (\refer{RCY10}).
  \end{itemize}
  }

% %--------------------------------------------------------------------
% \subsection{Future work}
% %--------------------------------------------------------------------
% \frame{ \frametitle{Future work}

%   \paragraph{Theoretical properties of variational estimates.}
%   Although the graph context seems favourable, we still need more
%   understanding about variational and variational Bayes inference
%   properties. 

%   \bigskip\pause
%   \paragraph{SBM = discrete version of $W$-graph.} Let 
%   \begin{eqnarray*}
%     \phi :  [0 ,1]^2 & \rightarrow & [0, 1] \\
%     \{Z_i\} \text{ i.i.d.} & \sim & \Ucal[0, 1] \\
%     \{X_{ij}\} \text{ indep.} | \{Z_i\} & \sim & \Bcal[\phi(Z_i,
%     Z_j)]
%   \end{eqnarray*}
%   \begin{itemize}
%   \item Approximation of the $\phi(u, v)$ function by a step function
%     $\gamma_{k\ell}$ (SBM)
%   \item Model averaging based on optimal variational weights
%     (\Refer{Volant (2011)})
%   \end{itemize}
%   }

%--------------------------------------------------------------------
\subsection{Acknowledgements}
%--------------------------------------------------------------------
\frame{ \frametitle{Acknowledgements}

  \paragraph{People:} \\
  A.~Célisse, A.~Channarond, J.-J.~Daudin, S.~Gazall, M.~Mariadassou,
  V.~Miele, F.~Picard, C.~Vacher



  \bigskip
  \paragraph{Grant:} \\
  Supported by the French Agence Nationale de la
  Recherche \\
  \centerline{\emphase{NeMo} project ANR-08-BLAN-0304-01}  
  \pause

  \bigskip
  \paragraph{Softwares:}
  \begin{itemize}
  \item Stand-alone \emphase{MixNet}: \\
    \centerline{\url{stat.genopole.cnrs.fr/software/mixnet/}}
  \item R-package \emphase{Mixer}: \\
    \centerline{\url{cran.r-project.org/web/packages/mixer/index.html}}
  \item R-package \emphase{NeMo}: Network motif detection \\
    in preparation
  \end{itemize}
  }


%--------------------------------------------------------------------
{\tiny
  \bibliography{/media/donnees/Biblio/ARC} %,/Biblio/AST,/Biblio/SSB}
  \bibliographystyle{/media/donnees/LATEX/astats}
  %\bibliographystyle{plaine}
  }

%--------------------------------------------------------------------
\appendix
\section{Appendix}
\frame{ \frametitle{Appendix} }

%-------------------------------------------------------------------- 
\frame{ \frametitle{Few more about inference} 
  \paragraph{Identifiability.} Even for binary edges, MixNet (SBM) is
  identifiable (\refer{AMR09}) ... although mixtures of Bernoulli are
  not.

  \bigskip\bigskip\pause
  \paragraph{Model selection.} 
  \begin{itemize}
  \item \refer{DPR08} propose the penalised criterion
    $$
    ICL(K) = \Esp_{Q^*}[\log P(\Zbf, \Xbf)] - \frac12 \left\{(K-1)\log n + K^2
      \log[n(n-1)/2]\right\}.
    $$
  \item \pause The difference between ICL and BIC is the \emphase{entropy
      term $\Hcal(Q^*)$} ... which is almost zero (due to the
    concentration of $P(\Zbf|\Xbf)$).
  \item \pause BIC and ICL-like criteria are also considered in
    \refer{LBA11b} for SBM in the context of variational Bayes
    inference.
  \end{itemize}
 }
  
%--------------------------------------------------------------------
\subsection{Quality of the variational Bayes approximation}
%-------------------------------------------------------------------- 
\frame{ \frametitle{Variational Bayes approximation: Simulation Study}

  Few is known about the properties of variational-bayes inference:
  \begin{itemize}
  \item Consistency is proved for \emphase{some incomplete data models}
    (\refer{McT09}).
  \item In practice, VB-EM often under-estimates the posterior variances.
  \end{itemize}

  \bigskip\pause
  \paragraph{Simulation design:} 
  \begin{itemize}
  \item \pause 2-group binary SBM with parameters with 2 scenarios
    $$
    \pi=\left(\begin{array}{cc}
        0.6 & 0.4
      \end{array}\right),  
    \qquad
    \gamma=\left(\begin{array}{cc}
        0.8 & 0.2 \\
        0.2 & \emphase{0.5/0.3}
      \end{array}\right)
    $$
  \item \pause Comparison of 4 methods: EM (when possible), {\VEM}, BP and
    {\VBEM}
%     ($n = 18$: because of the computation time required by for EM). 
  \item \pause Belief Propagation (BP) algorithm: 
%     Provides a specific approximation for the second term in
    $$
    \Esp_Q[\log P(\Xbf, \Zbf)] = \sum_{i, k}
    \underset{\emphase{\normalsize
        \tau_{ik}}}{\underbrace{\Esp_Q[Z_{ik}]}} \log \pi_k + \sum_{i,
      j} \sum_{k, \ell} \underset{\emphase{\normalsize
        \Delta_{ijk\ell} \neq
        \tau_{ik}\tau_{j\ell}}}{\underbrace{\Esp_Q[Z_{ik} Z_{j\ell}]}}
    \log f(X_{ij}; \gamma_{k\ell}).
    $$
  \item \pause 500 graphs are simulated for each scenario and each graph
    size.
  \end{itemize}
  }

%--------------------------------------------------------------------
\frame{ \frametitle{Estimates, standard deviation and likelihood}

  \paragraph{Comparison on small graphs ($n = 18$):}
  {\small
    \begin{tabular}{ccccccccc} 
      & $\pi_1$ & $\gamma_{11}$ & $\gamma_{12}$ & $\gamma_{22}$ &
      $\log P(X)$ \\   
      \hline
      \emphase{Scenario 1} & 60\% & 80\% & 20\% & 50\% & \\ 
      \hline 
      EM & 59.1 (13.1) & 78.5 (13.5) & 20.9 (8.4) & 50.9 (15.4) & -90.68 \\  
      {\VEM} & 57.7 (16.6) & 78.8 (12.4) & 22.4 (10.7) & 50.3 (14.6) & -90.87\\  
      BP & 57.9 (16.2) & 78.9 (12.3) & 22.2 (10.5) & 50.3 (14.5) & -90.85 \\  
      {\VBEM} & 58.1 (13.3) & 78.2 (9.7) & 21.6 (7.7) & 50.8 (13.3) & -90.71 \\ 
      \hline \\
      \hline
      \emphase{Scenario 2} & 60\% & 80\% & 20\% & 30\% &  \\ 
      \hline
      EM & 59.5 (14.1) & 78.7 (15.6) & 21.2 (8.7) & 30.3 (14.3) & -88.18 \\  
      {\VEM} & 55.6 (19.0) & 80.1 (14.0) & 24.0 (11.8) & 30.8 (13.8) & -88.54 \\  
      BP & 56.6 (17.8) & 80.0 (13.6) & 23.2 (11.0) & 30.8 (13.8) & -88.40 \\  
      {\VBEM} & 58.4 (14.6) & 77.9 (12.0) & 22.3 (9.3) & 32.1 (12.3) & -88.26 \\  
    \end{tabular} 
    }
  \bigskip\pause
  \begin{itemize}
  \item All methods provide similar results.
  \item {\EM} achieves the best ones.
  \item Belief propagation ({\BP}) does not significantly improve {\VEM}.
  \end{itemize}
  }

%--------------------------------------------------------------------
\frame{ \frametitle{Influence of the graph size}
  Comparison of \textcolor{red}{{\VEM}: $\bullet$} and
  \textcolor{blue}{{\VBEM}: $+$} in scenario 2 (most difficult). \\
  Left to right: $\pi_1$, $\gamma_{11}$, $\gamma_{12}$, $\gamma_{22}$.

  \bigskip
  \emphase{Means.} \\
  \includegraphics[width=1\textwidth]{../FIGURES/im-etudnVB1} \\
  
  \pause
  \emphase{Standard deviations.} \\
  \includegraphics[width=1\textwidth]{../FIGURES/im-etudnVB2}
  
  \begin{itemize}
  \item {\VBEM} estimates converge more rapidly than {\VEM} estimates.
  \item Their precision is also better.
  \end{itemize}
  }

%--------------------------------------------------------------------
\frame{ \frametitle{{\VBEM} Credibility intervals}

  \paragraph{Actual level as a function of $n$:}   $\pi_1$: $+$,
  $\gamma_{11}$: \textcolor{red}{$\triangle$}, $\gamma_{12}$:
  \textcolor{blue}{$\circ$}, $\gamma_{22}$: \textcolor{green}{$\bullet$}
  $$
  \includegraphics[width=1\textwidth]{../FIGURES/im-ICQ2-2-new} 
  $$

  \pause
  \begin{itemize}
  \item For all parameters, {\VBEM} posterior credibility intervals
    achieve the nominal level (90\%), as soon as $n \geq 30$.
  \item \ra \emphase{The {\VBEM} approximation seems to work well}.
%   \item These may be due to the concentration of $P(Z|X)$ around the
%     true value of $Z$ (work in progress).
  \end{itemize}
  }

%--------------------------------------------------------------------
\frame{ \frametitle{Convergence rate of the {\VBEM} estimates}
  \emphase{Width of the posterior credibility intervals.}
  {$\pi_1$}, \textcolor{red}{$\gamma_{11}$},
  \textcolor{blue}{$\gamma_{12}$}, \textcolor{green}{$\gamma_{22}$}
  \\
  \includegraphics[width=1\textwidth]{../FIGURES/im-ICQ2-3} \\

  \pause
  \begin{itemize}
  \item The width decreases as $1/\sqrt{n}$ for $\pi_1$.
  \item It decreases as $1/n = 1/sqrt{n^2}$ for $\gamma_{11}$,
    $\gamma_{12}$ and $\gamma_{22}$.
  \item Consistent with the penalty of the ICL criterion
    proposed by \refer{DPR08} (see next slide).
  \end{itemize}
  }

%--------------------------------------------------------------------
%--------------------------------------------------------------------
\end{document}
%--------------------------------------------------------------------
%--------------------------------------------------------------------

  \begin{tabular}{cc}
    \hspace{-.5cm}
    \begin{tabular}{p{.5\textwidth}}
    \end{tabular}
    & 
    \hspace{-.5cm}
    \begin{tabular}{p{.5\textwidth}}
    \end{tabular}
  \end{tabular}


