\documentclass[dvips, lscape]{foils}
%\documentclass[dvips, french]{slides}
\textwidth 18.5cm
\textheight 25cm 
\topmargin -1cm 
\oddsidemargin  -1cm 
\evensidemargin  -1cm

% Maths
\usepackage{amsfonts, amsmath, amssymb}

\newcommand{\coefbin}[2]{\left( 
    \begin{array}{c} #1 \\ #2 \end{array} 
  \right)}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Hbf}{{\bf H}}
\newcommand{\Bcal}{\mathcal{B}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Tcal}{\mathcal{T}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\alphabf}{\mbox{\mathversion{bold}{$\alpha$}}}
\newcommand{\betabf}{\mbox{\mathversion{bold}{$\beta$}}}
\newcommand{\gammabf}{\mbox{\mathversion{bold}{$\gamma$}}}
\newcommand{\psibf}{\mbox{\mathversion{bold}{$\psi$}}}
\newcommand{\taubf}{\mbox{\mathversion{bold}{$\tau$}}}
\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Sbf}{{\bf S}}
% \newcommand{\bps}{\mbox{bps}}
\newcommand{\ubf}{{\bf u}}
\newcommand{\vbf}{{\bf v}}
\newcommand{\Esp}{{\mathbb E}}
% \newcommand{\Var}{{\mathbb V}}
\newcommand{\Ibb}{{\mathbb I}}
%\newcommand{\liste}{$\bullet \quad$}
\newcommand{\lFDR}{\ell FDR}

% Couleur et graphiques
\usepackage{color}
\usepackage{graphics}
\usepackage{epsfig} 
\usepackage{pstcol}

% Texte
\usepackage{lscape}
\usepackage{../../../../Latex/fancyheadings, rotating, enumerate}
%\usepackage[french]{babel}
\usepackage[latin1]{inputenc}
\definecolor{darkgreen}{cmyk}{0.5, 0, 0.5, 0.5}
\definecolor{orange}{cmyk}{0, 0.6, 0.8, 0}
\definecolor{jaune}{cmyk}{0, 0.5, 0.5, 0}
\newcommand{\textblue}[1]{\textcolor{blue}{#1}}
\newcommand{\textred}[1]{\textcolor{red}{#1}}
\newcommand{\textgreen}[1]{\textcolor{green}{ #1}}
\newcommand{\textlightgreen}[1]{\textcolor{green}{#1}}
%\newcommand{\textgreen}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\textorange}[1]{\textcolor{orange}{#1}}
\newcommand{\textyellow}[1]{\textcolor{yellow}{#1}}
\newcommand{\refer}[2]{{\sl #1}}

% Sections
%\newcommand{\chapter}[1]{\centerline{\LARGE \textblue{#1}}}
% \newcommand{\section}[1]{\centerline{\Large \textblue{#1}}}
% \newcommand{\subsection}[1]{\noindent{\Large \textblue{#1}}}
% \newcommand{\subsubsection}[1]{\noindent{\large \textblue{#1}}}
% \newcommand{\paragraph}[1]{\noindent {\textblue{#1}}}
% Sectionsred
\newcommand{\chapter}[1]{
  \addtocounter{chapter}{1}
  \setcounter{section}{0}
  \setcounter{subsection}{0}
  {\centerline{\LARGE \textblue{\arabic{chapter} - #1}}}
  }
\newcommand{\section}[1]{
  \addtocounter{section}{1}
  \setcounter{subsection}{0}
  {\centerline{\Large \textblue{\arabic{chapter}.\arabic{section} - #1}}}
  }
\newcommand{\subsection}[1]{
  \addtocounter{subsection}{1}
  {\noindent{\large \textblue{\arabic{chapter}.\arabic{section}.\arabic{subsection} - #1}}}
  }
\newcommand{\paragraph}[1]{\noindent{\textblue{#1}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\begin{tabular}{ll}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\landscape
\newcounter{chapter}
\newcounter{section}
\newcounter{subsection}
\setcounter{chapter}{0}
\headrulewidth 0pt 
\pagestyle{fancy} 
\cfoot{}
\rfoot{\begin{rotate}{90}{
      \hspace{1cm} \tiny S. Robin: Differential analysis of microarrays
      }\end{rotate}}
\rhead{\begin{rotate}{90}{
      \hspace{-.5cm} \tiny \thepage
      }\end{rotate}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{center}
  \textblue{\LARGE Differential analysis of microarray data} 

   \vspace{1cm}
   {\large S. {Robin}} \\
   robin@inapg.inra.fr

   {UMR INA-PG / ENGREF / INRA, Paris} \\
   {Mathématique et Informatique Appliquées}
   
    \vspace{1cm}
    {Microarray Design and Statistical Analysis} \\
    {Lisbon, March 2006}
\end{center}

\paragraph{Outline}

1 - Test for one gene

2 - Multiple testing problem

3 - Mixture model and local FDR

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Test for one gene} 
\bigskip
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip
\section{Differentially expressed gene} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Question:} Does the expression level of a given gene differ
from condition (or treatment) $A$ to condition $B$? (\textblue{``class
  comparison''}) 

\paragraph{Data:} Replicate measurements of the expression level of
a given gene under the 2 conditions
$$
\begin{tabular}{cccc}
  Condition & Measurements & (Mean) & (Variance) \\
  \\
  $A$: & $X_{A1}, \dots, X_{AR_A}$ & $\overline{X}_{A}$ & $\hat{\sigma}^2_{A}$ \\
  \\
  $B$: & $X_{B1}, \dots, X_{BR_B}$ & $\overline{X}_{B}$ & $\hat{\sigma}^2_{B}$ \\
\end{tabular}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Differential score}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Criterion:} \\
A test statistic is used to measure this difference. \\
The $t$-statistic is one of the most popular
$$
T = \frac{\overline{X}_{A} - \overline{X}_{B}}{\hat{\sigma}
  \sqrt{\frac1{R_A} + \frac1{R_B}}} \qquad \mbox{where} \quad
\hat{\sigma}^2 = \frac{(R_A - 1)\hat{\sigma}^2_A + (R_B -
  1)\hat{\sigma}^2_B}{R_A + R_B - 2}
$$

The choice of the criterion is partially arbitrary.

For a given gene $g$, should $T_g$ be based 
$$
\mbox{on}\qquad
\overline{X}_{Ag} - \overline{X}_{Bg}
\qquad \mbox{or on}\qquad
(\overline{X}_{Ag} - \overline{X}_{Bg}) - (\overline{X}_{A\bullet} - \overline{X}_{B\bullet})?
$$
This depends on the definition of ``differentially expressed genes''.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Case of paired data:} we then deal with differences 
$$
Y_r = X_{Ar} - X_{Br}, \qquad \mbox{for } 1 \leq r \leq R
$$
and use the statistic
$$
T = \frac{\overline{Y}}{\hat{\sigma} / \sqrt{R}} \qquad \mbox{where
  $\hat{\sigma}^2$ is the estimated variance of the $\{Y_r\}$'s}
$$

\paragraph{Hypothesis:} \\
To assess the significancy of the difference, we compare the observed
score $T$ to what could have been observed under $\Hbf_0$:
$$
\Hbf_0 = \{\mbox{the gene is not differentially expressed}\}.
$$

$\Hbf_0$ is called the ``null hypothesis''.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Decision rule:} 
$$
\begin{tabular}{rcl}
  If & \qquad & $T$ is ``too large'' \\
  \\
  then & & \textblue{reject} $\Hbf_0$
\end{tabular}
$$

\vspace{1cm}
\paragraph{Risks:} \\
\centerline{
  \begin{tabular}{cc|cc}
    \multicolumn{2}{c|}{risk} & \multicolumn{2}{c}{decision} \\
    & & accept $\Hbf_0$ & reject $\Hbf_0$ \\
    \hline
    & $\Hbf_0$ true & $1-\alpha$ & $\alpha$ \\
    truth & \\
    & $\Hbf_0$ false & $\beta$ & $1-\beta$ \\
  \end{tabular}
}

\paragraph{Conservative approach} aims to control the risk $\alpha$ first.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Parametric approach}

Assuming that the expression levels are Gaussian with the same
variance
$$
\forall r, X_{Ar} \sim \Ncal(\mu_A, \sigma^2), 
\qquad
\forall r, X_{Br} \sim \Ncal(\mu_B, \sigma^2)
$$
hypothesis $\Hbf_0$ is
$$
\Hbf_0 = \{\mu_A = \mu_B\}
$$
and ``under $\Hbf_0$'', $T$ has a student distribution
$$
\begin{array}{ll}
  \mbox{unpaired data:} &
  T \underset{\Hbf_0}{\sim} \Tcal_{R_A + R_B - 2}, \\
  \\
  \mbox{paired data:} &
  T \underset{\Hbf_0}{\sim} \Tcal_{R-1}.
\end{array}
$$

For large $(R_A + R_B)$ (or $R$), $T \approx \Ncal(0, 1)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Decision rule:} \\
For a given level $\alpha$, calculate the threshold $t(\alpha)$ such as, 
$$
\Pr\left\{ |T| > t(\alpha)\right\} = \alpha
$$
(ex. t(5\%) = 1.96 for $\Ncal(0, 1)$ ie $\Pr\{\Ncal(0, 1) > 1.96\} = 2.5\%$)
$$
\begin{pspicture}(20, 10)
  \rput[b](10, -1){ 
    \epsfig{figure=../Figures/Rejet-N01.ps, height=11cm, width=22cm, clip=}    
    }
  
  \rput[B](2.5, 9.5){down-regulated}
  \rput[B](2.5, 8.5){genes}

  \rput[B](10, 9.5){not regulated genes}

  \rput[B](17.5, 9.5){up-regulated}
  \rput[B](17.5, 8.5){genes}

  \rput[B](16.5, 3){$\alpha/2$}
  %\psline[linewidth=0.1]{<-}(15, 1)(16.5, 3)

  \rput[B](4.5, 3){$\alpha/2$}
  %\psline[linewidth=0.1]{<-}(5, 1)(3.5, 3)

  \rput[B](10.5, 4){$1 - \alpha$}

  \rput[B](10, -0.6){$\Hbf_0$ accepted}
  \rput[B](18, -0.6){$\Hbf_0$ rejected}
  \rput[B](2.7, -0.6){$\Hbf_0$ rejected}

\end{pspicture}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{$p$-value:} \\
Consider the probability to get a score larger than the
observed one $T_g$: $P_g = \Pr_{\Hbf_0}\{|T| > |T_g|\}$. \\
$$
\begin{pspicture}(20, 8.7)(0, -1.3)
  \rput[b](10, -2){ 
     \epsfig{figure=../Figures/pvalue-N01.ps, height=12cm, width=24cm, clip=}    
    }

  \rput[B](17.5, 3){$P_g/2$}
  %\psline[linewidth=0.1]{<-}(15, 0.8)(16.7, 3)

  \rput[B](3.5, 3){$P_g/2$}
  %\psline[linewidth=0.1]{<-}(5, 0.8)(3.5, 3)

  \rput[B](4.5, -0.6){$-|T_g|$}
  \rput[B](16.5, -0.6){$+|T_g|$}
\end{pspicture}
$$
Gene $g$ is declared differentially expressed if $ P_g < \alpha$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{tabular}{lll}
  \mbox{\textblue{Data}} & & \mbox{\textblue{Test}} \\
  \\
  $
  \begin{array}{rccc}
     & ~~X_A & ~~X_B & ~~Y \\
    \\
    \hline
    \\
    1 & ~~4.01 & ~~4.09 & -0.08 \\
    2 & ~~0.84 & ~~0.97 & -0.12 \\
    3 & ~~4.45 & ~~3.92 & ~~0.53 \\
    4 & ~~4.73 & ~~6.01 & -1.28 \\
    5 & ~~6.16 & ~~6.01 & ~~0.15 \\
    6 & ~~4.23 & ~~6.48 & -2.26 \\
    7 & ~~4.70 & ~~5.85 & -1.15 \\
    8 & 10.65 & 11.02 & -0.37 \\
    9 & ~~2.02 & ~~4.18 & -2.16 \\
    10 & ~~3.96 & ~~5.19 & -1.23 \\
    \\
    \hline
    \\
    \mbox{mean}& ~~4.58 & ~~5.37 & -0.80 \\
    \\
    \mbox{std} & ~~2.60 & ~~2.55 & ~~0.96 \\
  \end{array}
  $
  &
  &
  \begin{tabular}{l}
    $\begin{array}{rl}
      \Hbf_0 & = \{\Esp(X_A) = \Esp(X_B)\} \\
      \\
      & = \{\Esp(Y) = 0\}
      \\
      \\
    \end{array}$
    \\
    $
    \begin{array}{rcc}
      & \mbox{unpaired} & \mbox{paired}  \\
      \\
      \hline
      \\
      \overline{X}_A - \overline{X}_B & -0.80 & -0.80 \\
      \\
      S & ~~2.58 & ~~0.96 \\
      \\
      T & -0.69 & -2.62 \\
      \\
      \hline
      \\
      \mbox{dist.}|\; \Hbf_0 & ~~\Tcal_{18} & ~~\Tcal_{9} \\ 
      \\
      p-\mbox{value} & ~~0.50 & ~~0.03 \\
    \end{array}
    $
    \end{tabular}
\end{tabular}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Power of the $t$-test:} power measures the ability to 
detect a given difference $\delta$ \\
\centerline{
  \begin{pspicture}(25, 15)
    \rput[br](24, 13){unpaired data, $R = $ number of replicates
    $\rightarrow 2R$ data}
    \rput[bl](0, 0){ 
      \epsfig{figure=../Figures/PowerT.ps, height=15cm, width=25cm, clip=}    
      }
    \rput[B](12.5, 1){$\delta = (\mu_A - \mu_B)/\sigma$}
    \rput[B](16.25, 8){$R = 2$}
    \rput[B](10.75, 9){$4$}
    \rput[B](8.5, 9.5){$8$}
    \rput[B](4.75, 10){$64$}
  \end{pspicture}
}
% $$
% \begin{array}{c}
%   1 - \beta = \Pr\{\mbox{gene declared significant} \;|\; \delta\} \\
%   \epsfig{figure=../Figures/PowerT.ps, height=15cm, width=25cm, clip=} \\
%   \vspace{-2.5cm}
%   \\
%   \delta = (\mu_A - \mu_B)/\sigma
% \end{array}
% $$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Golub data}

$R_A = 27$ patients with AML, $R_B = 11$ with ALL, 7070 genes.

\paragraph{Gene ranking}
$$
\begin{tabular}{crrrrrr}
  & \quad~ & \multicolumn{2}{c}{student test } & \quad~ & \multicolumn{2}{c}{Welch
  test} \\
  rank & & $T_1$ & $p$-value & & $T_1$ & $p$-value \\
  \hline
  1  & & -8.32 &    $< 10^{-16}$  & & -8.09 & 6.66\;$10^{-16}$ \\
  2  & &  5.16 & 2.37\;$10^{-7}$  & &  7.90 & 2.66\;$10^{-15}$ \\
  3  & &  4.57 & 4.81\;$10^{-6}$  & &  6.80 & 1.02\;$10^{-11}$ \\
  4  & & -8.86 &    $< 10^{-16}$  & & -6.43 & 1.22\;$10^{-10}$ \\
  5  & &  4.45 & 8.37\;$10^{-6}$  & &  6.29 & 3.09\;$10^{-10}$ \\
  6  & &  4.37 & 1.20\;$10^{-5}$  & &  6.28 & 3.35\;$10^{-10}$ \\
  7  & & -6.74 & 1.55\;$10^{-11}$ & & -6.26 & 3.65\;$10^{-10}$ \\
  8  & &  4.10 & 4.11\;$10^{-5}$  & &  6.21 & 5.05\;$10^{-10}$ \\
  9  & &  3.94 & 7.97\;$10^{-5}$  & &  6.18 & 6.34\;$10^{-10}$ \\
  10 & & -5.79 & 6.86\;$10^{-9}$  & & -6.14 & 7.93\;$10^{-10}$ \\
\end{tabular}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{tabular}{l}
  \paragraph{Comparison} ($\alpha = 5 \%$) \\
  \\
  \\
  \begin{tabular}{c|cc|c}
    & $N_2$ & $P_2$ & \\
    & & & \\
    \hline
    & & & \\
    $N_1$ & 4974 &  345 & 5319 \\
    & & & \\
    $P_1$  &  209 & 1542 & 1751 \\
    & & & \\
    \hline
    & & & \\
    &    5183 & 1887 & 7070
  \end{tabular} \\
  \\
  $P = $ positive  (significant) \\
  \\
  $N = $ negative \\
\end{tabular}
\begin{tabular}{l}
  \epsfig{figure=../Figures/Golub-pval.eps, height=15cm, width=15cm,
    bbllx=64, bblly=209, bburx=542, bbury=585, clip}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Non-parametric approach} 

\vspace{-0.5cm}
\paragraph{Permutation tests / bootstrap:} 
avoid parametric assumption (e.g. Gaussian distribution)

The distribution of $T_g$ under $\Hbf_0$ is estimated assigning ``a
large number of times'' (denoted $S$) the $(R_A + R_B)$ values
($X_{A1}$, \dots, $X_{AR_A}$, $X_{B1}$, \dots, $X_{BR_B}$) randomly to
conditions $A$ or $B$.

Each permutation $s$ provides a pseudo value $\tilde{T}^s_g$.

The $p$-value associated to $T_g$ is estimated by the proportion of
pseudo values $T_g^s$ exceeding $T_g$: 
$$
\hat{p} = \frac{\mbox{number of permutations where }|T^s_g| >
  |T_g|}{\mbox{total number of permutations }(S)}.
$$
$(R_A+R_B)$ has to be large enough to estimate small $p$-values: $S \leq
\coefbin{R_A+R_B}{R_B}$:
$$
\coefbin{8}{4} = 70, \qquad\coefbin{10}{5} = 252.
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Variance modeling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Power of the test:}
Power is ability to detect a (small) difference, that is the probability to
reject $\Hbf_0$ when it is false.

\begin{description}
\item[One gene:] the power is related to the number of replicates and
  increases with the number of replicates $n = R_A+R_B$ \\
  but is also related to the precision with which the variance
  $\sigma^2_g$ is estimated.
\item[$G$ genes:] assuming that the variance is the \textblue{same for
    all genes} improves the power and \textblue{stabilizes} the
  statistics $T_g$ (avoid large $T_g$ due to small $\hat{\sigma}_g$).
\end{description}

\paragraph{Gene specific variance} 
requires a gene-by-gene estimate $\hat{\sigma}^2_g$ and lead to a
low-power testing procedure.
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Homoscedasticity hypothesis:} 
  $\sigma^2_g \equiv \sigma^2$ is not realistic.
$$
\epsfig{figure=../Figures/ResiduLoessA-Swap700-700.eps, height=15cm,
      width=25cm, bbllx=15, bblly=0, bburx=560, bbury=360, clip=}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Variance modeling} 
provides an intermediate solution 
\begin{itemize}
\item use a weighted mean of $\hat{\sigma}^2_g$ and
  $\hat{\sigma}^2$: $s^2_g = a \hat{\sigma}^2_g + (1-a)
  \hat{\sigma}^2$
\item add a ``large'' constant to $\hat{\sigma}^2_g$ (to
  avoid false positives)
\item model $s^2_g$ as a function of the mean expression $m_g$: $s^2_g
  = f(m_g)$ 
\item use a mixture model for the variance estimates
  $$
  (R_A+R_B-2) \hat{\sigma}^2_g 
  \sim
  \sum_{k=1}^K \pi_k \; s^2_k \; \chi^2_{R_A+R_B-2}
  $$
\end{itemize}
\refer{Rudemo \& al.~(02)}{}, \refer{Huber \& al.~(02)}{},
\refer{Delmar \& al.~(03)}{submitted}, \refer{Tusher \& al.~(01)}{}

\paragraph{Problem:} 
what is the null distribution of $T_g = \Delta_d / \hat{\sigma}_g$?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\vspace{-2cm}
$$
\epsfig{figure=../Figures/RawVariance-Swap700-700.eps, height=15cm,
      width=25cm, bbllx=0, bblly=15, bburx=585, bbury=360, clip}
$$
%\comment{Pas la bonne ref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\vspace{-2cm}
\centerline{
  \begin{tabular}{c}
    \begin{tabular}{lr}
      \begin{tabular}{l}
        \paragraph{Mixture model:} \\
        \\
        QQ plot shows that the distribution of \\
        \\
        \multicolumn{1}{c}{
          $T_g = \Delta_g / \hat{\sigma}(\Delta_g)$ 
          }\\
        \\
        is far from normal \\
        \\
        \\
        BIC chooses 6 groups
      \end{tabular}
      &
      \begin{tabular}{c}
%         \epsfig{file = ../Figures/DRD03-Fig4-red.ps, height=8cm,
%         width=8cm, bbllx=225, bblly=553, bburx=410, bbury=705, clip=} 
        \epsfig{file = ../Figures/DRD03-Fig4.ps, height=8cm,
        width=8cm, bbllx=225, bblly=553, bburx=410, bbury=705, clip=} 
      \end{tabular}
    \end{tabular}
    \\
    \begin{tabular}{c}
%       \epsfig{file = ../Figures/DRD03-Fig6-red.ps, height=10cm,
%       width=24cm, bbllx=125, bblly=510, bburx=544, bbury=694, clip=} 
      \epsfig{file = ../Figures/DRD03-Fig6.ps, height=10cm,
      width=24cm, bbllx=125, bblly=510, bburx=544, bbury=694, clip=} 
    \end{tabular}
  \end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\vspace{-2cm}
\hspace{-1cm}
\begin{tabular}{cc}
  \begin{tabular}{l}
    Each gene is affected to a \\
    group $j$ according to the \\
    Maximum A Posteriori rule \\
    \\
    \\
    The $p$-value is calculated with \\
    \\
    \multicolumn{1}{c}{
      $\displaystyle{T_g \sim \sum_k P(k\;|\;j) \Ncal\left(0,
          \frac{\sigma_k}{\sigma_j}\right)}$ 
      }\\
    \\
    where $P(k\;|\;j)$ is the probability \\
    to be affected to group $j$ while \\
    actually belonging to group $k$
  \end{tabular}
  &
  \begin{tabular}{c}
    \begin{pspicture}(12, 18)
      \rput[bl](0.5, 17){\tiny $\hat{\pi}_1 = 0.03$}
      \rput[br](5.5, 13){\tiny $\hat{\sigma}^2_1 = 0.010$}
      \rput[bl](6.7, 17){\tiny $\hat{\pi}_2 = 0.08$}
      \rput[br](11.5, 13){\tiny $\hat{\sigma}^2_2 = 0.027$}
      \rput[bl](0.5, 10.75){\tiny $\hat{\pi}_3 = 0.41$}
      \rput[br](5.5, 7){\tiny $\hat{\sigma}^2_3 = 0.060$}
      \rput[bl](6.7, 10.75){\tiny $\hat{\pi}_4 = 0.34$}
      \rput[br](11.5, 7){\tiny $\hat{\sigma}^2_4 = 0.114$}
      \rput[bl](0.5, 4.5){\tiny $\hat{\pi}_5 = 0.13$}
      \rput[br](5.5, 1){\tiny $\hat{\sigma}^2_5 = 0.220$}
      \rput[bl](6.7, 4.5){\tiny $\hat{\pi}_6 = 0.01$}
      \rput[br](11.5, 1){\tiny $\hat{\sigma}^2_6 = 0.517$}
%       \epsfig{file = ../Figures/DRD03-Fig5-red.ps, height=18cm,
%       width=12cm, bbllx=135, bblly=235, bburx=490, bbury=660, clip=} 
      \epsfig{file = ../Figures/DRD03-Fig5.ps, height=18cm,
      width=12cm, bbllx=135, bblly=235, bburx=490, bbury=660, clip=} 
    \end{pspicture}        
  \end{tabular}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Multiple testing} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip
\section{Testing simultaneously $G$ genes} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
$$
\begin{tabular}{c|cc}
  truth & \begin{tabular}{c}declared\\ non diff. exp.\end{tabular} 
  & \begin{tabular}{c}declared\\ diff. exp.\end{tabular} \\
  & & \\
  \hline
  & & \\
  $G_0$ \begin{tabular}{c}non differentially\\ expressed genes\end{tabular}
  & $TN$ \begin{tabular}{c}true\\ negatives\end{tabular}
  & $FP$ \begin{tabular}{c}false\\ positives\end{tabular} \\
  & & \\
  & & \\
  $G_1$ \begin{tabular}{c}differentially\\ expressed genes\end{tabular}
  & $FN$ \begin{tabular}{c}false\\ negatives\end{tabular} 
  & $TP$ \begin{tabular}{c}true\\ positives\end{tabular} \\
  & & \\
  \hline
  & & \\
  $G$ genes & $N$ negatives  & $P$ positives 
\end{tabular}
$$
\refer{Dudoit \& al (03)}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Basic problem:} 
If
\begin{itemize}
\item all the genes are non differentially expressed ($G_0 =
  G$),
\item all tests are made with level $\alpha$
\end{itemize}
then
$$
FP \sim \Bcal(G, \alpha) \qquad \Longrightarrow \qquad \Esp(FP) = G \alpha.
$$
$G=10\;000, \alpha = 5\% \Longrightarrow \Esp(FP) = 500$ genes to be
studied for nothing.

\paragraph{Global risk $\alpha^*$:} 
\begin{itemize}
\item Family-Wise Error Rate:
$$
FWER = \Pr\{FP > 0\}
$$
\item False Discovery Rate:
  $$
  \begin{array}{rcll}
    FDR & = & \Esp(FP / P) & \qquad \mbox{if } P > 0, \\
    & =& 1 & \qquad \mbox{otherwise.}
  \end{array}
  $$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Family-Wise Error Rate (FWER)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Sidak:} 
If tests are independent and if $G_0 = m$, we have
$$
FWER = 1 - \Pr\{FP = 0\} = 1 - (1-\alpha)^G.
$$
that insures $FWER = \alpha^*$ if each test is performed at level
$$
\alpha = 1 - (1-\alpha^*)^{1/G}.
$$

But the genes expressions (and tests) are \textblue{not independent}.

\paragraph{Bonferroni correction}
is based on the inequality
$$
\Pr\left\{\bigcup_i A_i\right\} \leq \sum_i \Pr\{A_i\}
$$
and implies that performing each test at level $\alpha^*/G$ insures \\
\centerline{$FWER \leq \alpha^*$.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Adaptive procedure for FWER}

\paragraph{Idea:} \\
One step procedure are designed for the smallest $p$-value \\
\centerline{$\Longrightarrow$ they are too conservative}

\paragraph{Principle:} \\
Order the $G$ $p$-values
$$
p_{(1)} < p_{(2)} < \dots < p_{(G)}.
$$
\begin{enumerate}
\item[1.] Apply Sidak or Bonferroni correction to $p_{(1)}$, 
\item[2.] Apply the same correction to $p_{(2)}$, 
replacing $G$ by $G-1$ 
\item[$\vdots$]
\item[$k.$] Apply the same correction to $p_{(k)}$, replacing $G$ by
  $G-k+1$.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Thresholds for Golub data}   (Welch test)

\begin{tabular}{l}
  \epsfig{figure=../Figures/Golub-seuil.eps, height=15cm, width=20cm,
    bbllx=64, bblly=209, bburx=542, bbury=585, clip}
\end{tabular}
\begin{tabular}{l}
  $\bullet$ $p$-value \\
  {\bf --} $5\%$ \\
  \textred{--} Bonferroni \\
  \textred{\dots} Holm \\
  \textlightgreen{--} Sidak \\
  \textlightgreen{\dots} Sidak ad.
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Adjusted $p$-values} are $p$ values that can be directly
compared to the desired FWER $\alpha^*$. 
\begin{itemize}
\item one step Bonferroni
  $$ 
  p_g \leq \alpha^* / G
  \qquad \Longleftrightarrow \qquad 
  \tilde{p}_g = \min(G p_g, 1) \leq \alpha^*
  $$
\item  one step Sidak 
  $$
  p_g \leq 1 - (1 - \alpha^*)^{1/G}
  \qquad \Longleftrightarrow \qquad 
  \tilde{p}_g = 1-(1-p_g)^G  \leq \alpha^*
  $$
\item adaptive Bonferroni \qquad (\refer{Holm (79}{}))
  $$
  \tilde{p}_{(g)} = \max_{j \leq g}\{\min[(G-j+1)p_{(j)}, 1]\} 
  $$
\item adaptive Sidak 
  $$
  \tilde{p}_g = \max_{j \leq g}\{\min[1-(1-p_{(j)})^{G-j+1}, 1]\}
  $$
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Accounting for dependency} \\
The Westfall \& Young procedure preserves the correlation between
genes using permutation tests and applying the \textblue{same
  permutations} to all the genes.

Adjusted $p$-values are estimated by
$$
\begin{array}{rll}
  \hat{\tilde{p}} =
    &
    \displaystyle{\frac1S \sum_s \Ibb\{p^s_{(g)} < p_g\}}
    & 
    \qquad\mbox{"minP'' procedure} \\
    \\
    &
    \displaystyle{\frac1S \sum_s \Ibb\{|T^s_{(g)}| > |T_g|\}}
    & 
    \qquad \mbox{"maxT'' procedure}
  \end{array}
$$

The number of replicates in each condition is, again, a limitation of
this estimation procedure.

\refer{Westfall \& Young (93)}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{False Discovery Rate (FDR)} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Idea:} Not to control the risk of one error, but the
proportion of errors $\Rightarrow$ \textblue{less conservative} than
controlling FWER. 

\paragraph{Principle:} The number of declared positive genes $P$ is
given by the largest $g$ such as
$$
p_{(g)} \leq g \alpha^* / G.
$$

\paragraph{Property:} \\
If tests are independent, this guarantees that
$$
FDR \leq (G_0 / G) \alpha^* \leq \alpha^*.
$$

\refer{Benjamini \& Hochberg (95)}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage 
\paragraph{Central idea:}
The $p$-values of null genes are uniformly distributed: $g$ null
$\Rightarrow p_g \sim \Ucal[0, 1]$.

The histogram of the $p$-values must look like this: \\
%$$
\centerline{
  \epsfig{figure=../Figures/HistoPvalRef.ps, height=14cm, width=22cm,
    clip=}%, bbllx=25.3, bblly=16, bburx=243, bbury=185}   
}
%$$

\newpage 
$$
\begin{tabular}{cc}
  not like this: & nor like this: \\
  \epsfig{figure=../Figures/TDdiff.eps, height=10cm, width=12cm,
    clip=, bbllx=25, bblly=28, bburx=176, bbury=131} & 
  \epsfig{figure=../Figures/CompExcepLRT.eps, height=10cm, width=12cm,
    clip=, bbllx=108, bblly=663, bburx=509, bbury=730.5} 
\end{tabular}
$$

The absence of a 'large' uniform part (on the right) reveals a
\textblue{lack of fit} for the null distribution of test statistic.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage 
\paragraph{Remarks:} 
\begin{itemize}
\item Estimating $G_0$ improves the procedure.
\item The Benjamini \& Yekutieli (01) criterion: \qquad
  $p_{(g)} \leq g \alpha^* \left/ \left( G \sum_j 1/j\right) \right.$ \\
  controls the FDR for some positive correlations between tests.
\end{itemize}

\paragraph{Adjusted $p$-value:} \\
For the Benjamini \& Hochberg procedure:
$$
\tilde{p}_{(g)} = \min_{j \geq g}\{\min[G p_{(j)} / j, 1]\}.
$$

\paragraph{Statistical Analysis of Microarray  (SAM):} \\
Proposed by Tusher \& al. (01) aims to control FDR using a Westfall \&
Young estimates of the adjusted $p$-value.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Adjusted $p$-values for Golub data} 

\begin{tabular}{l}
  \epsfig{figure=../Figures/Golub-adjp.eps, height=15cm, width=20cm,
    bbllx=64, bblly=209, bburx=542, bbury=585, clip}
\end{tabular}
\begin{tabular}{l}
  $\bullet$ $p$-value \\
  {\bf --} $5\%$ \\
  \textred{--} Bonferroni \\
  \textred{\dots} Holm \\
  \textlightgreen{--} Sidak \\
  \textlightgreen{\dots} Sidak ad. \\
  \textblue{\dots} FDR
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Number of positive genes} 

\begin{tabular}{l}
  $p$-value: \\
  \qquad 1887 \\
  \\
  Bonferroni: \\
  \qquad 111 \\
  \\
  Sidak: \\
  \qquad 113 \\
  \\
  Holm: \\\\
  \qquad 112 \\
  \\
  Sidak adp.: \\
  \qquad 113 \\
  \\
  FDR: \\
  \qquad  903 \\
\end{tabular}
\begin{tabular}{l}
  \epsfig{figure=../Figures/Golub-adjp-zoom.eps, height=15cm, width=20cm,
    bbllx=64, bblly=209, bburx=542, bbury=585, clip}
\end{tabular}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Estimation of the proportion of 'null' genes}

\paragraph{Empirical cdf.} The cumulative distribution function (cdf) of the
$p$-values can be estimated via its empirical version:
$$
\widehat{F}(p) = \frac1n \sum_{g=1}^G \Ibb\{P_g \leq p\}.
$$
The cdf of the negative $p$-values is given by the uniform
distribution:
$$
\Pr\{P_i \leq p \;|\; i \in \Hbf_0\} = p.
$$

\paragraph{Cdf mixture.} Denoting $F_1$ the cdf of the positive
$p$-values, we have
$$
F(p) = \pi_0F_1(p) + (1-\pi_0) F_0(p) = \pi_0F_1(p) + (1-\pi_0) p.
$$
Above a certain threshold $\lambda$, $F_1(p)$ should be close to 1: 
$$
p > \lambda: \quad F(p) \simeq \pi_0+ (1-\pi_0) p.  
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage 
\noindent
\begin{tabular}{p{12cm}p{12cm}}
  \paragraph{Empirical proportion.} Storey \& al, Genovese \& Wasserman
  (JRSS-B, 02) propose an estimate of $a$ based on this approximation: 
  &
  \paragraph{Linear regression.}
  $\pi_0$ can also be estimated by the coefficient of the linear
  regression of $\widehat{F}(p)$ wrt $p$, for $p > \lambda$: \\
  \qquad $\widehat{\pi}_0 = \left.[R(\lambda)/G] \right/ (1-\lambda)$ 
  & \qquad $\widehat{F}(p) \simeq \widehat{\pi}_0 p + \mbox{cst}$ 
\\
  \epsfig{file=../Figures/RegGenoWas.eps, width=12cm, height=12cm,
  clip=, angle=90, bbllx=66, bblly=424, bburx=555, bbury=694}
  & \epsfig{file=../Figures/RegGenoWas.eps, width=12cm, height=12cm,
    clip=, angle=90, bbllx=66, bblly=85, bburx=555, bbury=353} 
\end{tabular} \\
Both method provide upwardly biased estimates $\Rightarrow$
conservative estimates of FDR.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\chapter{Mixture model} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bigskip
\section{Local False Discovery Rate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent FDR provides a general information about the risk of the whole
procedure (up to step $i$). But we are actually interested in a
specific risk, associated to each gene.

\paragraph{Derivative of the FDR:} $\lFDR_{(i)}$ can be also defined as the
derivative of the FDR
$$
\lFDR(t) = \lim_{h \downarrow 0} \frac{FDR(t+h) - FDR(t)}{h}
$$
which can be estimated by 
$$
\widehat{n}_0 (P_{(i)} - P_{(i-1)})
$$
(Aubert \& al., BMC Bioinfo., 04).

\paragraph{Local FDR ($\lFDR$).} First defined by Efron \&
al. (JASA, 2001) in a mixture model framework:
$$
\lFDR_i := \Pr\{\Hbf_0(i) \mbox{ is false} \;|\; T_i\}.
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Mixture model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Mixture for the test statistics:} Instead of looking at
$p$-values, one could suppose that genes come from three populations
\begin{itemize}
\item down-regulated (population ``$-$'') 
\item not regulated (population ``$0$'')
\item up-regulated (population ``$+$'')
\end{itemize}

This leads to an univariate mixture model on statistics $T_g$:
$$
T_g \sim \pi_{-} \phi_{-}(\cdot) + \pi_{0} \phi_{0}(\cdot) + \pi_{+} \phi_{+}(\cdot) 
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
$$
  \begin{tabular}{cc}
    \textblue{Model:} & \textblue{Posterior probability:} \\
    \\
    $f(x) = \textred{\pi_1 f_1(x)} + \textgreen{\pi_2 f_2(x)} +
    \textblue{\pi_3 f_3(x)}$ & $\tau_{gk} = \Pr\{g \in f_k \;|\; x_g\}
    = \pi_k f_k(x_g) / f(x_g)$\\
    \\
    \epsfig{file=../Figures/Melange-densite.ps, height=6cm,
      width=12cm, bbllx=77, bblly=328, bburx=549, bbury=528, clip=}
    &
    \epsfig{file=../Figures/Melange-posteriori.ps, height=6cm, width=12cm,
      bbllx=83, bblly=320, bburx=549, bbury=537, clip=} 
  \end{tabular}
$$
$$  
\begin{array}{cccc}
  \quad \tau_{gk}~~(\%) \quad & \qquad g=1 \qquad & \qquad g=2 \qquad &
  \qquad g=3 \qquad \\
  \hline
  k = 1 & 65.8 & 0.7 & 0.0 \\
  k = 2 & 34.2 & 47.8 & 0.0 \\
  k = 3 & 0.0 & 51.5 & 1.0
\end{array}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Distribution of the test statistic.} Efron \& al. (01)
propose to describe the distribution of the test statistic $T_i$ using
a mixture model.  
$$
T_i \sim f(t) = p_1 f_1(t) + p_0 f_0(t)
$$
where both, $a$, $f_0$ and $f_1$ have are unknown.

\bigskip
\centerline{
  \epsfig{file = ../Figures/ETG00-Fig2.ps,
    width=20cm, height=10cm, bbllx=90, bblly=315, bburx=520, bbury=560,
    clip=}
}

\newpage 
Using the high number of replicates of their example (Affymetrix
data), they use a local logistic regression to estimate the \paragraph{\sl
  local FDR} ($\lFDR$):
$$
\lFDR_i = p_0 f_0(T_i) / f(T_i)
$$
which is actually the \textblue{\sl posterior probability} that
the test $i$ is actually negative given the value of the test
statistic.
$$
\epsfig{file = ../Figures/ETG00-Fig1.ps,
  width=20cm, height=10cm, bbllx=90, bblly=330, bburx=510, bbury=580,
  clip=}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Mixture for the $p$-values}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Allison (02) proposes the same strategy regarding the $p$-values,
assuming that
$$
P \sim a B(r,s) + (1-a) \Ucal_{[0; 1]}
$$
where the proportion $a$ and the parameters $r$ and $s$
have to be estimated, for example, using the E-M algorithm.

\noindent\begin{tabular}{ccc}
  \begin{tabular}{l}
    \paragraph{Beta density:} \\
    \\
    \\
    $\beta(p; r, s) = $ \\
    \\
    $\displaystyle{\frac{p^{r-1} (1-p)^{s-1}}{B(r, s)},}$ \\
    \\
    \\
    $0 \leq p \leq 1.$
  \end{tabular}
  &
  \begin{tabular}{c}
    \epsfig{file=../Figures/FigBeta.eps, width=10cm, height=10cm, clip=}
  \end{tabular}
  &  
  \begin{tabular}{l}
    {\bf ---} $r = s = 1$ \\ \\
    \textred{\bf ---} $r = 1, s = 10$ \\ \\
    \textblue{\bf ---} $ r = 2, s = 10$ \\ \\
    \textgreen{\bf ---} $ r = s = 0.33$ \\
%    \\ \\ \\ \\
  \end{tabular}
\end{tabular}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{E-M algorithm.} The most popular algorithm to estimate the
parameters of a mixture model is Expectation-Maximization. The
principle is to alternate the two steps.
\begin{description}
\item[E step:] For each observation $i$ calculate the posterior
  probability $\tau_i$ that it comes from the non-null
  distribution using Bayes' formula
  $$
  \tau^{h+1}_i = \frac{\widehat{a}^h \beta(p_i; \widehat{s}^h,
  \widehat{r}^h)}{\widehat{g}^h(p_i)}, 
\qquad \widehat{g}^h(p_i) = \widehat{a}^h \beta(p_i; \widehat{r}^h,
  \widehat{s}^h) + (1- \widehat{a}^h)
  $$
\item[M step:] Calculate the maximum-likelihood estimates of
  $r$ and $s$ giving to each observation $i$ a weight
  $\tau_i^{h+1}$.
\end{description}

\paragraph{Properties:} 
\begin{enumerate}
\item \vspace{-1cm} 
  At each E-M step, the likelihood of the data under the mixture
  model increases.
\item E-M provide estimates of the posterior probabilities which are
  actually the most relevant quantities.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Semi-parametric mixture model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bigskip
\paragraph{Property of the test statistic.} The standard hypotheses
testing theory implies that, under $\Hbf_0(i)$, $P_i$ is uniformly
distributed over $[0, 1]$: \\
\begin{tabular}{cc}
  \begin{tabular}{p{13.5cm}}
    $$
    P_i \underset{\Hbf_0(i)}{\sim} \Ucal_{[0, 1]}
    $$
    \\
    ~\\
    The $P_i$'s are distributed according to a mixture distribution
    with density 
    $$
    g(p) = a f(p) + (1-a) 
    $$
    \\
    The problem is then to estimate
  \end{tabular}
  &
  \begin{tabular}{c}
  \epsfig{figure=../Figures/HistoPvalRef.ps, height=8cm, width=8cm,
    clip=, bbllx=26, bblly=40, bburx=176, bbury=130}   
%     \epsfig{
%       file=../Figures/HistoPval.ps,
%       height=8cm, width=8cm, bbllx=82, bblly=304, bburx=277,
%       bbury=460, angle=90, clip=} 
  \end{tabular}
\end{tabular}
$$
\begin{tabular}{ll}
  \textblue{$a$:} & the proportion of differentially expressed genes
  \\
  \textblue{$f$:} & the alternative density \\
\end{tabular}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Generalization:} We consider an i.i.d. sample $\{X_1,
\dots, X_n\}$ with mixture density
$$
g(x) = a f(x) + (1-a) \phi(x)
$$
\begin{tabular}{lr}
  The proportion $a$ is unknown &  $\longrightarrow$ \textblue{parametric part} \\ 
  \\
  The density $f$ is completely unknown & \quad $\longrightarrow$ \textblue{non
    parametric part} \\ 
  \\
  The density $\phi$ in completely specified & ($\Ucal_{[0, 1]}$,
  $\Ncal(0, 1)$, {\it etc.})
\end{tabular}

\bigskip \bigskip
\paragraph{Posterior probability.} We are interested in the
estimation of 
$$
\tau_i = \Pr\{Z_i = 1 \;|\; x_i\} = \Esp(Z_i \;|\; x_i) = \frac{a
  f(x_i)}{g(x_i)}
$$
where $Z_i =
\left\{
\begin{tabular}{rll}
  $Z_i = 1$ & if $i$ comes from $f$ & ($\Hbf_0(i)$ false), \\
  \\
  $Z_i = 0$ & otherwise & ($\Hbf_0(i)$ true).
\end{tabular}
\right.$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Density estimation}

\hspace{-2cm}
\begin{tabular}{ll}
  \begin{tabular}{p{10cm}}
  \paragraph{Kernel estimate.} A natural non-parametric estimate of $f$
  is \\
  $\displaystyle{\widehat{f}(x) = \frac1{\sum_i Z_i} \sum_i Z_i k_i(x)}$ \\
  where \\
  $\displaystyle{k_i(x) = \frac1h k\left(\frac{x-x_i}h\right)}$  \\
  \\
  $k$ being a kernel, i.e. a symmetric density function with mean
  0. \\
  \\
  \end{tabular}
  &
  \begin{tabular}{c}
    \epsfig{file=../Figures/FigKernelEstim.eps, width=9cm, height=13cm,
    angle=90, clip=, bbllx=77, bblly=61, bburx=552, bbury=676}
  \end{tabular}
\end{tabular}

\vspace{-0.5cm}
\paragraph{Weighted kernel estimate.} Since the $Z_i$'s are unknown,
we propose to replace them by their conditional expectations:
$$
\widehat{f}(x) = \frac1{\sum_i \tau_i}\sum_i \tau_i k_i(x)
$$
$\tau_i$ is the weight of observation $i$ in the estimation of $f$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Property of the $\widehat{\tau}_i$}. The estimates of the
$\tau_i$'s must satisfy
$$
\widehat{\tau}_j = \frac{a \widehat{f}(x_j)}{\widehat{g}(x_j)} 
= \frac{a \sum_i \widehat{\tau}_i k_i(x_j)}{a \sum_i \widehat{\tau}_i
  k_i(x_j) + (1-a) \phi(x_j) \sum_i \widehat{\tau}_i}
$$
or 
$$
\widehat{\tau}_j =\frac{\sum_i \widehat{\tau}_i b_{ij}}{\sum_i \widehat{\tau}_i b_{ij}
  + \sum_i \widehat{\tau}_i}
\qquad \mbox{with} \qquad
b_{ij} = \frac{a}{1-a} \frac{k_i(x_j)}{\phi(x_j)} \geq 0
$$

\paragraph{Function $\psibf$.}
$$
\begin{array}{rcl}
  \psibf: \Rbb^n & \rightarrow & \Rbb^n \\
  \ubf & \rightarrow &  \displaystyle{\psibf(\ubf): \psi_j(\ubf) = \frac{\sum_i
  u_i b_{ij}}{\sum_i u_i b_{ij}+ \sum_i u_i}}  
\end{array}
$$

\bigskip \bigskip 
\centerline{\fbox{$\widehat{\taubf} = (\widehat{\tau}_1, \dots,
  \widehat{\tau}_n)$ is a fixed point of $\psibf$.}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Estimation algorithm of $\taubf$.} Given some initial
$\widehat{\taubf}^{0}$, iterate $\psibf$:
$$
\widehat{\taubf}^{h+1} = \psibf(\widehat{\taubf}^{h}).
$$

\paragraph{$a$ remains fix:} it has to be estimated independently.

\paragraph{2 steps of the algorithm:} \\
{\bf ''E'' step:} given $\widehat{f}^h$ and $\widehat{g}^h$, calculate
$$
\widehat{\taubf}^{h+1} = a \widehat{f}^h(x_i) \left/
  \widehat{g}^h(x_i) \right..
$$
{\bf Other step:} given $\widehat{\taubf}^{h}$, estimate $f$ and
$g$:
$$
\widehat{f}^h(x) = \sum_i \widehat{\tau}^h_i k_i(x) \left/ \sum_i
  \widehat{\tau}^h_i \right., 
\qquad 
\widehat{g}^h(x) = a \widehat{f}^h(x) + (1-a) \phi(x).
$$
This second step does not maximize the likelihood $\longrightarrow$
not an E-M algorithm.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Important trick: the Probit transform}

%Instead of modeling the distribution of the $P_i$', we consider the
%$$
\begin{tabular}{lcr}
  $P_i \in [0, 1]$
  &   
  \qquad 
  &
  $X_i = \Phi^{-1}(P_i) \in \Rbb$ 
  \\
  \multicolumn{3}{c}{(Efron, JASA, 2005)}
  \\
  \textred{$\phi = \Ucal_{[0; 1]}$}
  & 
  & 
  \textred{$\phi = \Ncal(0, 1)$}
  \\
  \\
  \\
  \epsfig{figure=../Figures/HistoPval.ps, height=8cm, width=11.5cm,
    clip=, bbllx=26, bblly=40, bburx=176, bbury=130}   
  &
  &
%  \vspace{-2cm}
  \epsfig{figure=../Figures/HistoProbit.ps, height=8cm, width=11.5cm,
    clip=, bbllx=26, bblly=39, bburx=176, bbury=130}   
\end{tabular}
%$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Application to Hedenfalk data}

% \newpage
% \section{Interest of cross-validation.}

% \paragraph{Hedenfalk data.} Comparison of 2 breast cancers (BRCA1 /
% BRCA2): \\
% $n = 3226$ genes, Epanechnikov kernel, Cochran test %(t-test with heterogenous group variances)

% \bigskip\bigskip \hspace{-2cm}
% \begin{tabular}{c@{}c@{}c} 
%   \multicolumn{2}{c}{log-likelihood $\Lcal(a, h)$, \quad ($V = 5$)} \\
%   \\
%   training set & test set: $\Lcal_{CV}$ \\
%   \begin{tabular}{c}
%     \epsfig{
%       file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Amax/Asup-0.8/Cochran-Gaus.eps,
%       height=8cm, width=8cm, bbllx=330, bblly=500, bburx=550,
%       bbury=710, clip=, angle=90} 
%   \end{tabular}
%   &
%   \begin{tabular}{c} 
%     \epsfig{
%       file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Amax/Asup-0.8/Cochran-Gaus.eps,
%       height=8cm, width=8cm, bbllx=330, bblly=290, bburx=550,
%       bbury=500, clip=, angle=90} 
%   \end{tabular}
%   &
%   \begin{tabular}{c} 
%     \epsfig{
%       file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Amax/Asup-0.8/Cochran-Gaus.eps,
%       height=8cm, width=8cm, bbllx=330, bblly=80, bburx=555,
%       bbury=290, clip=, angle=90} 
%   \end{tabular}
%   \\
%   $\widehat{a} \rightarrow 1, \quad \widehat{h} \rightarrow 0$
%   & $\widehat{h} = 0.177$
%   & $\widehat{a} = 0.443$
% \end{tabular}

\paragraph{Student t-test} with homogenous variance $\sigma_g = $
cst. Gaussian kernel.

\hspace{-2cm}
\begin{tabular}{cc} 
  \begin{tabular}{c}
    $\widehat{a} = 20.6 \%$ \\
    $\textblue{\widehat{g}(x)}  = \widehat{a} \textred{\widehat{f}(x)} + (1-\widehat{a})
    \textgreen{\widehat{f}(x)}$ \\ 
    \epsfig{
      file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Ainit/Asup-1.0/Homogen-Gaus.eps,
      height=12cm, width=9cm, bbllx=66, bblly=510, bburx=283,
      bbury=691, clip=, angle=90} 
  \end{tabular}
  &
  \begin{tabular}{c} 
    $\textred{\widehat{FDR}_i}, \textblue{\widehat{\tau}_i} \times
    \Phi^{-1}(P_i)$ \\ 
    \epsfig{
      file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Ainit/Asup-1.0/Homogen-Gaus.eps,
      height=12cm, width=4.5cm, bbllx=66, bblly=295, bburx=283,
      bbury=480, clip=, angle=90} 
    \\
    $\textred{\widehat{FDR}_i}, \textblue{\widehat{\tau}_i} \times
    P_i$ \\ 
    \epsfig{
      file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Ainit/Asup-1.0/Homogen-Gaus.eps,
      height=12cm, width=4.5cm, bbllx=66, bblly=85, bburx=283,
      bbury=265, clip=, angle=90} 
  \end{tabular}
\end{tabular}

\noindent $\Hbf_0$ (negative) $p$-values are not uniformly distributed
over [0, 1]. \\
The non-parametric part $\widehat{f}$ captures this departure of the
null distribution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\paragraph{Variance modeling} $K = 5$ groups of variances.

\hspace{-2cm}
\begin{tabular}{cc} 
  \begin{tabular}{c}
    $\widehat{a} = 30.5 \%$ \\
    $\textblue{\widehat{g}(x)}  = a \textred{\widehat{f}(x)} + (1-a)
    \textgreen{\widehat{f}(x)}$ \\ 
    \epsfig{
      file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Ainit/Asup-1.0/Varmixt-Gaus.eps,
      height=12cm, width=9cm, bbllx=66, bblly=510, bburx=283,
      bbury=691, clip=, angle=90} 
  \end{tabular}
  &
  \begin{tabular}{c} 
    $\textred{\widehat{FDR}_i}, \textblue{\widehat{\tau}_i} \times
    \Phi^{-1}(P_i)$ \\ 
    \epsfig{
      file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Ainit/Asup-1.0/Varmixt-Gaus.eps,
      height=12cm, width=4.5cm, bbllx=66, bblly=295, bburx=283,
      bbury=480, clip=, angle=90} 
    \\
    $\textred{\widehat{FDR}_i}, \textblue{\widehat{\tau}_i} \times
    P_i$ \\ 
    \epsfig{
      file=/RECHERCHE/EXPRESSION/EXEMPLES/HEDENFALK/Ainit/Asup-1.0/Varmixt-Gaus.eps,
      height=12cm, width=4.5cm, bbllx=66, bblly=85, bburx=283,
      bbury=265, clip=, angle=90} 
  \end{tabular}
\end{tabular}

\centerline{$
  \begin{array}{ccccc}
    \quad \widehat{FDR}_{(i)} \quad & \qquad i \qquad & \quad P_{(i)}
    \quad & \quad \widehat{\tau}_{(i)} \quad & \quad
    \widehat{FNR}_{(i)} \quad \\  
    \hline
    1\% & 4 & 2.5\;10^{-5} & 0.988 & 31.5 \% \\
    5\% & 142 & 3.1\;10^{-3} & 0.914 & 28.7 \% \\
    10\% & 296 & 1.3\;10^{-2} & 0.798 & 25.7 \% \\
  \end{array}
$}
\vspace{-0.5cm}
$\widehat{FDR}_{(i)} = \widehat{FNR}_{(i)} = 19.7 \%$ for $(i) = 633,
P_{(i)} = 5.4 \%, \widehat{\tau}_{(i)} = 43.5 \%$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
